import multiprocessing
import os
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°ç£è‚¡ç¥¨åˆ†æç³»çµ± v40 Enhanced (å‡ç·šå¤šé ­å„ªåŒ–ç‰ˆ) - æ¶æ§‹å¸«ä¿®æ­£ç‰ˆ
æ¶æ§‹å¸«: è³‡æ·±è»Ÿé«”æ¶æ§‹å¸«
ä¿®æ­£æ—¥æœŸ: 2024-12-07

è¦å‰‡åš´æ ¼éµå®ˆ(1.ç¹é«”ä¸­æ–‡ã€2.Aè¦å‰‡ã€3.ä¸‰è¡Œé€²åº¦ã€4.æ–·æª”çºŒè®€ã€5.è³‡æ–™é¡¯ç¤ºæ–¹å¼ã€6.ä½¿ç”¨å®˜æ–¹çš„çœŸå¯¦æ•¸æ“šæŠ“åˆ°ä»€éº¼å°±è¼¸å‡ºä»€éº¼ï¼Œ
7.ä¸è¦æœ‰æŒ‰ä»»æ„éµè¿”å›/ç¹¼çºŒï¼Œä¸€å¾‹ç›´æ¥é€²å…¥é¸å–®æˆ–é¡¯ç¤ºã€8.çµ±ä¸€è¨­å®šæ•¸å­—0ç‚ºè¿”å›ï¼Œ9.å°‡å„æ–¹æ‰€æŠ“å–ä¾†çš„è³‡æ–™çµ±ä¸€æˆä¸€å€‹å½¢å¼å¾Œï¼Œè¼¸å…¥è³‡æ–™åº«ï¼Œæ–¹ä¾¿ä¹‹å¾Œçš„è³‡æ–™èª¿ç”¨)
10.æ‰€æœ‰æ•¸å­—åªå–åˆ°å°æ•¸é»å¾ŒäºŒä½ï¼Œ"""

# ==============================
# å®‰è£éœ€æ±‚ (æ‰‹æ©Ÿèˆ‡é›»è…¦é€šç”¨)
# ==============================
# 
# ã€é›»è…¦ (Windows/Mac/Linux)ã€‘
#   pip install requests twstock lxml pandas numpy colorama
#
# ã€æ‰‹æ©Ÿ (Pydroid 3)ã€‘
#   pip install requests twstock lxml pandas numpy colorama
# ==============================
# è‡ªå‹•å®‰è£/æ›´æ–° twstock (æ¯æ—¥ä¸€æ¬¡)
# ==============================
import sys
import subprocess
import os
from datetime import datetime

def _should_update_twstock():
    """æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–° twstock (æ¯æ—¥ä¸€æ¬¡)"""
    flag_file = os.path.join(os.path.dirname(__file__), '.twstock_updated')
    today = datetime.now().strftime('%Y-%m-%d')
    
    if os.path.exists(flag_file):
        with open(flag_file, 'r') as f:
            last_update = f.read().strip()
            if last_update == today:
                return False  # ä»Šå¤©å·²æ›´æ–°é
    
    # å¯«å…¥ä»Šæ—¥æ¨™è¨˜
    with open(flag_file, 'w') as f:
        f.write(today)
    return True

if _should_update_twstock():
    try:
        print("æ­£åœ¨æª¢æŸ¥ twstock ç‰ˆæœ¬...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", "-U", "twstock"], 
                              stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        print("twstock æ›´æ–°å®Œæˆ")
    except Exception as e:
        print(f"twstock æ›´æ–°å¤±æ•—: {e}")

import os
import time
import json
import re
import sqlite3
import logging
import requests
import threading
import warnings
import pandas as pd
import numpy as np
import ssl
import urllib3
import twstock
from twstock.stock import TPEXFetcher
from pathlib import Path
from datetime import datetime, timedelta
import queue
import gc
import math
from typing import Optional, Dict, List, Tuple, Any
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor, as_completed, Future
from dataclasses import dataclass, field
import colorama
try:
    colorama.just_fix_windows_console()
except AttributeError:
    # Fallback for older colorama versions
    colorama.init()

# Supabase Support
try:
    from supabase import create_client, Client
    HAS_SUPABASE = True
except ImportError:
    HAS_SUPABASE = False

# ==============================
# Logging Configuration
# ==============================
logging.basicConfig(
    level=logging.DEBUG,  # é–‹å•Ÿæ‰€æœ‰ç´šåˆ¥
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("system.log", encoding='utf-8'),  # æ‰€æœ‰è¨Šæ¯å¯«å…¥ log
    ]
)
# æ§åˆ¶å°åªé¡¯ç¤º CRITICAL (éš±è— ERROR/WARNING/INFO)
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.CRITICAL)  # åªé¡¯ç¤º CRITICAL ç´šåˆ¥
logging.getLogger().addHandler(console_handler)

logger = logging.getLogger("TWSE_System")
# æŠ‘åˆ¶ç¬¬ä¸‰æ–¹åº«çš„æ—¥èªŒ
logging.getLogger("urllib3").setLevel(logging.WARNING)
logging.getLogger("requests").setLevel(logging.WARNING)

# ==============================
# SSL Patch (Fix for Android/Windows SSL errors)
# ==============================
ssl._create_default_https_context = ssl._create_unverified_context
requests.packages.urllib3.disable_warnings(requests.packages.urllib3.exceptions.InsecureRequestWarning)

# Monkey patch requests.Session.request to disable verification globally
old_request = requests.Session.request
def new_request(self, method, url, *args, **kwargs):
    kwargs['verify'] = False
    return old_request(self, method, url, *args, **kwargs)
requests.Session.request = new_request

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        CONFIG                                 â•‘
# â•‘  æ‰€æœ‰ç¡¬ç¢¼å¸¸æ•¸ã€API ç«¯é»è¡¨ã€SQL æ¨¡æ¿é›†ä¸­æ–¼æ­¤                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ==============================
# å…¨åŸŸé…ç½® (Config Class)
# ==============================
class Config:
    """ç³»çµ±å…¨åŸŸé…ç½®ï¼Œæ¶ˆé™¤é­”è¡“æ•¸å­—"""
    # è³‡æ–™å›æº¯è¨­å®š
    HISTORY_DAYS_LOOKBACK = 1095    # æ­·å²è³‡æ–™å›æº¯å¤©æ•¸ (3å¹´)
    CALC_LOOKBACK_DAYS = 450        # æŒ‡æ¨™è¨ˆç®—å›æº¯å¤©æ•¸
    
    # é¡¯ç¤ºè¨­å®š
    DEFAULT_DISPLAY_LIMIT = 30      # é è¨­é¡¯ç¤ºç­†æ•¸
    DEFAULT_DISPLAY_DAYS = 10       # é è¨­é¡¯ç¤ºå¤©æ•¸
    
    # æƒæåƒæ•¸
    VP_TOLERANCE_PCT = 0.02         # VP æ”¯æ’å£“åŠ›å®¹è¨±èª¤å·® (2%)
    MIN_VOLUME_DEFAULT = 500        # é è¨­æœ€å°æˆäº¤é‡ (å¼µ)
    
    # API è¨­å®š
    API_TIMEOUT = 10                # API è«‹æ±‚è¶…æ™‚ (ç§’)
    
    # è·¯å¾‘è¨­å®š
    DB_PATH = "taiwan_stock.db"     # è³‡æ–™åº«è·¯å¾‘
    PROGRESS_FILE = "progress.json" # é€²åº¦æª”æ¡ˆè·¯å¾‘
    
    # ==============================
    # ç’°å¢ƒè‡ªé©æ‡‰é…ç½® (Phase 5)
    # ==============================
    # è‡ªå‹•è¨­å®šï¼Œå°‡åœ¨æ¨¡çµ„è¼‰å…¥å¾Œæ›´æ–°
    IS_ANDROID = False              # æ˜¯å¦ç‚º Android ç’°å¢ƒ
    MAX_WORKERS = 6                 # å¤šç·šç¨‹æœ€å¤§å·¥ä½œæ•¸
    BATCH_SIZE = 200                # æ‰¹æ¬¡è™•ç†å¤§å°
    LIGHTWEIGHT_MODE = False        # è¼•é‡æ¨¡å¼ (æ‰‹æ©Ÿå°ˆç”¨)

# ==============================
# TPEX Patch (Fix for 404 Error)
# ==============================
def tpex_fetch(self, year: int, month: int, sid: str, retry: int = 5):
    # TPEX New API URL
    url = "https://www.tpex.org.tw/www/zh-tw/afterTrading/tradingStock"
    
    # Construct date param (use the first day of the month)
    date_str = f"{year}/{month:02d}/01"
    
    params = {
        "date": date_str,
        "code": sid,
        "response": "json"
    }
    
    for retry_i in range(retry):
        try:
            # åŠ å…¥éš¨æ©Ÿå»¶é²ï¼Œé¿å…è§¸ç™¼ Rate Limit
            time.sleep(np.random.uniform(1.5, 3.0))
            
            r = requests.get(url, params=params, headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            })
            data = r.json()
        except Exception:
            continue
        else:
            break
    else:
        return {"data": []}

    result = {"data": []}
    if data.get("stat") == "ok" and data.get("tables"):
        raw_data = data["tables"][0]["data"]
        result["data"] = [self._make_datatuple(row) for row in raw_data]
        
    return result

TPEXFetcher.fetch = tpex_fetch

# ==============================
# ç’°å¢ƒé©é…
# ==============================
try:
    import termios
    import tty
    HAS_TERMIOS = True
except ImportError:
    HAS_TERMIOS = False

warnings.filterwarnings("ignore")

# å®‰å…¨åœ°ç¦ç”¨ SSL è­¦å‘Š
try:
    requests.packages.urllib3.disable_warnings()
except Exception:
    pass

# Windows ç’°å¢ƒçš„çµ‚ç«¯æ©Ÿè¨­å®š
if os.name == 'nt':
    try:
        import ctypes
        kernel32 = ctypes.windll.kernel32
        kernel32.SetConsoleMode(kernel32.GetStdHandle(-11), 7)
    except Exception:
        pass

# ==============================
# é…ç½®åƒæ•¸
# ==============================
def get_work_directory():
    """ç²å–å·¥ä½œç›®éŒ„ - å¹³å°æ„ŸçŸ¥"""
    if os.name == 'nt':
        return Path(__file__).parent.absolute()
    
    # Android è·¯å¾‘
    android_paths = [
        Path('/sdcard/Download/stock_app'),
        Path('/storage/emulated/0/Download/stock_app')
    ]
    
    for path in android_paths:
        if path.exists() or path.parent.exists():
            path.mkdir(parents=True, exist_ok=True)
            return path
    
    return Path(__file__).parent.absolute()

WORK_DIR = get_work_directory()

# ç’°å¢ƒæ„ŸçŸ¥: æª¢æ¸¬æ˜¯å¦ç‚º Android ç’°å¢ƒ
IS_ANDROID = any(p in str(WORK_DIR) for p in ['/sdcard', '/storage/emulated']) or os.path.exists('/data/data/com.termux')

# æ›´æ–° Config ç’°å¢ƒè‡ªé©æ‡‰é…ç½®
Config.IS_ANDROID = IS_ANDROID
Config.MAX_WORKERS = 2 if IS_ANDROID else 6
Config.BATCH_SIZE = 50 if IS_ANDROID else 200
Config.LIGHTWEIGHT_MODE = IS_ANDROID

if not WORK_DIR.exists():
    WORK_DIR.mkdir(parents=True, exist_ok=True)

# æª”æ¡ˆè·¯å¾‘é…ç½®
DB_FILE = WORK_DIR / 'taiwan_stock.db'
STOCK_LIST_PATH = WORK_DIR / 'stock_list.csv'
PROGRESS_FILE = WORK_DIR / 'download_progress.json'
BACKUP_DIR = WORK_DIR / 'backups'
BACKUP_DIR.mkdir(exist_ok=True)
REQUEST_TIMEOUT = 30

# API è¨­å®š
FINMIND_TOKEN = "eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJkYXRlIjoiMjAyNS0xMi0xNyAyMjowMzowMiIsInVzZXJfaWQiOiJ5dW5ndGFuZyAiLCJpcCI6IjExMS43MS4yMTIuMjUifQ.fYv38gHAin0IZu5GZZyFFjj5tPU8BCCORDTUTandpDg"

# ==============================
# Phase 1: è¡¨é©…å‹•æ³• - API ç«¯é»é…ç½®è¡¨
# ==============================
API_ENDPOINTS = {
    'finmind': {
        'base': 'https://api.finmindtrade.com/api/v4/data',
    },
    'twse': {
        'daily_all': 'https://openapi.twse.com.tw/v1/exchangeReport/STOCK_DAY_ALL',
        'daily': 'https://www.twse.com.tw/rwd/zh/afterTrading/STOCK_DAY',
        'bwibbu': 'https://openapi.twse.com.tw/v1/exchangeReport/BWIBBU_ALL',
        'institutional': 'https://openapi.twse.com.tw/v1/fund/T86_ALL',
        'margin': 'https://openapi.twse.com.tw/v1/exchangeReport/MI_MARGN',
        'pepb': 'https://openapi.twse.com.tw/v1/exchangeReport/BWIBBU_d',
        'stock_list': 'https://openapi.twse.com.tw/v1/opendata/t187ap03_L',
    },
    'tpex': {
        'daily': 'https://www.tpex.org.tw/openapi/v1/tpex_mainboard_daily_close_quotes',
        'daily_trading': 'https://www.tpex.org.tw/web/stock/aftertrading/daily_trading_info/st43_result.php',
        'institutional': 'https://www.tpex.org.tw/openapi/v1/tpex_3insti_daily_trading',
        'margin': 'https://www.tpex.org.tw/openapi/v1/tpex_mainboard_margin_balance',
        'pepb': 'https://www.tpex.org.tw/openapi/v1/tpex_mainboard_peratio_analysis',
        'stock_list': 'https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap03_O',
    },
    'tdcc': {
        'shareholder': 'https://smart.tdcc.com.tw/opendata/getOD.ashx?id=1-5',
    }
}

# å–å¾— API ç«¯é»çš„ä¾¿åˆ©å‡½æ•¸
def get_api_url(market, endpoint):
    """è¡¨é©…å‹•æ³•: å–å¾— API ç«¯é» URL"""
    return API_ENDPOINTS.get(market, {}).get(endpoint, '')

# å‘å¾Œç›¸å®¹çš„åˆ¥å (èˆŠç¨‹å¼ç¢¼ç”¨)
FINMIND_URL = API_ENDPOINTS['finmind']['base']
TWSE_BWIBBU_URL = API_ENDPOINTS['twse']['bwibbu']
TWSE_STOCK_DAY_ALL_URL = API_ENDPOINTS['twse']['daily_all']
TPEX_MAINBOARD_URL = API_ENDPOINTS['tpex']['daily']
TWSE_STOCK_DAY_URL = API_ENDPOINTS['twse']['daily']
TPEX_DAILY_TRADING_URL = API_ENDPOINTS['tpex']['daily_trading']

# ==============================
# è¡¨é©…å‹•æ³• - SQL æŸ¥è©¢æ¨¡æ¿
# ==============================
QUERY_TEMPLATES = {
    'get_latest_date': "SELECT MAX(date_int) FROM stock_history WHERE code = ?",
    'get_stock_history': """
        SELECT date_int, open, high, low, close, volume, amount
        FROM stock_history WHERE code = ? ORDER BY date_int DESC LIMIT ?
    """,
    'get_all_stocks': "SELECT code, name FROM stock_meta WHERE is_normal = 1",
    'get_snapshot': "SELECT * FROM stock_snapshot WHERE code = ?",
    'get_institutional': """
        SELECT date_int, foreign_buy, foreign_sell, trust_buy, trust_sell, 
               dealer_buy, dealer_sell
        FROM institutional_investors 
        WHERE code = ? ORDER BY date_int DESC LIMIT ?
    """,
    'count_stocks': "SELECT COUNT(*) FROM stock_meta",
    'get_latest_market_date': "SELECT MAX(date_int) FROM stock_history",
}

# ==============================
# è¡¨é©…å‹•æ³• - SQL å¯«å…¥æ¨¡æ¿
# ==============================
SQL_UPSERT_TEMPLATES = {
    'history_upsert': """
        INSERT OR REPLACE INTO stock_history 
        (code, date_int, open, high, low, close, volume, amount)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """,
    'snapshot_upsert': """
        INSERT OR REPLACE INTO stock_snapshot
        (code, date_int, close, volume, ma5, ma20, ma60, ma120, ma200, 
         rsi, mfi14, smart_score)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
    """,
    'institutional_upsert': """
        INSERT OR REPLACE INTO institutional_investors
        (code, date_int, foreign_buy, foreign_sell, trust_buy, trust_sell,
         dealer_buy, dealer_sell)
        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
    """,
    'meta_upsert': """
        INSERT OR REPLACE INTO stock_meta (code, name, market, is_normal)
        VALUES (?, ?, ?, ?)
    """,
}

# ==============================
# 2025 å¹´å°è‚¡ä¼‘å¸‚æ—¥ (ç”¨æ–¼è·³éè£œæ¼)
# ==============================
MARKET_HOLIDAYS_2025 = {
    20250101,  # å…ƒæ—¦
    20250127, 20250128, 20250129, 20250130, 20250131,  # è¾²æ›†æ˜¥ç¯€
    20250203, 20250204,  # è¾²æ›†æ˜¥ç¯€
    20250228,  # å’Œå¹³ç´€å¿µæ—¥
    20250303, 20250304,  # å’Œå¹³ç´€å¿µæ—¥èª¿æ•´
    20250404,  # æ¸…æ˜ç¯€
    20250501,  # å‹å‹•ç¯€
    20250530, 20250531,  # ç«¯åˆç¯€
    20251006,  # ä¸­ç§‹ç¯€
    20251010,  # åœ‹æ…¶æ—¥
    20251024,  # å°ç£å…‰å¾©ç¯€è£œå‡
}

def is_market_holiday(date_int):
    """æª¢æŸ¥æ˜¯å¦ç‚ºä¼‘å¸‚æ—¥"""
    return date_int in MARKET_HOLIDAYS_2025


# é›²ç«¯åŒæ­¥è¨­å®š
SUPABASE_URL = "https://gqiyvefcldxslrqpqlri.supabase.co"
SUPABASE_KEY = "sb_publishable_yXSGYxyxPMaoVu4MbGK5Vw_IuZsl5yu"
ENABLE_CLOUD_SYNC = False # bool(SUPABASE_URL and SUPABASE_KEY)

# å…¨åŸŸå¿«å–ï¼ˆå»¶é²åˆå§‹åŒ–ï¼‰


# é¡è‰²è¨­å®š
COFFEE_COLOR = '\033[38;5;130m'
RESET_COLOR = '\033[0m'


def _worker_calc_indicators(args):
    """Step 7 Worker: è¨ˆç®—å–®æ”¯è‚¡ç¥¨æŒ‡æ¨™"""
    code, name, preloaded_df = args
    try:
        # è¨ˆç®—æŒ‡æ¨™ (ä½¿ç”¨é è¼‰å…¥çš„ DataFrame)
        indicators_list = calculate_stock_history_indicators(
            code, 
            display_days=1, 
            limit_days=Config.CALC_LOOKBACK_DAYS, 
            conn=None, 
            preloaded_df=preloaded_df
        )
        
        if not indicators_list:
            return None
            
        # å–å¾—æœ€æ–°ä¸€ç­†è³‡æ–™
        latest = indicators_list[0]
        
        # å»ºæ§‹æ›´æ–° Tuple (å¿…é ˆèˆ‡ SQL UPDATE é †åºå®Œå…¨ä¸€è‡´)
        return (
            latest.get('MA3'), latest.get('MA20'), latest.get('MA60'), latest.get('MA120'), latest.get('MA200'),
            latest.get('WMA3'), latest.get('WMA20'), latest.get('WMA60'), latest.get('WMA120'), latest.get('WMA200'),
            latest.get('MFI'), latest.get('VWAP'), latest.get('CHG14'), latest.get('RSI'), latest.get('MACD'), latest.get('SIGNAL'),
            latest.get('POC'), latest.get('VP_upper'), latest.get('VP_lower'),
            latest.get('Month_K'), latest.get('Month_D'),
            latest.get('Daily_K'), latest.get('Daily_D'),
            latest.get('Week_K'), latest.get('Week_D'),
            latest.get('MA3_prev'), latest.get('MA20_prev'), latest.get('MA60_prev'), latest.get('MA120_prev'), latest.get('MA200_prev'),
            latest.get('WMA3_prev'), latest.get('WMA20_prev'), latest.get('WMA60_prev'), latest.get('WMA120_prev'), latest.get('WMA200_prev'),
            latest.get('MFI_prev'), latest.get('VWAP_prev'), latest.get('CHG14_prev'),
            latest.get('Month_K_prev'), latest.get('Month_D_prev'),
            latest.get('Daily_K_prev'), latest.get('Daily_D_prev'),
            latest.get('Week_K_prev'), latest.get('Week_D_prev'),
            latest.get('close_prev'), latest.get('vol_prev'),
            latest.get('SMI'), latest.get('SVI'), latest.get('NVI'), latest.get('PVI'), latest.get('clv'),
            latest.get('Smart_Score'), latest.get('SMI_Signal'), latest.get('SVI_Signal'), latest.get('NVI_Signal'), latest.get('VSA_Signal'),
            latest.get('SMI_Signal_prev'), latest.get('SVI_Signal_prev'), latest.get('NVI_Signal_prev'), latest.get('Smart_Score_prev'),
            latest.get('Vol_Div_Signal'), latest.get('Weekly_NVI_Signal'),
            latest.get('Div_3Day_Bull'), latest.get('Div_3Day_Bear'),
            latest.get('Vol_MA3'), latest.get('pvi_prev'),
            latest.get('VWAP60'), latest.get('BBW'), latest.get('Fib_0618'),
            latest.get('Weekly_Close'), latest.get('Weekly_Open'),
            latest.get('Monthly_Close'), latest.get('Monthly_Open'),
            latest.get('VWAP200'), latest.get('Mansfield_RS'),
            latest.get('ADL'), latest.get('RS'),
            code # WHERE code=?
        )
    except Exception:
        return None


def batch_load_history(codes, limit_days=400, conn=None):
    """æ‰¹æ¬¡è¼‰å…¥å¤šæ”¯è‚¡ç¥¨çš„æ­·å²è³‡æ–™ (å„ªåŒ–ç‰ˆ - ç›´æ¥é€£ç·š)"""
    if not codes:
        return {}
    
    # è¨ˆç®—æˆªæ­¢æ—¥æœŸ
    cutoff_date = (datetime.now() - timedelta(days=730)).strftime("%Y%m%d")
    cutoff_int = int(cutoff_date)
    
    placeholders = ','.join(['?'] * len(codes))
    query = f"""
        SELECT 
            code,
            CAST(date_int/10000 AS TEXT) || '-' || 
            SUBSTR('0'||CAST((date_int/100)%100 AS TEXT),-2) || '-' ||
            SUBSTR('0'||CAST(date_int%100 AS TEXT),-2) as date,
            date_int,
            open, high, low, close, volume, amount
        FROM stock_history 
        WHERE code IN ({placeholders}) AND date_int >= ?
        ORDER BY code, date_int ASC
    """
    
    params = list(codes) + [cutoff_int]
    should_close = False
    
    try:
        if conn is None:
            # ç›´æ¥å»ºç«‹é€£ç·šï¼Œé¿é–‹ db_manager å¯èƒ½çš„å•é¡Œ
            conn = sqlite3.connect(DB_FILE)
            should_close = True
            
        df_all = pd.read_sql_query(query, conn, params=params)
            
    except Exception as e:
        print_flush(f"æ‰¹æ¬¡è¼‰å…¥å¤±æ•—: {e}")
        return {}
    finally:
        if should_close and conn:
            conn.close()
    
    result = {}
    if not df_all.empty:
        df_all['date'] = pd.to_datetime(df_all['date'])
        groups = list(df_all.groupby('code'))
        for i, (code, group) in enumerate(groups):
            result[code] = group.reset_index(drop=True)
            
    return result


# ==============================
# Phase 6: è¡›èªå¥ - é€šç”¨é©—è­‰å·¥å…·
# ==============================
def validate_dataframe(df, min_rows: int = 1, required_cols: List[str] = None) -> bool:
    """
    é©—è­‰ DataFrame (è¡›èªå¥è¼”åŠ©å·¥å…·)
    
    ä½¿ç”¨æ–¹å¼ï¼š
        if not validate_dataframe(df, min_rows=20, required_cols=['close', 'volume']):
            return None
    
    Args:
        df: è¦é©—è­‰çš„ DataFrame
        min_rows: æœ€å°è¡Œæ•¸
        required_cols: å¿…è¦æ¬„ä½åˆ—è¡¨
        
    Returns:
        bool: True=é©—è­‰é€šé, False=é©—è­‰å¤±æ•—
    """
    if df is None:
        return False
    if not isinstance(df, pd.DataFrame):
        return False
    if df.empty:
        return False
    if len(df) < min_rows:
        return False
    if required_cols:
        for col in required_cols:
            if col not in df.columns:
                return False
    return True


def validate_code(code) -> bool:
    """
    é©—è­‰è‚¡ç¥¨ä»£ç¢¼ (è¡›èªå¥è¼”åŠ©å·¥å…·)
    
    Args:
        code: è‚¡ç¥¨ä»£ç¢¼
        
    Returns:
        bool: True=é©—è­‰é€šé, False=é©—è­‰å¤±æ•—
    """
    if not code:
        return False
    if not isinstance(code, str):
        return False
    if len(code) < 4:
        return False
    return True


# ==============================
# Phase 2: è¡›èªå¥ - å®‰å…¨å·¥å…·å‡½æ•¸
# ==============================
def safe_api_request(url, headers=None, timeout=30, method='GET', params=None):
    """
    å®‰å…¨çš„ API è«‹æ±‚ (è¡›èªå¥æ¨¡å¼)
    æˆåŠŸå›å‚³ Responseï¼Œå¤±æ•—å›å‚³ None
    """
    if not url:
        return None
    
    default_headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'application/json'
    }
    headers = headers or default_headers
    
    try:
        if method.upper() == 'GET':
            resp = requests.get(url, headers=headers, params=params, timeout=timeout, verify=False)
        else:
            resp = requests.post(url, headers=headers, data=params, timeout=timeout, verify=False)
        
        if resp.status_code != 200:
            logger.warning(f"API å›æ‡‰é 200: {url} -> {resp.status_code}")
            return None
        
        return resp
    except requests.exceptions.Timeout:
        logger.warning(f"API é€¾æ™‚: {url}")
        return None
    except requests.exceptions.ConnectionError:
        logger.warning(f"API é€£ç·šå¤±æ•—: {url}")
        return None
    except Exception as e:
        logger.error(f"API è«‹æ±‚ç•°å¸¸: {url} -> {e}")
        return None


def safe_json_parse(response, default=None):
    """
    å®‰å…¨çš„ JSON è§£æ (è¡›èªå¥æ¨¡å¼)
    æˆåŠŸå›å‚³è³‡æ–™ï¼Œå¤±æ•—å›å‚³ default
    """
    if response is None:
        return default
    
    try:
        text = response.text.strip()
        if not text:
            return default
        return response.json()
    except Exception as e:
        logger.warning(f"JSON è§£æå¤±æ•—: {e}")
        return default


def get_nested(obj, *keys, default=None):
    """
    å®‰å…¨å–å¾—å·¢ç‹€ç‰©ä»¶å€¼ (é¡ä¼¼ Optional Chaining)
    ç”¨æ³•: get_nested(data, 'user', 'address', 'city', default='Unknown')
    """
    for key in keys:
        if obj is None:
            return default
        if isinstance(obj, dict):
            obj = obj.get(key)
        elif isinstance(obj, (list, tuple)) and isinstance(key, int):
            obj = obj[key] if 0 <= key < len(obj) else None
        else:
            obj = getattr(obj, key, None)
    return obj if obj is not None else default


# ==============================
# Phase 6: å¤šç·šç¨‹ä¸¦è¡Œå·¥å…·
# ==============================
def run_parallel_tasks(tasks, max_workers=None, show_progress=True, silent_execution=False):
    """
    ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹ä»»å‹™ (æ–¹æ¡ˆ A: API ç¾¤çµ„ä¸¦è¡Œ)
    
    :param tasks: [(func, args, kwargs, name, label), ...]
                  label ç‚ºå¯é¸çš„æ­¥é©Ÿæ¨™ç±¤ (å¦‚ "3.5", "3.6")
    :param max_workers: æœ€å¤§ç·šç¨‹æ•¸ (é è¨­ä½¿ç”¨ Config.MAX_WORKERSï¼Œæ‰‹æ©Ÿ=2, é›»è…¦=6)
    :param show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦
    :param silent_execution: æ˜¯å¦éœé»˜åŸ·è¡Œ (æŠ‘åˆ¶å­ä»»å‹™è¼¸å‡ºï¼Œæœ€å¾Œçµ±ä¸€é¡¯ç¤º)
    :return: {name: result}
    """
    # [Phase 7] ç’°å¢ƒè‡ªé©æ‡‰ï¼šä½¿ç”¨ Config.MAX_WORKERS
    if max_workers is None:
        max_workers = Config.MAX_WORKERS
    import io
    import sys
    
    results = {}
    task_results = {}  # å„²å­˜æ ¼å¼åŒ–çµæœ
    completed = 0
    total = len(tasks)
    
    # ä»»å‹™åç¨±å°æ‡‰æ¨™ç±¤
    task_labels = {}
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_info = {}
        for task in tasks:
            func = task[0]
            args = task[1] if len(task) > 1 else ()
            kwargs = task[2] if len(task) > 2 else {}
            name = task[3] if len(task) > 3 else func.__name__
            label = task[4] if len(task) > 4 else ""
            
            task_labels[name] = label
            future = executor.submit(func, *args, **kwargs)
            future_to_info[future] = name
        
        for future in as_completed(future_to_info):
            name = future_to_info[future]
            label = task_labels.get(name, "")
            completed += 1
            try:
                result = future.result()
                results[name] = result
                
                # æ ¼å¼åŒ–è¼¸å‡º
                if show_progress:
                    label_str = f"[{label}] " if label else ""
                    # å˜—è©¦å¾çµæœä¸­æå–æ•¸é‡
                    if isinstance(result, int):
                        count_str = f"{result} ç­†" if result > 0 else "è·³é"
                    elif isinstance(result, (set, list)):
                        count_str = f"{len(result)} ç­†"
                    elif result is None:
                        count_str = "å®Œæˆ"
                    else:
                        count_str = "âœ“"
                    
                    print_flush(f"  {label_str}{name:<12}: âœ“ {count_str}")
                    
            except Exception as e:
                logger.error(f"ä¸¦è¡Œä»»å‹™å¤±æ•— [{name}]: {e}")
                results[name] = None
                if show_progress:
                    label_str = f"[{label}] " if label else ""
                    print_flush(f"  {label_str}{name:<12}: âš  å¤±æ•—")
    
    return results


def fetch_both_markets_parallel(twse_func, tpex_func, twse_name='TWSE', tpex_name='TPEx'):
    """
    ä¸¦è¡Œç²å– TWSE + TPEx è³‡æ–™ (æ–¹æ¡ˆ B: å…§éƒ¨ä¸¦è¡Œ)
    
    :param twse_func: TWSE ç²å–å‡½æ•¸
    :param tpex_func: TPEx ç²å–å‡½æ•¸
    :return: åˆä½µå¾Œçš„çµæœåˆ—è¡¨
    """
    results = []
    
    with ThreadPoolExecutor(max_workers=2) as executor:
        future_twse = executor.submit(twse_func)
        future_tpex = executor.submit(tpex_func)
        
        try:
            twse_data = future_twse.result()
            if twse_data:
                results.extend(twse_data)
        except Exception as e:
            logger.warning(f"{twse_name} ç²å–å¤±æ•—: {e}")
        
        try:
            tpex_data = future_tpex.result()
            if tpex_data:
                results.extend(tpex_data)
        except Exception as e:
            logger.warning(f"{tpex_name} ç²å–å¤±æ•—: {e}")
    
    return results


def safe_float_preserving_none(value, default=None):
    """å¼·åŒ–ç‰ˆ Null è™•ç† (è™•ç†æ‰€æœ‰é‚Šç•Œæƒ…æ³)"""
    if value is None:
        return default
    
    # è™•ç† bytes é¡å‹ (SQLite INTEGER å›å‚³)
    if isinstance(value, bytes):
        try:
            value = int.from_bytes(value, byteorder='little', signed=True)
        except (ValueError, OverflowError):
            return default
    
    # è™•ç† NaN
    if isinstance(value, float) and (np.isnan(value) or np.isinf(value)):
        return default
    
    # è™•ç†å­—ä¸²é¡å‹
    if isinstance(value, str):
        value = value.strip().replace(',', '')
        if value in ('', '--', 'N/A'):
            return default
    
    try:
        return float(value)
    except (ValueError, TypeError, OverflowError):
        return default

def safe_num(value, default=None):
    """å®‰å…¨è½‰æ›æ•¸å€¼ (Alias for safe_float_preserving_none)"""
    return safe_float_preserving_none(value, default)

def safe_int(value, default=0):
    """å¼·åŒ–ç‰ˆæ•´æ•¸è™•ç†"""
    result = safe_float_preserving_none(value, default)
    if result is None:
        return default
    try:
        return int(result)
    except (ValueError, TypeError, OverflowError):
        return default

def safe_json_parse(text):
    """å®‰å…¨è§£æ JSON"""
    try:
        return json.loads(text)
    except:
        return None

def roc_to_western_date(roc_date_str):
    """æ°‘åœ‹æ—¥æœŸè½‰è¥¿å…ƒæ—¥æœŸ"""
    if pd.isna(roc_date_str) or roc_date_str is None:
        return "1970-01-01"
    
    roc_date_str = str(roc_date_str).strip()
    
    try:
        if len(roc_date_str) == 7 and roc_date_str.isdigit():
            y = int(roc_date_str[:3]) + 1911
            m = int(roc_date_str[3:5])
            d = int(roc_date_str[5:])
            return f"{y}-{m:02d}-{d:02d}"
        
        parts = re.split(r'[/-]', roc_date_str)
        if len(parts) == 3:
            return f"{int(parts[0])+1911}-{int(parts[1]):02d}-{int(parts[2]):02d}"
    except:
        pass
    
    return roc_date_str

def convert_numeric_columns(df):
    """å°‡å­—ä¸²æ•¸å­—æ¬„ä½è½‰æ›ç‚ºæ•¸å€¼å‹æ…‹"""
    numeric_cols = ['æˆäº¤è‚¡æ•¸', 'æˆäº¤é‡‘é¡', 'æˆäº¤ç­†æ•¸', 'é–‹ç›¤åƒ¹', 'æœ€é«˜åƒ¹', 'æœ€ä½åƒ¹', 'æ”¶ç›¤åƒ¹', 'æ¼²è·Œåƒ¹å·®']
    
    for col in numeric_cols:
        if col in df.columns:
            # ç§»é™¤åƒåˆ†ä½é€—è™Ÿä¸¦è½‰æ›ç‚ºæ•¸å€¼
            df[col] = df[col].astype(str).str.replace(',', '').str.replace('--', '0').str.replace('X', '0')
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df

def convert_dates_to_western(df):
    """å°‡æ°‘åœ‹æ—¥æœŸè½‰æ›ç‚ºè¥¿å…ƒæ—¥æœŸ"""
    if 'æ—¥æœŸ' in df.columns:
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(roc_to_western_date)
    return df

def standardize_dataframe(df, source, stock_code):
    """å°‡ DataFrame æ¬„ä½æ¨™æº–åŒ–"""
    column_mapping = {
        'æ—¥æœŸ': 'date',
        'é–‹ç›¤åƒ¹': 'open',
        'æœ€é«˜åƒ¹': 'high',
        'æœ€ä½åƒ¹': 'low',
        'æ”¶ç›¤åƒ¹': 'close',
        'æˆäº¤è‚¡æ•¸': 'volume',
        'æˆäº¤é‡‘é¡': 'amount'
    }
    
    # é‡æ–°å‘½åæ¬„ä½
    df = df.rename(columns=column_mapping)
    
    # åªä¿ç•™éœ€è¦çš„æ¬„ä½
    keep_cols = ['date', 'open', 'high', 'low', 'close', 'volume', 'amount']
    df = df[[col for col in keep_cols if col in df.columns]]
    
    # è¨­ç½®æ—¥æœŸç‚ºç´¢å¼•
    if 'date' in df.columns:
        df['date'] = pd.to_datetime(df['date'], errors='coerce')
        df = df.dropna(subset=['date'])
        df = df.set_index('date')
    
    # ç§»é™¤ç„¡æ•ˆè³‡æ–™
    if 'close' in df.columns:
        df = df[df['close'] > 0]
    
    return df

def print_flush(s="", end="\n"):
    """ç«‹å³è¼¸å‡ºä¸¦åˆ·æ–°ç·©è¡å€ (è™•ç†ç·¨ç¢¼å•é¡Œ)"""
    try:
        print(s, end=end)
    except UnicodeEncodeError:
        # ç§»é™¤ç„¡æ³•ç·¨ç¢¼çš„å­—å…ƒå¾Œé‡è©¦
        safe_s = s.encode(sys.stdout.encoding or 'utf-8', errors='replace').decode(sys.stdout.encoding or 'utf-8', errors='replace')
        print(safe_s, end=end)
    sys.stdout.flush()


# ==============================
# çµ±ä¸€è¼¸å‡ºæ ¼å¼é¡åˆ¥
# ==============================
class StepOutput:
    """çµ±ä¸€æ­¥é©Ÿè¼¸å‡ºæ ¼å¼"""
    
    # é¡è‰²ä»£ç¢¼
    GREEN = '\033[92m'
    RED = '\033[91m'
    YELLOW = '\033[93m'
    BLUE = '\033[94m'
    CYAN = '\033[96m'
    RESET = '\033[0m'
    BOLD = '\033[1m'
    
    @staticmethod
    def header(title, step_num=None):
        """è¼¸å‡ºæ­¥é©Ÿæ¨™é¡Œ"""
        if step_num:
            print_flush(f"\n{StepOutput.CYAN}[Step {step_num}]{StepOutput.RESET} {StepOutput.BOLD}{title}{StepOutput.RESET}")
        else:
            print_flush(f"\n{StepOutput.BOLD}{title}{StepOutput.RESET}")
    
    @staticmethod
    def success(msg, indent=0):
        """æˆåŠŸè¨Šæ¯"""
        prefix = "  " * indent
        print_flush(f"{prefix}{StepOutput.GREEN}âœ“{StepOutput.RESET} {msg}")
    
    @staticmethod
    def warn(msg, indent=0):
        """è­¦å‘Šè¨Šæ¯"""
        prefix = "  " * indent
        print_flush(f"{prefix}{StepOutput.YELLOW}âš {StepOutput.RESET} {msg}")
    
    @staticmethod
    def error(msg, indent=0):
        """éŒ¯èª¤è¨Šæ¯"""
        prefix = "  " * indent
        print_flush(f"{prefix}{StepOutput.RED}âœ—{StepOutput.RESET} {msg}")
    
    @staticmethod
    def info(msg, indent=0):
        """ä¸€èˆ¬è¨Šæ¯"""
        prefix = "  " * indent
        print_flush(f"{prefix}{msg}")
    
    @staticmethod
    def progress(current, total, desc=""):
        """é€²åº¦é¡¯ç¤º"""
        pct = current / total * 100 if total > 0 else 0
        bar_len = 30
        filled = int(bar_len * current / total) if total > 0 else 0
        bar = "â–ˆ" * filled + "â–‘" * (bar_len - filled)
        print_flush(f"\r  [{bar}] {pct:5.1f}% {desc}", end="")
        if current >= total:
            print_flush()  # æ›è¡Œ
    
    @staticmethod
    def separator(char="â”€", width=50):
        """åˆ†éš”ç·š"""
        print_flush(char * width)
    
    @staticmethod
    def box_start(title):
        """é–‹å§‹æ¡†"""
        print_flush(f"\n{'â•' * 60}")
        print_flush(f"ğŸ“Š {title}")
        print_flush(f"{'â•' * 60}")
    
    @staticmethod
    def box_end(msg="å®Œæˆ"):
        """çµæŸæ¡†"""
        print_flush(f"\n{'â•' * 60}")
        print_flush(f"âœ“ {msg}")
        print_flush(f"{'â•' * 60}")
    
    @staticmethod
    def table_row(cols, widths=None):
        """è¡¨æ ¼è¡Œ"""
        if widths is None:
            widths = [12] * len(cols)
        row = " | ".join(f"{str(c):<{w}}" for c, w in zip(cols, widths))
        print_flush(f"  {row}")


def read_single_key(prompt="è«‹é¸æ“‡: "):
    """è®€å–å–®ä¸€æŒ‰éµ (æ”¯æ´ Windows/Linux)"""
    print(prompt, end='', flush=True)
    
    # æ”¯æ´è‡ªå‹•åŒ–æ¸¬è©¦ (é TTY ç’°å¢ƒ)
    if not sys.stdin.isatty():
        try:
            s = sys.stdin.readline().strip()
            if len(s) > 0:
                return s[0]
        except:
            pass
    
    if HAS_TERMIOS:
        fd = sys.stdin.fileno()
        old = termios.tcgetattr(fd)
        try:
            tty.setraw(sys.stdin.fileno())
            ch = sys.stdin.read(1)
        finally:
            termios.tcsetattr(fd, termios.TCSADRAIN, old)
        print()
        return ch
    else:
        import msvcrt
        while True:
            try:
                ch = msvcrt.getch()
                # Skip special keys
                if ch in [b'\x00', b'\xe0']:
                    msvcrt.getch()
                    continue
                
                try:
                    decoded = ch.decode('utf-8')
                except:
                    continue
                    
                if decoded:
                    print(decoded)
                    return decoded
            except:
                continue

def get_display_limit(default=30):
    """ç²å–é¡¯ç¤ºæª”æ•¸é™åˆ¶"""
    try:
        limit = input(f"è«‹è¼¸å…¥é¡¯ç¤ºæª”æ•¸ (é è¨­{default}): ").strip()
        return int(limit) if limit.isdigit() and int(limit) > 0 else default
    except:
        return default

def get_volume_limit(default=500):
    """ç²å–æˆäº¤é‡é™åˆ¶"""
    try:
        limit = input(f"è«‹è¼¸å…¥æœ€å°æˆäº¤é‡(å¼µ) (é è¨­{default}): ").strip()
        return int(limit) * 1000 if limit.isdigit() else default * 1000
    except:
        return default * 1000

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                       INFRA/DB                                â•‘
# â•‘  DatabaseManager / SingleWriterDBManager / ProxyConnection    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ==============================
# åŸºç¤è¨­æ–½å±¤
# ==============================
class DatabaseManager:
    _instance = None
    _pool_size = 5
    _connection_pool = None
    _pool_lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(DatabaseManager, cls).__new__(cls)
            cls._instance._init_pool()
            cls._instance._remove_stale_locks()
        return cls._instance
    
    def _init_pool(self):
        """åˆå§‹åŒ–é€£ç·šæ±  (Lazy Initialization)"""
        with self._pool_lock:
            if self._connection_pool is None:
                self._connection_pool = queue.Queue(maxsize=self._pool_size)
                for _ in range(self._pool_size):
                    conn = self._create_connection()
                    self._connection_pool.put(conn)
    
    def _create_connection(self):
        """å»ºç«‹å–®ä¸€é€£ç·š (DRY Principle)"""
        conn = sqlite3.connect(DB_FILE, timeout=60, check_same_thread=False)
        if not IS_ANDROID:
            conn.execute("PRAGMA journal_mode=WAL;")
        else:
            conn.execute("PRAGMA journal_mode=DELETE;")
            conn.execute("PRAGMA busy_timeout=60000;")
        conn.execute("PRAGMA synchronous=NORMAL;")
        return conn

    def _remove_stale_locks(self):
        """ç§»é™¤ SQLite æ®˜ç•™çš„é–å®šæª”æ¡ˆ"""
        lock_files = [
            DB_FILE.with_suffix(DB_FILE.suffix + '-journal'),
            DB_FILE.with_suffix(DB_FILE.suffix + '-wal'),
            DB_FILE.with_suffix(DB_FILE.suffix + '-shm')
        ]
        
        for lock_file in lock_files:
            if lock_file.exists():
                try:
                    lock_file.unlink()
                except:
                    pass
    
    @contextmanager
    def get_connection(self, timeout=30):
        """å¾é€£ç·šæ± å–å¾—é€£ç·š (Thread-Safe)"""
        conn = None
        try:
            conn = self._connection_pool.get(timeout=timeout)
            yield conn
        except queue.Empty:
            raise sqlite3.OperationalError("é€£ç·šæ± å·²æ»¿ï¼Œè«‹ç¨å¾Œé‡è©¦")
        finally:
            if conn:
                # æ­¸é‚„é€£ç·šåˆ°æ± ä¸­ (è€Œéé—œé–‰)
                try:
                    self._connection_pool.put_nowait(conn)
                except queue.Full:
                    conn.close()  # æ± å·²æ»¿ï¼Œé—œé–‰å¤šé¤˜é€£ç·š

class IndicatorCacheManager:
    """å–®ä¾‹æ¨¡å¼ + åŸ·è¡Œç·’å®‰å…¨çš„å¿«å–ç®¡ç†å™¨"""
    _instance = None
    _lock = threading.RLock()  # ä½¿ç”¨ RLock å…è¨±é‡å…¥
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:  # Double-Checked Locking
                    cls._instance = super().__new__(cls)
                    cls._instance._data = {}
                    cls._instance._timestamp = None
                    cls._instance._cache_duration = 3600
        return cls._instance
    
    def get_data(self):
        """åŸ·è¡Œç·’å®‰å…¨è®€å–"""
        with self._lock:
            # æª¢æŸ¥å¿«å–éæœŸ
            if self._timestamp and (time.time() - self._timestamp) > self._cache_duration:
                self._data = {}
                self._timestamp = None
            return self._data.copy()  # è¿”å›å‰¯æœ¬é¿å…å¤–éƒ¨ä¿®æ”¹
    
    def set_data(self, data):
        """åŸ·è¡Œç·’å®‰å…¨å¯«å…¥"""
        with self._lock:
            self._data = data
            self._timestamp = time.time()
    
    def clear(self):
        """æ¸…é™¤å¿«å–"""
        with self._lock:
            self._data = {}
            self._timestamp = None

# å‰µå»ºå…¨å±€å¯¦ä¾‹
db_manager = DatabaseManager()
GLOBAL_INDICATOR_CACHE = IndicatorCacheManager()

# ==============================
# é€²åº¦è¿½è¹¤å™¨
# ==============================
class ProgressTracker:
    """
    å¼·å¥ç‰ˆé€²åº¦è¿½è¹¤å™¨
    - Windows VT100 æ”¯æ´
    - ç·šç¨‹å®‰å…¨
    - è‡ªå‹•é™æµ
    """
    _lock = threading.Lock()
    _last_update_time = 0
    _UPDATE_INTERVAL = 0.1  # é™åˆ¶æœ€å¤§åˆ·æ–°ç‡ç‚º 10 FPS
    
    def __init__(self, total_lines=3):
        self.total_lines = total_lines
        self._initialized = False
        self._lines_buffer = [""] * total_lines
        self._last_update_time = 0
        

    
    def __enter__(self):
        self.reset()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        # ç¢ºä¿æœ€å¾Œæ›è¡Œï¼Œé¿å…å¾ŒçºŒè¼¸å‡ºè¦†è“‹
        sys.stdout.write('\n')
        sys.stdout.flush()
    
    def update_lines(self, *messages, force=False):
        """æ›´æ–°å¤šè¡Œé€²åº¦ (VT100 ä¸‰è¡Œé€²åº¦)"""
        current_time = time.time()
        if not force and (current_time - self._last_update_time < self._UPDATE_INTERVAL):
            return
        
        with self._lock:
            # æº–å‚™å…§å®¹ï¼Œä¸è¶³è£œç©ºè¡Œ
            lines = list(messages) + [""] * (self.total_lines - len(messages))
            lines = lines[:self.total_lines]
            
            # æ›´æ–°å…§éƒ¨ç·©è¡å€ï¼Œç¢ºä¿æ··åˆèª¿ç”¨æ™‚ç‹€æ…‹ä¸€è‡´
            self._lines_buffer = lines
            
            # ä½¿ç”¨ VT100 æ¸¸æ¨™æ§åˆ¶å¯¦ç¾å¤šè¡Œé€²åº¦
            if self._initialized:
                # æ¸¸æ¨™ä¸Šç§» N è¡Œ
                sys.stdout.write(f'\033[{self.total_lines}A')
            
            for line in lines:
                # æ¸…é™¤è©²è¡Œä¸¦å¯«å…¥æ–°å…§å®¹
                # é™åˆ¶è¡Œé•·åº¦é¿å…æ›è¡Œå°è‡´è·³å‹•
                display_line = str(line)[:78] if line else ""
                sys.stdout.write(f'\033[2K\r{display_line}\n')
            
            sys.stdout.flush()
            self._initialized = True
            self._last_update_time = current_time
    
    def reset(self):
        """é‡ç½®è¿½è¹¤å™¨ç‹€æ…‹"""
        self._initialized = False
        self._lines_buffer = [""] * self.total_lines
    
    def info(self, message, level=1):
        """é¡¯ç¤ºä¸€èˆ¬è¨Šæ¯"""
        self._update_single_line(message, level)
    
    def warning(self, message, level=1):
        """é¡¯ç¤ºè­¦å‘Šè¨Šæ¯"""
        self._update_single_line(f"âš  {message}", level)
    
    def success(self, message, level=1):
        """é¡¯ç¤ºæˆåŠŸè¨Šæ¯"""
        self._update_single_line(f"âœ“ {message}", level)
    
    def error(self, message, level=1):
        """é¡¯ç¤ºéŒ¯èª¤è¨Šæ¯"""
        self._update_single_line(f"âŒ {message}", level)
    
    def _update_single_line(self, message, level):
        """æ›´æ–°å–®è¡Œå…§å®¹ä¸¦åˆ·æ–°é¡¯ç¤º"""
        # å‹•æ…‹é™åˆ¶ç´¢å¼•ç¯„åœ
        idx = max(0, min(level - 1, self.total_lines - 1))
        self._lines_buffer[idx] = message
        self.update_lines(*self._lines_buffer)

# ==============================
# é€²åº¦è¿½è¹¤å‡½æ•¸
# ==============================
def load_progress():
    """è¼‰å…¥é€²åº¦è¿½è¹¤ç³»çµ±"""
    try:
        if PROGRESS_FILE.exists():
            with open(PROGRESS_FILE, 'r', encoding='utf-8') as f:
                progress = json.load(f)
                # ç¢ºä¿æ‰€æœ‰å¿…è¦çš„éµå­˜åœ¨
                default_progress = {
                    "last_code_index": 0,
                    "missing_stocks": [],
                    "new_stocks": [],
                    "failed_stocks": [],
                    "in_progress": None,
                    "stock_list_last_idx": 0,
                    "stock_list_processed": [],
                    "calc_last_idx": 0,
                    "timestamp": datetime.now().isoformat()
                }
                
                for key, default_value in default_progress.items():
                    if key not in progress:
                        progress[key] = default_value
                
                return progress
    except Exception as e:
        print_flush(f"âš  ç„¡æ³•è¼‰å…¥é€²åº¦æª”: {e}")
    
    return {
        "last_code_index": 0,
        "missing_stocks": [],
        "new_stocks": [],
        "failed_stocks": [],
        "in_progress": None,
        "stock_list_last_idx": 0,
        "stock_list_processed": [],
        "calc_last_idx": 0,
        "timestamp": datetime.now().isoformat()
    }

def save_progress(last_idx=None, missing_stocks=None, new_stocks=None, failed_stocks=None, in_progress=None,
                  stock_list_last_idx=None, stock_list_processed=None, calc_last_idx=None):
    """å„²å­˜é€²åº¦è¿½è¹¤ç³»çµ±"""
    try:
        # è¼‰å…¥ç¾æœ‰é€²åº¦
        current = load_progress()
        
        # æ›´æ–°é€²åº¦ï¼ˆåªæ›´æ–°æä¾›çš„åƒæ•¸ï¼‰
        progress = {
            "last_code_index": last_idx if last_idx is not None else current.get("last_code_index", 0),
            "missing_stocks": list(set(missing_stocks)) if missing_stocks is not None else current.get("missing_stocks", []),
            "new_stocks": list(set(new_stocks)) if new_stocks is not None else current.get("new_stocks", []),
            "failed_stocks": list(set(failed_stocks)) if failed_stocks is not None else current.get("failed_stocks", []),
            "in_progress": in_progress if in_progress is not None else current.get("in_progress"),
            "stock_list_last_idx": stock_list_last_idx if stock_list_last_idx is not None else current.get("stock_list_last_idx", 0),
            "stock_list_processed": stock_list_processed if stock_list_processed is not None else current.get("stock_list_processed", []),
            "calc_last_idx": calc_last_idx if calc_last_idx is not None else current.get("calc_last_idx", 0),
            "timestamp": datetime.now().isoformat()
        }
        
        with open(PROGRESS_FILE, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print_flush(f"âš  ç„¡æ³•å„²å­˜é€²åº¦æª”: {e}")

def clear_progress():
    """æ¸…é™¤é€²åº¦æª” (å®Œæˆæ™‚å‘¼å«)"""
    try:
        if PROGRESS_FILE.exists():
            PROGRESS_FILE.unlink()
    except Exception as e:
        print_flush(f"âš  ç„¡æ³•æ¸…é™¤é€²åº¦æª”: {e}")

def reset_progress():
    """é‡ç½®é€²åº¦æª”"""
    try:
        if PROGRESS_FILE.exists():
            PROGRESS_FILE.unlink()
    except Exception as e:
        print_flush(f"âš  ç„¡æ³•é‡ç½®é€²åº¦æª”: {e}")


# ==============================
# è³‡æ–™åº«ç®¡ç†å™¨ - å–®ä¸€å¯«å…¥å“¡æ¨¡å¼
# ==============================

@dataclass
class WriteOperation:
    """å¯«å…¥æ“ä½œå°è£"""
    query: str                              # SQL èªå¥
    params: tuple = ()                      # åƒæ•¸
    is_many: bool = False                   # æ˜¯å¦ç‚º executemany
    result_future: Optional[Future] = None  # çµæœ Future

class SingleWriterDBManager:
    """
    å–®ä¸€å¯«å…¥å“¡æ¨¡å¼è³‡æ–™åº«ç®¡ç†å™¨
    - æ‰€æœ‰å¯«å…¥æ“ä½œé€éä½‡åˆ—åºåˆ—åŒ–
    - è®€å–æ“ä½œä¿æŒä½µç™¼èƒ½åŠ›
    """
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls, db_path=None):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
                    cls._instance._initialized = False
        return cls._instance
    
    def __init__(self, db_path):
        if self._initialized:
            return
        self.db_path = db_path
        self._write_queue = queue.Queue()
        self._writer_thread = None
        self._shutdown = threading.Event()
        self._start_writer()
        self._initialized = True
    
    def _start_writer(self):
        """å•Ÿå‹•èƒŒæ™¯å¯«å…¥ç·šç¨‹"""
        self._writer_thread = threading.Thread(
            target=self._writer_loop, 
            daemon=True, 
            name="DBWriter"
        )
        self._writer_thread.start()
        logger.debug("è³‡æ–™åº«å¯«å…¥ç·šç¨‹å·²å•Ÿå‹•")
    
    def _writer_loop(self):
        """å¯«å…¥ç·šç¨‹ä¸»è¿´åœˆ - æ‰¹æ¬¡è™•ç†"""
        conn = sqlite3.connect(str(self.db_path), timeout=60)
        if not IS_ANDROID:
            conn.execute("PRAGMA journal_mode=WAL")
        else:
            conn.execute("PRAGMA journal_mode=DELETE")
            conn.execute("PRAGMA busy_timeout=60000")
        conn.execute("PRAGMA synchronous=NORMAL")
        
        while not self._shutdown.is_set():
            batch = []
            # å‹•æ…‹æ‰¹æ¬¡å¤§å°ï¼šä¾ä½‡åˆ—æ·±åº¦è‡ªå‹•èª¿æ•´
            queue_depth = self._write_queue.qsize()
            if queue_depth < 50:
                max_batch = 50       # æ·ºä½‡åˆ—ï¼šå¿«é€ŸéŸ¿æ‡‰
            elif queue_depth < 200:
                max_batch = 100      # ä¸­ç­‰ä½‡åˆ—ï¼šå¹³è¡¡
            elif queue_depth < 500:
                max_batch = 300      # æ·±ä½‡åˆ—ï¼šé«˜ååé‡
            else:
                max_batch = 500      # è¶…è¼‰ï¼šæœ€å¤§æ‰¹æ¬¡
            
            try:
                op = self._write_queue.get(timeout=0.1)
                batch.append(op)
                # å˜—è©¦æ”¶é›†æ›´å¤šæ“ä½œ (ä¾å‹•æ…‹æ‰¹æ¬¡å¤§å°)
                while len(batch) < max_batch:
                    try:
                        batch.append(self._write_queue.get_nowait())
                    except queue.Empty:
                        break
            except queue.Empty:
                continue
            
            # åŸ·è¡Œæ‰¹æ¬¡
            try:
                cursor = conn.cursor()
                for op in batch:
                    try:
                        if op.is_many:
                            cursor.executemany(op.query, op.params)
                        else:
                            cursor.execute(op.query, op.params)
                        if op.result_future and not op.result_future.done():
                            op.result_future.set_result(cursor.rowcount)
                    except Exception as e:
                        if op.result_future and not op.result_future.done():
                            op.result_future.set_exception(e)
                conn.commit()
            except Exception as e:
                logger.error(f"è³‡æ–™åº«æ‰¹æ¬¡å¯«å…¥å¤±æ•—: {e}")
                conn.rollback()
                for op in batch:
                    if op.result_future and not op.result_future.done():
                        op.result_future.set_exception(e)
        
        conn.close()
        logger.debug("è³‡æ–™åº«å¯«å…¥ç·šç¨‹å·²é—œé–‰")
    
    def execute_write(self, query, params=(), is_many=False, wait=True):
        """æäº¤å¯«å…¥æ“ä½œ"""
        future = Future() if wait else None
        op = WriteOperation(query=query, params=params, 
                           is_many=is_many, result_future=future)
        self._write_queue.put(op)
        if wait and future:
            try:
                return future.result(timeout=30)
            except Exception as e:
                logger.error(f"å¯«å…¥æ“ä½œå¤±æ•—: {e}")
                raise
        return None
    
    def get_read_connection(self, timeout=30):
        """å–å¾—è®€å–å°ˆç”¨é€£ç·š"""
        conn = sqlite3.connect(
            str(self.db_path), 
            timeout=timeout,
            check_same_thread=False
        )
        return conn
    
    def shutdown(self):
        """é—œé–‰å¯«å…¥ç·šç¨‹"""
        self._shutdown.set()
        if self._writer_thread:
            self._writer_thread.join(timeout=5)

class ProxyCursor:
    """
    æ¸¸æ¨™ä»£ç† - è‡ªå‹•åˆ¤æ–·è®€/å¯«æ“ä½œ
    ä½¿ç”¨ç‰ˆæœ¬è™Ÿè¿½è¹¤ç¢ºä¿ cursor å§‹çµ‚æœ‰æ•ˆ
    """
    _WRITE_KEYWORDS = ('INSERT', 'UPDATE', 'DELETE', 'CREATE', 'DROP', 'ALTER', 'REPLACE')
    
    def __init__(self, proxy_conn):
        self._proxy_conn = proxy_conn
        self._read_cursor = None  # Lazy initialization
        self._cursor_version = -1  # è¿½è¹¤å‰µå»ºæ­¤ cursor æ™‚çš„é€£ç·šç‰ˆæœ¬
        self._last_description = None
        self._last_rowcount = -1
    
    def _get_cursor(self):
        """å‹•æ…‹ç²å– cursor (ç‰ˆæœ¬è®Šæ›´æ™‚è‡ªå‹•é‡æ–°ç²å–)"""
        current_version = getattr(self._proxy_conn, '_conn_version', 0)
        if self._read_cursor is None or self._cursor_version != current_version:
            # é€£ç·šç‰ˆæœ¬å·²è®Šæ›´ï¼Œéœ€è¦é‡æ–°ç²å– cursor
            self._read_cursor = self._proxy_conn._read_conn.cursor()
            self._cursor_version = current_version
        return self._read_cursor
    
    def execute(self, query, params=()):
        query_upper = query.strip().upper()
        if any(query_upper.startswith(kw) for kw in self._WRITE_KEYWORDS):
            # å¯«å…¥æ“ä½œï¼šåŠ å…¥å¾…è™•ç†ä½‡åˆ—
            self._proxy_conn._pending_writes.append((query, params, False))
            self._last_rowcount = 0
        else:
            # è®€å–æ“ä½œï¼šç›´æ¥åŸ·è¡Œ
            cursor = self._get_cursor()
            cursor.execute(query, params)
            self._last_description = cursor.description
            self._last_rowcount = cursor.rowcount
        return self
    
    def executemany(self, query, params_list):
        # executemany ç¸½æ˜¯å¯«å…¥æ“ä½œ
        self._proxy_conn._pending_writes.append((query, params_list, True))
        self._last_rowcount = len(params_list) if hasattr(params_list, '__len__') else 0
        return self
    
    def fetchone(self):
        return self._get_cursor().fetchone()
    
    def fetchall(self):
        return self._get_cursor().fetchall()
    
    def fetchmany(self, size=None):
        return self._get_cursor().fetchmany(size)
    
    @property
    def description(self):
        cursor = self._get_cursor()
        return self._last_description or cursor.description
    
    @property
    def rowcount(self):
        return self._last_rowcount
    
    @property
    def lastrowid(self):
        return self._get_cursor().lastrowid
    
    def close(self):
        """é—œé–‰æ¸¸æ¨™ (å¦‚æœå­˜åœ¨)"""
        if self._read_cursor is not None:
            try:
                self._read_cursor.close()
            except:
                pass
            self._read_cursor = None

class ProxyConnection:
    """
    é€£ç·šä»£ç† - å¯¦ç¾é€æ˜çš„è®€/å¯«åˆ†é›¢
    - SELECT: ç›´æ¥åŸ·è¡Œ
    - INSERT/UPDATE/DELETE: èµ°å¯«å…¥ä½‡åˆ—
    """
    def __init__(self, manager: SingleWriterDBManager):
        self._manager = manager
        self._read_conn = sqlite3.connect(
            str(manager.db_path), 
            timeout=30,
            check_same_thread=False
        )
        self._pending_writes = []
        self._conn_version = 0  # é€£ç·šç‰ˆæœ¬è™Ÿ
    
    @property
    def row_factory(self):
        """ä»£ç† row_factory å±¬æ€§ (è®€å–)"""
        return self._read_conn.row_factory
    
    @row_factory.setter
    def row_factory(self, value):
        """ä»£ç† row_factory å±¬æ€§ (è¨­å®š)"""
        self._read_conn.row_factory = value
    
    def cursor(self):
        return ProxyCursor(self)
    
    def execute(self, query, params=()):
        cursor = ProxyCursor(self)
        cursor.execute(query, params)
        return cursor
    
    def executemany(self, query, params_list):
        cursor = ProxyCursor(self)
        cursor.executemany(query, params_list)
        return cursor
    
    def commit(self):
        """æ‰¹æ¬¡æäº¤æ‰€æœ‰å¾…è™•ç†å¯«å…¥"""
        for query, params, is_many in self._pending_writes:
            self._manager.execute_write(query, params, is_many, wait=True)
        self._pending_writes.clear()
        # é‡æ–°é–‹å•Ÿè®€å–é€£ç·šä»¥çœ‹åˆ°æ–°è³‡æ–™ (WAL mode éš”é›¢)
        try:
            self._read_conn.close()
        except:
            pass
        self._read_conn = sqlite3.connect(
            str(self._manager.db_path), 
            timeout=30,
            check_same_thread=False
        )
        self._conn_version += 1  # å¢åŠ ç‰ˆæœ¬è™Ÿ
    
    def rollback(self):
        """æ¸…é™¤å¾…è™•ç†å¯«å…¥"""
        self._pending_writes.clear()
    
    def close(self):
        try:
            self._read_conn.close()
        except:
            pass
    
    def __enter__(self):
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if exc_type is None:
            self.commit()
        else:
            self.rollback()
        self.close()

class DBManager:
    """
    è³‡æ–™åº«ç®¡ç†å™¨ - ä½¿ç”¨å–®ä¸€å¯«å…¥å“¡æ¨¡å¼
    ç›¸å®¹ç¾æœ‰ APIï¼šdb_manager.get_connection()
    """
    def __init__(self, db_path):
        self.db_path = Path(db_path) if isinstance(db_path, str) else db_path
        self._writer = SingleWriterDBManager(self.db_path)
    
    @contextmanager
    def get_connection(self, timeout=30):
        """ç›¸å®¹æ€§æ–¹æ³• - è¿”å›ä»£ç†é€£ç·š"""
        conn = ProxyConnection(self._writer)
        try:
            yield conn
        finally:
            conn.close()
    
    def execute_write(self, query, params=(), is_many=False):
        """ç›´æ¥å¯«å…¥ API (bypass proxy)"""
        return self._writer.execute_write(query, params, is_many)
    
    def get_read_connection(self, timeout=30):
        """å–å¾—è®€å–å°ˆç”¨é€£ç·š (é«˜æ•ˆèƒ½è®€å–)"""
        return self._writer.get_read_connection(timeout)
    
    def shutdown(self):
        """é—œé–‰è³‡æ–™åº«ç®¡ç†å™¨"""
        self._writer.shutdown()


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         REPO                                  â•‘
# â•‘  SnapshotRepository / HistoryRepository - DB æ“ä½œæŠ½è±¡å±¤       â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class HistoryRepository:
    """æ­·å²è³‡æ–™å­˜å–å±¤ (Phase 4)"""
    
    def __init__(self, db_mgr):
        self._db = db_mgr
    
    def get_history(self, code: str, limit: int = 400) -> pd.DataFrame:
        """è®€å–æ­·å²è³‡æ–™"""
        sql = QUERY_TEMPLATES.get('get_stock_history', 
            "SELECT date_int, open, high, low, close, volume, amount FROM stock_history WHERE code = ? ORDER BY date_int DESC LIMIT ?")
        with self._db.get_read_connection() as conn:
            return pd.read_sql_query(sql, conn, params=(code, limit))
    
    def get_latest_date(self, code: str) -> Optional[int]:
        """å–å¾—æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ"""
        sql = QUERY_TEMPLATES.get('get_latest_date',
            "SELECT MAX(date_int) FROM stock_history WHERE code = ?")
        with self._db.get_read_connection() as conn:
            cur = conn.execute(sql, (code,))
            res = cur.fetchone()
            return res[0] if res else None
    
    def upsert(self, code: str, records: List[Dict]) -> int:
        """æ‰¹é‡å¯«å…¥æ­·å²è³‡æ–™"""
        # [Guard Clause]
        if not records:
            return 0
        
        sql = SQL_UPSERT_TEMPLATES.get('history_upsert', """
            INSERT OR REPLACE INTO stock_history 
            (code, date_int, open, high, low, close, volume, amount)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """)
        params = [(code, r.get('date_int'), r.get('open'), r.get('high'), 
                   r.get('low'), r.get('close'), r.get('volume'), r.get('amount')) 
                  for r in records]
        return self._db.execute_write(sql, params, is_many=True)


class SnapshotRepository:
    """å¿«ç…§è³‡æ–™å­˜å–å±¤ (Phase 4)"""
    
    def __init__(self, db_mgr):
        self._db = db_mgr
    
    def get_snapshot(self, code: str) -> Optional[Dict]:
        """è®€å–å¿«ç…§è³‡æ–™"""
        sql = QUERY_TEMPLATES.get('get_snapshot', 
            "SELECT * FROM stock_snapshot WHERE code = ?")
        with self._db.get_read_connection() as conn:
            cur = conn.execute(sql, (code,))
            row = cur.fetchone()
            if row:
                cols = [desc[0] for desc in cur.description]
                return dict(zip(cols, row))
        return None
    
    def upsert_indicators(self, code: str, date_int: int, indicators: Dict) -> int:
        """å¯«å…¥æŒ‡æ¨™å¿«ç…§"""
        sql = """
            INSERT OR REPLACE INTO stock_snapshot
            (code, date_int, close, volume, ma5, ma20, ma60, ma120, ma200, rsi, mfi14, smart_score)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """
        params = (
            code, date_int, 
            indicators.get('close'), indicators.get('volume'),
            indicators.get('ma5'), indicators.get('ma20'), indicators.get('ma60'),
            indicators.get('ma120'), indicators.get('ma200'),
            indicators.get('rsi'), indicators.get('mfi14'), indicators.get('smart_score')
        )
        return self._db.execute_write(sql, params)


db_manager = DBManager(Config.DB_PATH)

# ==============================
# è³‡æ–™åº«åˆå§‹åŒ–
# ==============================
def ensure_db():
    """ç¢ºä¿è³‡æ–™åº«è¡¨çµæ§‹å­˜åœ¨"""
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        
        # å»ºç«‹è‚¡ç¥¨åå†Šè¡¨
        cur.execute("""
            CREATE TABLE IF NOT EXISTS stock_meta (
                code TEXT PRIMARY KEY,
                name TEXT,
                list_date TEXT,
                delist_date TEXT,
                market_type TEXT
            )
        """)
        
        # å»ºç«‹æ­·å²è¡¨
        cur.execute("""
            CREATE TABLE IF NOT EXISTS stock_history (
                code TEXT,
                date_int INTEGER,
                open REAL,
                high REAL,
                low REAL,
                close REAL,
                volume INTEGER,
                amount INTEGER,
                PRIMARY KEY (code, date_int)
            )
        """)
        
        # å»ºç«‹å¿«ç…§è¡¨
        cur.execute("""
            CREATE TABLE IF NOT EXISTS stock_snapshot (
                code TEXT PRIMARY KEY,
                name TEXT,
                date TEXT,
                close REAL,
                volume INTEGER,
                close_prev REAL,
                vol_prev INTEGER,
                ma3 REAL, ma20 REAL, ma60 REAL, ma120 REAL, ma200 REAL,
                wma3 REAL, wma20 REAL, wma60 REAL, wma120 REAL, wma200 REAL,
                mfi14 REAL, vwap20 REAL, chg14_pct REAL,
                rsi REAL, macd REAL, signal REAL,
                vp_poc REAL, vp_upper REAL, vp_lower REAL,
                month_k REAL, month_d REAL,
                daily_k REAL, daily_d REAL,
                week_k REAL, week_d REAL,
                ma3_prev REAL, ma20_prev REAL, ma60_prev REAL, ma120_prev REAL, ma200_prev REAL,
                wma3_prev REAL, wma20_prev REAL, wma60_prev REAL, wma120_prev REAL, wma200_prev REAL,
                mfi14_prev REAL, vwap20_prev REAL, chg14_pct_prev REAL,
                month_k_prev REAL, month_d_prev REAL,
                daily_k_prev REAL, daily_d_prev REAL,
                week_k_prev REAL, week_d_prev REAL,
                smi REAL, svi REAL, nvi REAL, pvi REAL, clv REAL,
                major_holders_pct REAL,
                foreign_buy INTEGER, trust_buy INTEGER, dealer_buy INTEGER,
                adl REAL, rs REAL,
                smi_signal INTEGER, svi_signal INTEGER,
                nvi_signal INTEGER, vsa_signal INTEGER,
                smart_score INTEGER,
                smi_prev REAL, svi_prev REAL, nvi_prev REAL,
                smart_score_prev INTEGER,
                vol_div_signal INTEGER, weekly_nvi_signal INTEGER,
                vwap60 REAL, bbw REAL, fib_0618 REAL,
                weekly_close REAL, weekly_open REAL,
                monthly_close REAL, monthly_open REAL,
                vwap200 REAL, mansfield_rs REAL,
                margin_balance INTEGER, margin_util_rate REAL,
                short_balance INTEGER, short_util_rate REAL,
                FOREIGN KEY (code) REFERENCES stock_meta(code)
            )
        """)
        
        # å»ºç«‹èè³‡èåˆ¸è¡¨
        cur.execute("""
            CREATE TABLE IF NOT EXISTS margin_data (
                date_int INTEGER,
                code TEXT,
                margin_buy INTEGER,
                margin_sell INTEGER,
                margin_redemp INTEGER,
                margin_balance INTEGER,
                margin_util_rate REAL,
                short_buy INTEGER,
                short_sell INTEGER,
                short_redemp INTEGER,
                short_balance INTEGER,
                short_util_rate REAL,
                PRIMARY KEY (date_int, code)
            )
        """)
        
        # å»ºç«‹å¤§ç›¤æŒ‡æ•¸è¡¨
        cur.execute("""
            CREATE TABLE IF NOT EXISTS market_index (
                date_int INTEGER,
                index_id TEXT, -- 'TAIEX', 'TPEX', 'VIX'
                close REAL,
                open REAL,
                high REAL,
                low REAL,
                volume INTEGER,
                PRIMARY KEY (date_int, index_id)
            )
        """)
        
        # å»ºç«‹ç´¢å¼•
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_meta_code ON stock_meta(code)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_history_code ON stock_history(code)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_history_date ON stock_history(date_int)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_history_code_date ON stock_history(code, date_int DESC)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_snapshot_date ON stock_snapshot(date)")
        cur.execute("CREATE INDEX IF NOT EXISTS idx_stock_snapshot_smart_score ON stock_snapshot(smart_score)")
        
        # æª¢æŸ¥ stock_snapshot æ˜¯å¦æœ‰æ–°æ¬„ä½ (Migration)
        cur.execute("PRAGMA table_info(stock_snapshot)")
        snapshot_cols = {row[1] for row in cur.fetchall()}
        
        new_snapshot_cols = [
            ("vol_div_signal", "INTEGER"),
            ("weekly_nvi_signal", "INTEGER"),
            ("div_3day_bull", "INTEGER"),
            ("div_3day_bear", "INTEGER"),
            ("vol_ma3", "REAL"),
            ("pvi_prev", "REAL"),
            ("vwap60", "REAL"),
            ("bbw", "REAL"),
            ("fib_0618", "REAL"),
            ("weekly_close", "REAL"),
            ("weekly_open", "REAL"),
            ("monthly_close", "REAL"),
            ("monthly_open", "REAL"),
            ("vwap200", "REAL"),
            ("mansfield_rs", "REAL"),
            ("margin_balance", "INTEGER"),
            ("margin_util_rate", "REAL"),
            ("short_balance", "INTEGER"),
            ("short_util_rate", "REAL"),
            ("amount", "REAL"),
            ("pe", "REAL"),
            ("yield", "REAL"),
            ("pb", "REAL")
        ]
        
        for col_name, col_type in new_snapshot_cols:
            if col_name not in snapshot_cols:
                try:
                    cur.execute(f"ALTER TABLE stock_snapshot ADD COLUMN {col_name} {col_type}")
                    print_flush(f"âœ“ å·²æ–°å¢æ¬„ä½ {col_name} åˆ° stock_snapshot")
                except Exception as e:
                    print_flush(f"âš  æ·»åŠ æ¬„ä½ {col_name} å¤±æ•—: {e}")
        
        # [å·²ç§»é™¤èˆŠæ¶æ§‹ stock_data ç›¸å®¹æ€§ä»£ç¢¼ - çµ±ä¸€ä½¿ç”¨æ–°ä¸‰è¡¨æ¶æ§‹]
        
        # åŒæ­¥æ¬„ä½åˆ° stock_snapshot (æ–°ä¸‰è¡¨æ¶æ§‹)
        columns_to_sync = [
            ("ma3", "REAL"), ("ma20", "REAL"), ("ma60", "REAL"), ("ma120", "REAL"), ("ma200", "REAL"),
            ("wma3", "REAL"), ("wma20", "REAL"), ("wma60", "REAL"), ("wma120", "REAL"), ("wma200", "REAL"),
            ("mfi14", "REAL"), ("vwap20", "REAL"), ("chg14_pct", "REAL"), 
            ("rsi", "REAL"), ("macd", "REAL"), ("signal", "REAL"),
            ("vp_poc", "REAL"), ("vp_upper", "REAL"), ("vp_lower", "REAL"),
            ("month_k", "REAL"), ("month_d", "REAL"),
            ("daily_k", "REAL"), ("daily_d", "REAL"),
            ("week_k", "REAL"), ("week_d", "REAL"),
            ("ma3_prev", "REAL"), ("ma20_prev", "REAL"), ("ma60_prev", "REAL"), ("ma120_prev", "REAL"), ("ma200_prev", "REAL"),
            ("wma3_prev", "REAL"), ("wma20_prev", "REAL"), ("wma60_prev", "REAL"), ("wma120_prev", "REAL"), ("wma200_prev", "REAL"),
            ("mfi14_prev", "REAL"), ("vwap20_prev", "REAL"), ("chg14_pct_prev", "REAL"),
            ("month_k_prev", "REAL"), ("month_d_prev", "REAL"),
            ("daily_k_prev", "REAL"), ("daily_d_prev", "REAL"),
            ("week_k_prev", "REAL"), ("week_d_prev", "REAL"),
            ("smi", "REAL"), ("svi", "REAL"), ("nvi", "REAL"), 
            ("pvi", "REAL"), ("clv", "REAL"),
            ("smi_signal", "INTEGER"), ("svi_signal", "INTEGER"), 
            ("nvi_signal", "INTEGER"), ("vsa_signal", "INTEGER"),
            ("smart_score", "INTEGER"),
            ("smi_prev", "REAL"), ("svi_prev", "REAL"), ("nvi_prev", "REAL"), 
            ("pvi_prev", "REAL"), # [Fix] Add pvi_prev
            ("smart_score_prev", "INTEGER"),
            ("div_3day_bull", "INTEGER"), ("div_3day_bear", "INTEGER"),
            ("vol_ma3", "REAL")
        ]
        
        # åŒæ­¥æ›´æ–° stock_snapshot çš„æ¬„ä½
        cur.execute("PRAGMA table_info(stock_snapshot)")
        snapshot_columns = {row[1] for row in cur.fetchall()}
        
        for col_name, col_type in columns_to_sync:
            if col_name not in snapshot_columns:
                try:
                    print_flush(f"   -> Adding column to stock_snapshot: {col_name} ({col_type})...")
                    cur.execute(f"ALTER TABLE stock_snapshot ADD COLUMN {col_name} {col_type}")
                    print_flush(f"      âœ“ Added {col_name} to snapshot")
                except Exception as e:
                    print_flush(f"âš  æ·»åŠ  snapshot æ¬„ä½ {col_name} å¤±æ•—: {e}")

        conn.commit()



# ==============================
# æ ¸å¿ƒé‚è¼¯å‡½æ•¸
# ==============================
def is_normal_stock(code, name):
    """Aè¦å‰‡: æª¢æŸ¥æ˜¯å¦ç‚ºæ™®é€šè‚¡ - åš´æ ¼ç‰ˆæœ¬"""
    if not code or not name:
        return False
    
    c = str(code).strip()
    
    # åš´æ ¼: åªæ¥å—4ä½æ•¸å­—ä»£ç¢¼
    if len(c) != 4:
        return False
    
    # å¿…é ˆå…¨éƒ¨æ˜¯æ•¸å­—
    if not c.isdigit():
        return False
    
    # Aè¦å‰‡æ ¸å¿ƒ: ç¬¬ä¸€ä½å¿…é ˆæ˜¯ 1-9 (æ’é™¤0é–‹é ­çš„ETFç­‰)
    if c[0] not in ['1', '2', '3', '4', '5', '6', '7', '8', '9']:
        return False
        
    # æ’é™¤ DR (å­˜è¨—æ†‘è­‰)
    if "DR" in name.upper() or c.startswith('91'):
        return False
    
    # æ’é™¤ç‰¹æ®Šä»£ç¢¼
    if c in ['9999', '0000', '1111', '2222', '3333', '4444', '5555', '6666', '7777', '8888']:
        return False
    
    return True

def get_system_status():
    """å–å¾—ç³»çµ±ç‹€æ…‹è³‡è¨Š"""
    status_info = {
        'last_update': 'ç„¡è³‡æ–™',
        'total_stocks': 0,
        'a_rule_stocks': 0,
        'date_range': ('N/A', 'N/A')
    }
    
    try:
        with db_manager.get_connection() as conn:
            # å„ªå…ˆå¾æ–°è¡¨è®€å–
            try:
                # å–å¾—æœ€å¾Œæ›´æ–°æ—¥æœŸ
                res = conn.execute("SELECT MAX(date) FROM stock_snapshot").fetchone()
                if res and res[0]:
                    status_info['last_update'] = res[0]
                
                # å–å¾—ç¸½è‚¡ç¥¨æ•¸
                res = conn.execute("SELECT COUNT(*) FROM stock_snapshot").fetchone()
                status_info['total_stocks'] = res[0] if res else 0
                
                # å–å¾—ç¬¦åˆ A è¦å‰‡çš„è‚¡ç¥¨æ•¸
                res = conn.execute("SELECT code, name FROM stock_snapshot").fetchall()
                status_info['a_rule_stocks'] = sum(1 for row in res if is_normal_stock(row[0], row[1]))
                
                # å–å¾—æ—¥æœŸç¯„åœ
                res = conn.execute("""
                    SELECT MIN(date_int), MAX(date_int) FROM stock_history
                """).fetchone()
                if res and res[0] and res[1]:
                    min_date = f"{res[0]//10000}-{(res[0]//100)%100:02d}-{res[0]%100:02d}"
                    max_date = f"{res[1]//10000}-{(res[1]//100)%100:02d}-{res[1]%100:02d}"
                    status_info['date_range'] = (min_date, max_date)
                    
            except Exception:
                # æ–°ä¸‰è¡¨æ¶æ§‹ï¼šä¸å† Fallback åˆ°èˆŠè¡¨
                pass
    
    except Exception as e:
        print_flush(f"âš  å–å¾—ç³»çµ±ç‹€æ…‹å¤±æ•—: {e}")
    
    return status_info

def check_api_status():
    """æª¢æŸ¥ API å¯ç”¨æ€§"""
    status = {
        'finmind': False,
        'twse': False,
        'tpex': False,
        'supabase': False
    }
    
    # æª¢æŸ¥ Supabase
    if ENABLE_CLOUD_SYNC:
        try:
            headers = {
                "apikey": SUPABASE_KEY,
                "Authorization": f"Bearer {SUPABASE_KEY}"
            }
            url = f"{SUPABASE_URL}/rest/v1/stock_list?select=count"
            response = requests.get(url, headers=headers, timeout=3, verify=False)
            if response.status_code == 200:
                status['supabase'] = True
        except Exception:
            pass
    
    # æª¢æŸ¥ FinMind API
    try:
        url = f"{FINMIND_URL}?dataset=TaiwanStockPrice&stock_id=2330&start_date=2024-01-01&token={FINMIND_TOKEN}"
        response = requests.get(url, timeout=3, verify=False)
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 200 or 'data' in data:
                status['finmind'] = True
    except Exception:
        pass
    
    # æª¢æŸ¥ TWSE API
    try:
        response = requests.get(TWSE_BWIBBU_URL, timeout=3, verify=False)
        if response.status_code == 200 and response.json():
            status['twse'] = True
    except Exception:
        pass
    
    # æª¢æŸ¥ TPEx API
    try:
        response = requests.get(TPEX_MAINBOARD_URL, timeout=3, verify=False)
        if response.status_code == 200:
            status['tpex'] = True
    except Exception:
        pass
        
    return status

def display_system_status():
    """é¡¯ç¤ºç³»çµ±ç‹€æ…‹è³‡è¨Šæ¿"""
    print_flush("\n" + "=" * 80)
    print_flush("ğŸ“Š ç³»çµ±ç‹€æ…‹")
    print_flush("-" * 80)
    
    # å–å¾—ç³»çµ±è³‡è¨Š
    sys_status = get_system_status()
    
    # é¡¯ç¤ºè³‡æ–™åº«è³‡è¨Š
    print_flush(f"ğŸ“ è³‡æ–™åº«: {DB_FILE}")
    print_flush(f"ğŸ“… æœ€æ–°æ›´æ–°: {sys_status['last_update']}")
    print_flush(f"ğŸ“ˆ è‚¡ç¥¨ç¸½æ•¸: {sys_status['total_stocks']} æª”")
    print_flush(f"ğŸ“† è³‡æ–™ç¯„åœ: {sys_status['date_range'][0]} ~ {sys_status['date_range'][1]}")
    
    print_flush("-" * 80)
    print_flush("ğŸš€ ç³»çµ±å·²å°±ç·’")
    print_flush("=" * 80)

def get_correct_stock_name(code, current_name=None):
    """å–å¾—æ­£ç¢ºçš„è‚¡ç¥¨åç¨±ï¼Œå¦‚æœæ²’æœ‰å‚³å…¥å‰‡å¾ DB æŸ¥è©¢"""
    # å·²æœ‰æœ‰æ•ˆåç¨±å‰‡ç›´æ¥è¿”å›
    if current_name and current_name != code and current_name != "æœªçŸ¥":
        return current_name
    
    # å˜—è©¦å¾ DB æŸ¥è©¢
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            # å„ªå…ˆå¾ stock_snapshot æŸ¥è©¢
            cur.execute("SELECT name FROM stock_snapshot WHERE code=?", (code,))
            row = cur.fetchone()
            if row and row[0]:
                return row[0]
            # Fallback: å¾ stock_meta æŸ¥è©¢
            cur.execute("SELECT name FROM stock_meta WHERE code=?", (code,))
            row = cur.fetchone()
            if row and row[0]:
                return row[0]
    except:
        pass
    
    return current_name if current_name else code

def get_latest_date_for_code(code):
    """ç²å–æŒ‡å®šè‚¡ç¥¨çš„æœ€æ–°æ—¥æœŸ"""
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            # ä½¿ç”¨ stock_history (æ–°ä¸‰è¡¨æ¶æ§‹)
            cur.execute("SELECT MAX(date_int) FROM stock_history WHERE code=?", (code,))
            result = cur.fetchone()
            if result and result[0]:
                d = result[0]
                return f"{d//10000}-{(d//100)%100:02d}-{d%100:02d}"
            return None
    except Exception as e:
        print_flush(f"âš  ç²å–æœ€æ–°æ—¥æœŸå¤±æ•— {code}: {e}")
        return None

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      DATASOURCE                               â•‘
# â•‘  TWSE/TPEX/FinMind è³‡æ–™æŠ“å–å™¨ï¼Œè¡¨é©…å‹• API_ENDPOINTS           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ==============================
# è³‡æ–™æºé¡åˆ¥
# ==============================
class DataSource:
    """æ•¸æ“šæºæŠ½è±¡æ¥å£"""
    def __init__(self, progress_tracker=None):
        self.progress = progress_tracker or ProgressTracker()
        self.name = "BaseDataSource"
    
    def fetch_history(self, stock_code, start_date=None, end_date=None, retry=3):
        """ç²å–è‚¡ç¥¨æ­·å²æ•¸æ“š"""
        raise NotImplementedError

class FinMindDataSource(DataSource):
    """FinMind API æ•¸æ“šæº"""
    def __init__(self, progress_tracker=None, silent=False):
        super().__init__(progress_tracker)
        self.name = "FinMind"
        self.url = FINMIND_URL
        self.token = FINMIND_TOKEN
        self.silent = silent
    
    def fetch_history(self, stock_code, start_date=None, end_date=None, retry=3):
        """å¾FinMindå–å¾—æ­·å²è³‡æ–™"""
        try:
            # å¦‚æœæ²’æœ‰æŒ‡å®šé–‹å§‹æ—¥æœŸï¼Œè¨ˆç®—250å€‹äº¤æ˜“æ—¥æ‰€éœ€çš„æ™‚é–“
            if start_date is None:
                start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")
            
            if end_date is None:
                end_date = datetime.now().strftime("%Y-%m-%d")
                
            params = {
                "dataset": "TaiwanStockPrice",
                "data_id": stock_code,
                "start_date": start_date,
                "end_date": end_date,
                "token": self.token,
            }
            
            for attempt in range(retry):
                try:
                    if not self.silent:
                        self.progress.info(f"{self.name}: å˜—è©¦ç²å– {stock_code} ({attempt+1}/{retry})", 1)
                    
                    # ä½¿ç”¨ SSL é©—è­‰ä½†å¿½ç•¥è­¦å‘Š
                    response = requests.get(
                        self.url, 
                        params=params, 
                        timeout=REQUEST_TIMEOUT,
                        verify=False
                    )
                    
                    if response.status_code == 429:  # é€Ÿç‡é™åˆ¶
                        if not self.silent:
                            self.progress.warning(f"{self.name}: éå¤šè«‹æ±‚ï¼Œç­‰å¾… 2 ç§’", 1)
                        time.sleep(2)
                        continue
                    
                    if response.status_code == 402: # ä»˜è²»é™åˆ¶/æ¬¡æ•¸ä¸Šé™
                        if not self.silent:
                            self.progress.warning(f"{self.name}: è«‹æ±‚æ¬¡æ•¸é”ä¸Šé™ (402)", 1)
                        return None
                        
                    if response.status_code == 404: # æ‰¾ä¸åˆ°è³‡æ–™
                        if not self.silent:
                            self.progress.warning(f"{self.name}: æ‰¾ä¸åˆ°è³‡æ–™ (404)", 1)
                        return None
                    
                    if response.status_code != 200:
                        if not self.silent:
                            self.progress.warning(f"{self.name}: ç‹€æ…‹ç¢¼ {response.status_code}", 1)
                        if attempt < retry - 1:
                            time.sleep(1)
                        continue
                    
                    data = response.json()
                    
                    if data is None or data.get('status') != 200:
                        if not self.silent:
                            self.progress.warning(f"{self.name}: API éŸ¿æ‡‰ç„¡æ•ˆ", 4)
                        if attempt < retry - 1:
                            time.sleep(1)
                        continue
                    
                    if not data.get('data') or len(data['data']) == 0:
                        return None
                    
                    rows = []
                    for item in data['data']:
                        try:
                            date = item.get('date')
                            if not date:
                                continue
                            close = safe_num(item.get('close'))
                            if close is not None and close > 0:
                                rows.append({
                                    'date': date,
                                    'open': safe_num(item.get('open')),
                                    'high': safe_num(item.get('max')),
                                    'low': safe_num(item.get('min')),
                                    'close': close,
                                    'volume': safe_int(item.get('Trading_Volume')),
                                    'amount': safe_num(item.get('Trading_money'))
                                })
                        except Exception:
                            continue
                    
                    if not rows:
                        return None
                    
                    df = pd.DataFrame(rows)
                    
                    # æŒ‰æ—¥æœŸå»é‡
                    df = df.drop_duplicates(subset=['date'], keep='first')
                    
                    # é©—è­‰è³‡æ–™å®Œæ•´æ€§
                    df = df[df['close'] > 0]
                    
                    # æŒ‰æ—¥æœŸæ’åº
                    df = df.sort_values('date').reset_index(drop=True)
                    
                    if not self.silent:
                        self.progress.success(f"{self.name}: ç²å– {len(df)} ç­† {stock_code} æ•¸æ“š", 4)
                    
                    return df
                    
                except Exception as e:
                    if not self.silent:
                        self.progress.warning(f"{self.name} éŒ¯èª¤: {e}", 1)
                    if attempt < retry - 1:
                        time.sleep(1)
            
            return None
            
        except Exception as e:
            if not self.silent:
                self.progress.error(f"{self.name} ç•°å¸¸: {e}", 4)
            return None

class TwstockDataSource(DataSource):
    """twstock æ•¸æ“šæº (å‚™æ´)"""
    def __init__(self, progress_tracker=None, silent=False):
        super().__init__(progress_tracker)
        self.name = "twstock (Backup)"
        self.silent = silent

    def fetch_history(self, stock_code, start_date=None, end_date=None, retry=3):
        try:
            if not self.silent:
                self.progress.info(f"{self.name}: å˜—è©¦ç²å– {stock_code}", 4)
            
            # å¢åŠ éš¨æ©Ÿå»¶é²ä»¥é¿å… Rate Limit (3-6ç§’)
            time.sleep(np.random.uniform(3, 6))
            
            # ä½¿ç”¨ Patch éçš„ twstock
            stock = twstock.Stock(stock_code)
            
            # è¨ˆç®—éœ€è¦æŠ“å–çš„èµ·å§‹å¹´æœˆ (é è¨­ 3 å¹´å‰ï¼Œç¢ºä¿æœ‰è¶³å¤ è³‡æ–™)
            if start_date:
                try:
                    dt = datetime.strptime(start_date, "%Y-%m-%d")
                except:
                    dt = datetime.now() - timedelta(days=1095)  # 3 å¹´å‰
            else:
                dt = datetime.now() - timedelta(days=1095)  # 3 å¹´å‰
            
            # ä½¿ç”¨ fetch_from æŠ“å–å¾æŒ‡å®šå¹´æœˆåˆ°ç¾åœ¨çš„æ‰€æœ‰è³‡æ–™
            # åŠ å…¥è¶…æ™‚æ©Ÿåˆ¶ï¼šä½¿ç”¨ concurrent.futures
            from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
            
            def do_fetch():
                stock.fetch_from(dt.year, dt.month)
                return stock.data
            
            try:
                with ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(do_fetch)
                    stock_data = future.result(timeout=60)  # 60ç§’è¶…æ™‚
            except FuturesTimeoutError:
                if not self.silent:
                    self.progress.warning(f"{self.name}: {stock_code} è¶…æ™‚ (60ç§’)", 4)
                return None
            except Exception as e:
                if not self.silent:
                    self.progress.warning(f"{self.name}: fetch_from å¤±æ•—: {e}", 4)
                # Fallback: å˜—è©¦ fetch_31
                try:
                    stock.fetch_31()
                except:
                    return None
            
            if not stock.data:
                # å†æ¬¡å˜—è©¦ fetch_31 (å¦‚æœ fetch_from æ²’å ±éŒ¯ä½†æ²’è³‡æ–™)
                try:
                    stock.fetch_31()
                except:
                    pass
                if not stock.data:
                    return None
                
            # è½‰æ›ç‚º DataFrame
            rows = []
            for d in stock.data:
                # twstock çš„ date æ˜¯ datetime ç‰©ä»¶
                d_str = d.date.strftime("%Y-%m-%d")
                
                # éæ¿¾æ—¥æœŸç¯„åœ
                if start_date and d_str < start_date:
                    continue
                if end_date and d_str > end_date:
                    continue
                    
                rows.append({
                    'date': d_str,
                    'open': d.open,
                    'high': d.high,
                    'low': d.low,
                    'close': d.close,
                    'volume': d.capacity,
                    'amount': d.turnover
                })
                
            if not rows:
                return None
                
            df = pd.DataFrame(rows)
            df = df.drop_duplicates(subset=['date'], keep='first')
            df = df.sort_values('date').reset_index(drop=True)
            
            if not self.silent:
                self.progress.success(f"{self.name}: ç²å– {len(df)} ç­† {stock_code} æ•¸æ“š", 1)
                
            return df
            
        except Exception as e:
            if not self.silent:
                self.progress.warning(f"{self.name} éŒ¯èª¤: {str(e)}", 1)
            return None




class GoodinfoDataSource(DataSource):
    """Goodinfo çˆ¬èŸ²å‚™æ´è³‡æ–™æº"""
    def __init__(self, progress_tracker=None, silent=False):
        super().__init__(progress_tracker)
        self.name = "Goodinfo (Backup)"
        self.silent = silent
        self.base_url = "https://goodinfo.tw/tw/ShowK_Chart.asp"
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Sec-Ch-Ua': '"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"',
            'Sec-Ch-Ua-Mobile': '?0',
            'Sec-Ch-Ua-Platform': '"Windows"',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Site': 'same-origin',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-User': '?1',
            'Sec-Fetch-Dest': 'document',
            'Referer': 'https://goodinfo.tw/tw/index.asp',
        }
    
    def fetch_history(self, stock_code, start_date=None, end_date=None, retry=3):
        """å¾ Goodinfo çˆ¬å–æ­·å²è‚¡åƒ¹"""
        print(f"Goodinfo fetch_history: {stock_code}")
        try:
            if not self.silent:
                self.progress.info(f"{self.name}: å˜—è©¦ç²å– {stock_code}", 4)
            
            # å¢åŠ éš¨æ©Ÿå»¶é²é¿å…è¢«å°é– (3-6ç§’)
            time.sleep(np.random.uniform(3, 6))
            
            # å»ºç«‹ session ä»¥ç¶­æŒ cookie
            session = requests.Session()
            session.headers.update(self.headers)
            
            # è¨­å®šæ¨¡æ“¬ Cookie (é‡è¦ï¼šç¹éåˆå§‹åŒ–æª¢æŸ¥)
            session.cookies.set('IS_TOUCH_DEVICE', 'F')
            session.cookies.set('SCREEN_SIZE', '1920')
            
            # å…ˆè¨ªå•é¦–é ç²å–åŸºç¤ cookie
            try:
                session.get("https://goodinfo.tw/tw/index.asp", timeout=10, verify=False)
            except:
                pass
            
            # è«‹æ±‚æ­·å²è‚¡åƒ¹é é¢ (åŠ å…¥ STEP=DATA_INIT ç¹éåˆå§‹åŒ–)
            # CHT_CAT=DATE: æ—¥ç·š
            url = f"{self.base_url}?STOCK_ID={stock_code}&CHT_CAT=DATE&STEP=DATA_INIT"
            session.headers.update({'Referer': f'https://goodinfo.tw/tw/StockDetail.asp?STOCK_ID={stock_code}'})
            
            for attempt in range(retry):
                try:
                    print(f"Attempt {attempt+1}/{retry}: {url}")
                    response = session.get(url, timeout=REQUEST_TIMEOUT, verify=False)
                    print(f"Response status: {response.status_code}, len: {len(response.text)}")
                    
                    if response.status_code != 200:
                        print(f"{self.name}: HTTP {response.status_code}")
                        if attempt < retry - 1:
                            time.sleep(2)
                        continue
                    
                    # æª¢æŸ¥æ˜¯å¦ä»ç‚ºåˆå§‹åŒ–é é¢
                    if 'åˆå§‹åŒ–ä¸­' in response.text and 'STEP=DATA_INIT' not in response.url:
                        print(f"{self.name}: é˜²çˆ¬èŸ²é‡å®šå‘ï¼Œç­‰å¾…é‡è©¦")
                        if attempt < retry - 1:
                            time.sleep(3)
                        continue
                    
                    # å¼·åˆ¶è¨­å®šç·¨ç¢¼
                    response.encoding = 'utf-8'

                    # ä½¿ç”¨ pandas è§£æè¡¨æ ¼
                    try:
                        # ä½¿ç”¨ lxml è§£æå™¨è¼ƒå¿«ä¸”å®¹éŒ¯
                        tables = pd.read_html(response.text)
                        print(f"Parsed {len(tables)} tables")
                    except Exception as e:
                        print(f"{self.name}: è§£æè¡¨æ ¼å¤±æ•— - {e}")
                        continue
                    
                    # å°‹æ‰¾åŒ…å«æ—¥æœŸå’Œæ”¶ç›¤åƒ¹çš„è¡¨æ ¼
                    df = None
                    for i, table in enumerate(tables):
                        # è™•ç† MultiIndex (æ‰å¹³åŒ–)
                        if isinstance(table.columns, pd.MultiIndex):
                            table.columns = [' '.join(map(str, col)).strip() for col in table.columns.values]
                            
                        # è½‰ç‚ºå­—ä¸²ä¸¦å°å¯«ä»¥é€²è¡Œæ¨¡ç³Šæ¯”å°
                        cols_str = [str(c).lower() for c in table.columns]
                        cols_concat = " ".join(cols_str)
                        
                        # æ’é™¤æœŸè²¨è¡¨æ ¼
                        if 'æœŸè²¨' in cols_concat:
                            continue
                            
                        # å°‹æ‰¾åŒ…å« "æ—¥æœŸ", "æ”¶ç›¤" çš„è¡¨æ ¼
                        # é—œéµå­—: æ—¥æœŸ/date/äº¤æ˜“æ—¥, æ”¶ç›¤/close/æˆäº¤åƒ¹
                        has_date = any(k in cols_concat for k in ['æ—¥æœŸ', 'date', 'äº¤æ˜“æ—¥'])
                        has_price = any(k in cols_concat for k in ['æ”¶ç›¤', 'close', 'æˆäº¤åƒ¹'])
                        has_open = any(k in cols_concat for k in ['é–‹ç›¤', 'open'])
                        has_high = any(k in cols_concat for k in ['æœ€é«˜', 'high'])
                        has_low = any(k in cols_concat for k in ['æœ€ä½', 'low'])
                        
                        if has_date and has_price and has_open and has_high and has_low:
                            df = table
                            # print(f"Selected table with cols: {df.columns.tolist()}")
                            break
                    
                    if df is None or df.empty:
                        if not self.silent:
                            self.progress.warning(f"{self.name}: æœªæ‰¾åˆ°æœ‰æ•ˆè¡¨æ ¼", 4)
                        continue
                    
                    # æ¨™æº–åŒ–æ¬„ä½åç¨±
                    col_mapping = {}
                    # è™•ç†å¤šå±¤ç´¢å¼•æˆ–å–®å±¤ç´¢å¼•
                    if isinstance(df.columns, pd.MultiIndex):
                        df.columns = ['_'.join(map(str, col)).strip() for col in df.columns.values]
                    
                    for col in df.columns:
                        col_str = str(col).strip()
                        if 'æ—¥æœŸ' in col_str or 'äº¤æ˜“æ—¥' in col_str:
                            col_mapping[col] = 'date'
                        elif 'é–‹ç›¤' in col_str:
                            col_mapping[col] = 'open'
                        elif 'æœ€é«˜' in col_str:
                            col_mapping[col] = 'high'
                        elif 'æœ€ä½' in col_str:
                            col_mapping[col] = 'low'
                        elif 'æ”¶ç›¤' in col_str:
                            col_mapping[col] = 'close'
                        elif ('å¼µæ•¸' in col_str or 'æˆäº¤é‡' in col_str) and 'æˆäº¤' in col_str:
                            col_mapping[col] = 'volume'
                        elif ('é‡‘é¡' in col_str or 'æˆäº¤é¡' in col_str or 'å„„å…ƒ' in col_str) and 'æˆäº¤' in col_str:
                            col_mapping[col] = 'amount'
                    
                    df = df.rename(columns=col_mapping)
                    
                    # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
                    required = ['date', 'close']
                    if not all(col in df.columns for col in required):
                        # å˜—è©¦å°‹æ‰¾å…¶ä»–å¯èƒ½çš„æ¬„ä½å
                        continue
                    
                    # è½‰æ›æ—¥æœŸæ ¼å¼
                    def parse_date(d):
                        try:
                            d_str = str(d).strip().replace("'", "")
                            # è™•ç† Goodinfo ç‰¹æ®Šæ ¼å¼ (å¦‚ 24/12/11 æˆ– 2024/12/11)
                            if '/' in d_str:
                                parts = d_str.split('/')
                                if len(parts) == 3:
                                    # å˜—è©¦è§£æ 4 ä½æ•¸å¹´ä»½
                                    try:
                                        return datetime.strptime(d_str, '%Y/%m/%d').strftime('%Y-%m-%d')
                                    except ValueError:
                                        # å˜—è©¦è§£æ 2 ä½æ•¸å¹´ä»½
                                        try:
                                            return datetime.strptime(d_str, '%y/%m/%d').strftime('%Y-%m-%d')
                                        except ValueError:
                                            pass
                            elif '-' in d_str:
                                return datetime.strptime(d_str, '%Y-%m-%d').strftime('%Y-%m-%d')
                            return None
                        except:
                            return None
                    
                    df['date'] = df['date'].apply(parse_date)
                    df = df.dropna(subset=['date'])
                    print(f"Parsed dates: {len(df)} rows")
                    
                    # éæ¿¾æ—¥æœŸç¯„åœ
                    if start_date:
                        df = df[df['date'] >= start_date]
                    if end_date:
                        df = df[df['date'] <= end_date]
                    
                    if df.empty:
                        return None
                    
                    # è™•ç†æ•¸å€¼æ¬„ä½ (ç§»é™¤é€—è™Ÿ, è™•ç† '---')
                    numeric_cols = ['open', 'high', 'low', 'close', 'volume', 'amount']
                    for col in numeric_cols:
                        if col in df.columns:
                            df[col] = df[col].astype(str).str.replace(',', '').str.replace('+', '').str.replace('X', '')
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # å¡«å……ç¼ºå¤±æ¬„ä½
                    for col in numeric_cols:
                        if col not in df.columns:
                            df[col] = None
                    
                    # è½‰æ›å–®ä½ (Goodinfo æˆäº¤å¼µæ•¸æ˜¯å¼µ, é‡‘é¡æ˜¯å„„)
                    # ç³»çµ±é è¨­ amount æ˜¯å…ƒã€‚
                    # Goodinfo "æˆäº¤é‡‘é¡(å„„)" -> éœ€ * 100,000,000
                    if 'amount' in df.columns:
                         # ç¢ºä¿æ˜¯æ•¸å€¼å‹æ…‹
                         df['amount'] = pd.to_numeric(df['amount'], errors='coerce').fillna(0)
                         # è½‰æ›å–®ä½ï¼šå„„ -> å…ƒ
                         df['amount'] = df['amount'] * 100000000
                    
                    df = df[['date', 'open', 'high', 'low', 'close', 'volume', 'amount']]
                    df = df.drop_duplicates(subset=['date'], keep='first')
                    df = df.sort_values('date').reset_index(drop=True)
                    
                    if not self.silent:
                        self.progress.success(f"{self.name}: ç²å– {len(df)} ç­† {stock_code} æ•¸æ“š", 1)
                    
                    return df
                    
                except Exception as e:
                    if not self.silent:
                        self.progress.warning(f"{self.name} è«‹æ±‚éŒ¯èª¤: {e}", 4)
                    if attempt < retry - 1:
                        time.sleep(2)
            
            return None
            
        except Exception as e:
            if not self.silent:
                self.progress.warning(f"{self.name} éŒ¯èª¤: {str(e)}", 1)
            return None


class DataSourceManager:
    """æ•¸æ“šæºç®¡ç†å™¨"""
    def __init__(self, progress_tracker=None, silent=False):
        self.progress = progress_tracker or ProgressTracker()
        self.silent = silent
        self.sources = [
            FinMindDataSource(progress_tracker, silent=silent),
            TwstockDataSource(progress_tracker, silent=silent)
        ]
    
    def fetch_history(self, stock_code, start_date=None, end_date=None, retry=3):
        """å˜—è©¦æ‰€æœ‰æ•¸æ“šæºï¼Œç›´åˆ°æˆåŠŸæˆ–å…¨éƒ¨å¤±æ•—"""
        for i, source in enumerate(self.sources):
            # if not self.silent:
            #     self.progress.info(f"å˜—è©¦ä½¿ç”¨ {source.name} ç²å– {stock_code} æ•¸æ“š...", 4)
            
            df = source.fetch_history(stock_code, start_date, end_date, retry)
            
            if df is not None and not df.empty:
                return df
            
            # å‚™æ´åˆ‡æ›æç¤º (é†’ç›®é¡¯ç¤º)
            if i < len(self.sources) - 1:
                if not self.silent:
                    self.progress.warning(f"âš¡ {source.name} å¤±æ•—ï¼Œåˆ‡æ›è‡³ {self.sources[i+1].name}...", 4)
                
        if not self.silent:
            self.progress.error(f"âŒ æ‰€æœ‰æ•¸æ“šæºéƒ½ç„¡æ³•ç²å– {stock_code} æ•¸æ“š", 4)
        return None

# ==============================
# æ³•äººè²·è³£è¶… API
# ==============================
class InstitutionalInvestorAPI:
    """ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™ API"""
    
    # TWSE (ä¸Šå¸‚) æ³•äººè²·è³£è¶… API (ç¶²é ç‰ˆ - å‚™æ´)
    TWSE_T86_URL = "https://www.twse.com.tw/rwd/zh/fund/T86"
    # TPEx (ä¸Šæ«ƒ) æ³•äººè²·è³£è¶… API (ç¶²é ç‰ˆ - å‚™æ´)
    TPEX_INST_URL = "https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php"
    
    # === å®˜æ–¹ OpenAPI URLs (ä¸»è¦ä¾†æº) ===
    TWSE_OPENAPI_URL = "https://openapi.twse.com.tw/v1/fund/T86_ALL"
    TPEX_OPENAPI_URL = "https://www.tpex.org.tw/openapi/v1/tpex_3insti_daily_trading"
    
    @classmethod
    def fetch_twse_openapi(cls, progress=None):
        """å¾ TWSE å–å¾—ä»Šæ—¥æ³•äººè²·è³£è¶…è³‡æ–™ (ä¸‰å±¤å‚™æ´)
        å„ªå…ˆé †åº: 1. TWSE ç¶²é ç‰ˆ (JSON) 2. TWSE OpenAPI 3. FinMind
        """
        results = []
        today = datetime.now().strftime("%Y%m%d")
        
        # === 1. TWSE ç¶²é ç‰ˆ (ä¸»è¦ä¾†æº - JSON æ ¼å¼) ===
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ TWSE ç¶²é ç‰ˆå–å¾—æ³•äººè³‡æ–™...", level=1)
            
            url = f"https://www.twse.com.tw/fund/T86?response=json&date={today}&selectType=ALL"
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            resp = requests.get(url, headers=headers, timeout=20, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            
            # æª¢æŸ¥è³‡æ–™æ˜¯å¦å­˜åœ¨
            if data.get('stat') == 'OK' and 'data' in data:
                today_int = int(today)
                
                for row in data['data']:
                    try:
                        code = str(row[0]).strip().replace('=', '').replace('"', '')
                        if not code.isdigit() or len(code) > 4:
                            continue
                        
                        results.append({
                            'code': code,
                            'name': str(row[1]).strip(),
                            'date_int': today_int,
                            'foreign_buy': cls._parse_number(row[2]),   # å¤–è³‡è²·é€²
                            'foreign_sell': cls._parse_number(row[3]),  # å¤–è³‡è³£å‡º
                            'trust_buy': cls._parse_number(row[8]),     # æŠ•ä¿¡è²·é€²
                            'trust_sell': cls._parse_number(row[9]),    # æŠ•ä¿¡è³£å‡º
                            'dealer_buy': cls._parse_number(row[12]),   # è‡ªç‡Ÿå•†è²·é€²
                            'dealer_sell': cls._parse_number(row[13]),  # è‡ªç‡Ÿå•†è³£å‡º
                            'market': 'TWSE'
                        })
                    except:
                        pass
                
                if results:
                    if progress:
                        progress.success(f"âœ“ TWSE ç¶²é ç‰ˆæ³•äºº: {len(results)} ç­†", level=1)
                    return results
            else:
                if progress:
                    progress.warn("TWSE ç¶²é ç‰ˆç„¡ä»Šæ—¥è³‡æ–™", level=1)
                    
        except Exception as e:
            logger.error(f"TWSE ç¶²é ç‰ˆæ³•äººå¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TWSE ç¶²é ç‰ˆå¤±æ•—: {e}", level=1)
        
        # === 2. TWSE OpenAPI (ç¬¬ä¸€å‚™æ´) ===
        if not results:
            try:
                if progress:
                    progress.info("å˜—è©¦ TWSE OpenAPI å‚™æ´...", level=1)
                
                resp = requests.get(cls.TWSE_OPENAPI_URL, timeout=30, verify=False)
                content = resp.text.strip()
                
                if content and content != '[]':
                    data = resp.json()
                    if isinstance(data, list):
                        today_int = int(today)
                        for item in data:
                            code = str(item.get("è­‰åˆ¸ä»£è™Ÿ", "")).strip()
                            if not code.isdigit() or len(code) > 4:
                                continue
                            
                            results.append({
                                'code': code,
                                'name': str(item.get("è­‰åˆ¸åç¨±", "")).strip(),
                                'date_int': today_int,
                                'foreign_buy': cls._parse_number(item.get("å¤–è³‡åŠé™¸è³‡è²·é€²è‚¡æ•¸", 0)),
                                'foreign_sell': cls._parse_number(item.get("å¤–è³‡åŠé™¸è³‡è³£å‡ºè‚¡æ•¸", 0)),
                                'trust_buy': cls._parse_number(item.get("æŠ•ä¿¡è²·é€²è‚¡æ•¸", 0)),
                                'trust_sell': cls._parse_number(item.get("æŠ•ä¿¡è³£å‡ºè‚¡æ•¸", 0)),
                                'dealer_buy': cls._parse_number(item.get("è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸", 0)),
                                'dealer_sell': cls._parse_number(item.get("è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸", 0)),
                                'market': 'TWSE'
                            })
                        
                        if results:
                            if progress:
                                progress.success(f"âœ“ TWSE OpenAPI æ³•äºº: {len(results)} ç­†", level=1)
                            return results
                            
            except Exception as e:
                logger.error(f"TWSE OpenAPI æ³•äººå¤±æ•—: {e}")
        
        # === 3. FinMind (æœ€çµ‚å‚™æ´) ===
        if not results:
            try:
                if progress:
                    progress.info("å˜—è©¦ FinMind å‚™æ´...", level=1)
                results = cls._fetch_twse_from_finmind(progress)
            except Exception as e2:
                logger.error(f"FinMind æ³•äººå‚™æ´å¤±æ•—: {e2}")
        
        return results
    
    @classmethod
    def _fetch_twse_from_finmind(cls, progress=None):
        """ä½¿ç”¨ FinMind å–å¾— TWSE æ³•äººè³‡æ–™ (å‚™æ´)"""
        results = []
        today = datetime.now().strftime("%Y-%m-%d")
        
        try:
            params = {
                "dataset": "TaiwanStockInstitutionalInvestorsBuySell",
                "start_date": today,
                "end_date": today,
                "token": FINMIND_TOKEN
            }
            
            resp = requests.get(FINMIND_URL, params=params, timeout=30, verify=False)
            data = resp.json()
            
            if data.get('status') != 200 or 'data' not in data:
                if progress:
                    progress.warn("FinMind ç„¡ä»Šæ—¥æ³•äººè³‡æ–™", level=1)
                # ä¸ returnï¼Œç¹¼çºŒå˜—è©¦ç¶²é ç‰ˆå‚™æ´
            
            # æ•´ç†è³‡æ–™ - å°‡åŒä¸€è‚¡ç¥¨çš„ä¸åŒæ³•äººè³‡æ–™åˆä½µ
            stock_data = {}
            for row in data['data']:
                code = str(row.get('stock_id', '')).strip()
                if not code or len(code) > 4:
                    continue
                    
                name = row.get('name', '')
                buy = int(row.get('buy', 0) or 0)
                sell = int(row.get('sell', 0) or 0)
                
                if code not in stock_data:
                    stock_data[code] = {
                        'code': code,
                        'name': '',
                        'date_int': int(today.replace('-', '')),
                        'foreign_buy': 0, 'foreign_sell': 0,
                        'trust_buy': 0, 'trust_sell': 0,
                        'dealer_buy': 0, 'dealer_sell': 0,
                        'market': 'TWSE'
                    }
                
                # ä¾æ³•äººé¡åˆ¥ç´¯åŠ 
                if 'Foreign' in name:
                    stock_data[code]['foreign_buy'] += buy
                    stock_data[code]['foreign_sell'] += sell
                elif 'Investment_Trust' in name:
                    stock_data[code]['trust_buy'] += buy
                    stock_data[code]['trust_sell'] += sell
                elif 'Dealer' in name:
                    stock_data[code]['dealer_buy'] += buy
                    stock_data[code]['dealer_sell'] += sell
            
            results = list(stock_data.values())
            
            if progress:
                progress.success(f"âœ“ FinMind æ³•äºº: {len(results)} ç­†", level=1)
                
        except Exception as e:
            logger.error(f"FinMind æ³•äººå–å¾—å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— FinMind å¤±æ•—: {e}", level=1)
        
        # === TWSE ç¶²é ç‰ˆå‚™æ´ (ç•¶ FinMind ä¹Ÿå¤±æ•—æ™‚) ===
        if not results:
            try:
                if progress:
                    progress.info("å˜—è©¦ TWSE ç¶²é ç‰ˆå‚™æ´...", level=1)
                results = cls._fetch_twse_from_web(progress)
            except Exception as e3:
                logger.error(f"TWSE ç¶²é ç‰ˆå‚™æ´å¤±æ•—: {e3}")
        
        return results
    
    @classmethod
    def _fetch_twse_from_web(cls, progress=None):
        """ä½¿ç”¨ TWSE ç¶²é ç‰ˆå–å¾—æ³•äººè³‡æ–™ (æœ€çµ‚å‚™æ´)"""
        from io import StringIO
        import pandas as pd
        import time
        import random
        
        results = []
        today = datetime.now().strftime("%Y%m%d")
        
        try:
            # TWSE T86 CSV æ ¼å¼ API
            url = f"https://www.twse.com.tw/rwd/zh/fund/T86?response=csv&date={today}&selectType=ALLBUT0999"
            
            # æ·»åŠ å»¶é²é¿å…è¢«å°é–
            time.sleep(random.uniform(1.0, 2.0))
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            resp = requests.get(url, headers=headers, timeout=20, verify=False)
            
            if resp.status_code != 200 or len(resp.text) < 100:
                if progress:
                    progress.warn("TWSE ç¶²é ç‰ˆç„¡ä»Šæ—¥è³‡æ–™", level=1)
                return results
            
            # è§£æ CSV
            df = pd.read_csv(StringIO(resp.text), header=1).dropna(how='all', axis=1).dropna(how='any')
            df = df.astype(str).apply(lambda s: s.str.replace(',', ''))
            
            if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns:
                if progress:
                    progress.warn("TWSE ç¶²é ç‰ˆæ ¼å¼ç•°å¸¸", level=1)
                return results
            
            df['code'] = df['è­‰åˆ¸ä»£è™Ÿ'].str.replace('=', '').str.replace('"', '').str.strip()
            df = df[df['code'].str.len() == 4]
            
            today_int = int(today)
            
            for _, row in df.iterrows():
                try:
                    code = row['code']
                    results.append({
                        'code': code,
                        'name': str(row.get('è­‰åˆ¸åç¨±', '')).strip(),
                        'date_int': today_int,
                        'foreign_buy': cls._parse_number(row.get('å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)è²·é€²è‚¡æ•¸', 0)),
                        'foreign_sell': cls._parse_number(row.get('å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)è³£å‡ºè‚¡æ•¸', 0)),
                        'trust_buy': cls._parse_number(row.get('æŠ•ä¿¡è²·é€²è‚¡æ•¸', 0)),
                        'trust_sell': cls._parse_number(row.get('æŠ•ä¿¡è³£å‡ºè‚¡æ•¸', 0)),
                        'dealer_buy': cls._parse_number(row.get('è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸(è‡ªè¡Œè²·è³£)', 0)),
                        'dealer_sell': cls._parse_number(row.get('è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸(è‡ªè¡Œè²·è³£)', 0)),
                        'market': 'TWSE'
                    })
                except:
                    pass
            
            if progress:
                progress.success(f"âœ“ TWSE ç¶²é ç‰ˆæ³•äºº: {len(results)} ç­†", level=1)
                
        except Exception as e:
            logger.error(f"TWSE ç¶²é ç‰ˆå–å¾—å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TWSE ç¶²é ç‰ˆå¤±æ•—: {e}", level=1)
        
        return results
    
    @classmethod
    def fetch_tpex_openapi(cls, progress=None):
        """å¾ TPEx OpenAPI å–å¾—ä»Šæ—¥æ³•äººè²·è³£è¶…è³‡æ–™"""
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ TPEx OpenAPI å–å¾—æ³•äººè³‡æ–™...", level=2)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
            
            resp = requests.get(cls.TPEX_OPENAPI_URL, headers=headers, timeout=30, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            if not isinstance(data, list):
                logger.warning("TPEx æ³•äºº OpenAPI å›å‚³éé™£åˆ—æ ¼å¼")
                return results
            
            for item in data:
                code = str(item.get("SecuritiesCompanyCode", "")).strip()
                if not code or not code.isdigit() or len(code) > 4:
                    continue
                
                # è§£ææ—¥æœŸ (æ°‘åœ‹æ ¼å¼: 1141217 -> 20251217)
                date_str = str(item.get("Date", "")).strip()
                if len(date_str) == 7:
                    year = int(date_str[:3]) + 1911
                    date_int = int(f"{year}{date_str[3:]}")
                else:
                    date_int = int(datetime.now().strftime("%Y%m%d"))
                
                results.append({
                    'code': code,
                    'name': str(item.get("CompanyName", "")).strip(),
                    'date_int': date_int,
                    'foreign_buy': cls._parse_number(item.get("ForeignInvestorsBuy", 0)),
                    'foreign_sell': cls._parse_number(item.get("ForeignInvestorsSell", 0)),
                    'trust_buy': cls._parse_number(item.get("SecuritiesInvestmentTrustBuy", 0)),
                    'trust_sell': cls._parse_number(item.get("SecuritiesInvestmentTrustSell", 0)),
                    'dealer_buy': cls._parse_number(item.get("DealersBuy", 0)),
                    'dealer_sell': cls._parse_number(item.get("DealersSell", 0)),
                    'market': 'TPEx'
                })
            
            if progress:
                progress.success(f"âœ“ TPEx OpenAPI æ³•äºº: {len(results)} ç­†", level=2)
                
        except Exception as e:
            logger.error(f"TPEx æ³•äºº OpenAPI å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TPEx æ³•äºº OpenAPI å¤±æ•—: {e}", level=2)
        
        return results
    
    @classmethod
    def fetch_all_openapi(cls, progress=None):
        """å¾å®˜æ–¹ OpenAPI å–å¾—æ‰€æœ‰æ³•äººè³‡æ–™ä¸¦å„²å­˜åˆ°è³‡æ–™åº«"""
        twse_data = cls.fetch_twse_openapi(progress)
        tpex_data = cls.fetch_tpex_openapi(progress)
        
        all_data = twse_data + tpex_data
        
        if all_data:
            saved = cls.save_openapi_to_db(all_data)
            if progress:
                progress.success(f"âœ“ æ³•äººè³‡æ–™å·²å„²å­˜: {saved} ç­†", level=3)
            return saved
        
        return 0
    
    @classmethod
    def save_openapi_to_db(cls, data_list):
        """å°‡ OpenAPI æ³•äººè³‡æ–™å„²å­˜åˆ°è³‡æ–™åº«"""
        if not data_list:
            return 0
        
        cls.ensure_table()
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                records = [
                    (d['code'], d['date_int'], d['foreign_buy'], d['foreign_sell'],
                     d['trust_buy'], d['trust_sell'], d['dealer_buy'], d['dealer_sell'])
                    for d in data_list
                ]
                
                cur.executemany("""
                    INSERT OR REPLACE INTO institutional_investors 
                    (code, date_int, foreign_buy, foreign_sell, trust_buy, trust_sell, dealer_buy, dealer_sell)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, records)
                
                # åŒæ­¥æœ€æ–°æ³•äººæ•¸æ“šåˆ° stock_snapshot
                for d in data_list:
                    foreign_net = d['foreign_buy'] - d['foreign_sell']
                    trust_net = d['trust_buy'] - d['trust_sell']
                    dealer_net = d['dealer_buy'] - d['dealer_sell']
                    cur.execute("""
                        UPDATE stock_snapshot 
                        SET foreign_buy = ?, trust_buy = ?, dealer_buy = ?
                        WHERE code = ?
                    """, (foreign_net, trust_net, dealer_net, d['code']))
                
                conn.commit()
                return len(records)
                
        except Exception as e:
            logger.error(f"å„²å­˜æ³•äººè³‡æ–™å¤±æ•—: {e}")
            return 0
    
    @classmethod
    def ensure_table(cls):
        """ç¢ºä¿ institutional_investors è³‡æ–™è¡¨å­˜åœ¨"""
        try:
            with db_manager.get_connection() as conn:
                conn.execute("""
                    CREATE TABLE IF NOT EXISTS institutional_investors (
                        code TEXT NOT NULL,
                        date_int INTEGER NOT NULL,
                        foreign_buy INTEGER DEFAULT 0,
                        foreign_sell INTEGER DEFAULT 0,
                        trust_buy INTEGER DEFAULT 0,
                        trust_sell INTEGER DEFAULT 0,
                        dealer_buy INTEGER DEFAULT 0,
                        dealer_sell INTEGER DEFAULT 0,
                        PRIMARY KEY (code, date_int)
                    )
                """)
                conn.execute("CREATE INDEX IF NOT EXISTS idx_inst_code ON institutional_investors(code)")
                conn.execute("CREATE INDEX IF NOT EXISTS idx_inst_date ON institutional_investors(date_int)")
                conn.commit()
        except Exception as e:
            print_flush(f"âš  å»ºç«‹æ³•äººè³‡æ–™è¡¨å¤±æ•—: {e}")
    
    @classmethod
    def save_to_db(cls, data_list, date_str=None):
        """
        å°‡æ³•äººè³‡æ–™å„²å­˜åˆ°è³‡æ–™åº«
        data_list: fetch_twse/tpex_institutional å›å‚³çš„è³‡æ–™
        """
        if not data_list:
            return 0
        
        cls.ensure_table()
        
        if date_str is None:
            date_int = int(datetime.now().strftime("%Y%m%d"))
        else:
            date_int = int(date_str.replace('-', ''))
        
        saved = 0
        try:
            with db_manager.get_connection() as conn:
                for item in data_list:
                    # è¨ˆç®—è²·é€²è³£å‡ºè‚¡æ•¸ (å¾ net åæ¨)
                    foreign_net = item.get('foreign_net', 0)
                    trust_net = item.get('trust_net', 0)
                    dealer_net = item.get('dealer_net', 0)
                    
                    conn.execute("""
                        INSERT OR REPLACE INTO institutional_investors 
                        (code, date_int, foreign_buy, foreign_sell, trust_buy, trust_sell, dealer_buy, dealer_sell)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        item['code'], date_int,
                        max(0, foreign_net), max(0, -foreign_net),
                        max(0, trust_net), max(0, -trust_net),
                        max(0, dealer_net), max(0, -dealer_net)
                    ))
                    saved += 1
                conn.commit()
        except Exception as e:
            print_flush(f"âš  å„²å­˜æ³•äººè³‡æ–™å¤±æ•—: {e}")
        
        return saved
    
    @classmethod
    def get_from_db(cls, stock_code, days=30):
        """å¾è³‡æ–™åº«å–å¾—å€‹è‚¡æ³•äººæ­·å²è³‡æ–™"""
        cls.ensure_table()
        
        try:
            with db_manager.get_connection() as conn:
                cursor = conn.execute("""
                    SELECT date_int, foreign_buy - foreign_sell as foreign_net,
                           trust_buy - trust_sell as trust_net,
                           dealer_buy - dealer_sell as dealer_net
                    FROM institutional_investors
                    WHERE code = ?
                    ORDER BY date_int DESC
                    LIMIT ?
                """, (stock_code, days))
                
                results = []
                for row in cursor.fetchall():
                    results.append({
                        'date': str(row[0]),
                        'foreign_net': row[1],
                        'trust_net': row[2],
                        'dealer_net': row[3]
                    })
                return results
        except:
            return []
    
    @staticmethod
    def _parse_number(s):
        """è§£ææ•¸å­—å­—ä¸²ï¼Œç§»é™¤é€—è™Ÿä¸¦è½‰ç‚ºæ•´æ•¸"""
        if not s or s == '--':
            return 0
        try:
            return int(str(s).replace(',', '').replace(' ', ''))
        except:
            return 0
    
    @classmethod
    def fetch_twse_institutional(cls, date_str=None):
        """
        å–å¾—ä¸Šå¸‚(TWSE)ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™
        å›å‚³: list of dict, æ¯å€‹ dict åŒ…å« code, name, foreign_net, trust_net, dealer_net, total_net
        """
        if date_str is None:
            date_str = datetime.now().strftime("%Y%m%d")
        else:
            date_str = date_str.replace('-', '')
        
        try:
            url = f"{cls.TWSE_T86_URL}?date={date_str}&selectType=ALL&response=json"
            resp = requests.get(url, timeout=10, verify=False)
            resp.raise_for_status()
            data = resp.json()
            
            if data.get('stat') != 'OK' or 'data' not in data:
                return []
            
            results = []
            for row in data['data']:
                code = row[0].strip()
                # éæ¿¾éæ™®é€šè‚¡ (æ¬Šè­‰ã€ETF æ§“æ¡¿ç­‰)
                if len(code) > 4 and not code.isdigit():
                    continue
                    
                results.append({
                    'code': code,
                    'name': row[1].strip(),
                    'foreign_net': cls._parse_number(row[4]),   # å¤–é™¸è³‡è²·è³£è¶…(ä¸å«è‡ªç‡Ÿ)
                    'trust_net': cls._parse_number(row[10]),    # æŠ•ä¿¡è²·è³£è¶…
                    'dealer_net': cls._parse_number(row[11]),   # è‡ªç‡Ÿå•†è²·è³£è¶…
                    'total_net': cls._parse_number(row[18])     # ä¸‰å¤§æ³•äººè²·è³£è¶…
                })
            
            return results
            
        except Exception as e:
            print_flush(f"âš  å–å¾—ä¸Šå¸‚æ³•äººè³‡æ–™å¤±æ•—: {e}")
            return []
    
    @classmethod
    def fetch_tpex_institutional(cls, date_str=None):
        """
        å–å¾—ä¸Šæ«ƒ(TPEx)ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™
        å›å‚³: list of dict
        """
        if date_str is None:
            today = datetime.now()
            # è½‰ç‚ºæ°‘åœ‹å¹´æ ¼å¼
            roc_year = today.year - 1911
            date_str = f"{roc_year}/{today.month:02d}/{today.day:02d}"
        else:
            # è½‰æ› YYYY-MM-DD ç‚ºæ°‘åœ‹å¹´æ ¼å¼
            parts = date_str.replace('-', '/').split('/')
            if len(parts) == 3:
                roc_year = int(parts[0]) - 1911
                date_str = f"{roc_year}/{parts[1]}/{parts[2]}"
        
        try:
            url = f"{cls.TPEX_INST_URL}?l=zh-tw&d={date_str}&se=EW&t=D"
            resp = requests.get(url, timeout=10, verify=False)
            resp.raise_for_status()
            data = resp.json()
            
            if 'aaData' not in data:
                return []
            
            results = []
            for row in data['aaData']:
                code = str(row[0]).strip()
                # éæ¿¾éæ™®é€šè‚¡
                if len(code) > 4:
                    continue
                    
                results.append({
                    'code': code,
                    'name': str(row[1]).strip(),
                    'foreign_net': cls._parse_number(row[4]),   # å¤–è³‡è²·è³£è¶…
                    'trust_net': cls._parse_number(row[10]),    # æŠ•ä¿¡è²·è³£è¶…
                    'dealer_net': cls._parse_number(row[13]),   # è‡ªç‡Ÿå•†è²·è³£è¶…
                    'total_net': cls._parse_number(row[16])     # ä¸‰å¤§æ³•äººè²·è³£è¶…
                })
            
            return results
            
        except Exception as e:
            print_flush(f"âš  å–å¾—ä¸Šæ«ƒæ³•äººè³‡æ–™å¤±æ•—: {e}")
            return []
    
    @classmethod
    def get_all_institutional_data(cls, date_str=None):
        """å–å¾—æ‰€æœ‰(ä¸Šå¸‚+ä¸Šæ«ƒ)æ³•äººè³‡æ–™"""
        twse_data = cls.fetch_twse_institutional(date_str)
        tpex_data = cls.fetch_tpex_institutional(date_str)
        return twse_data + tpex_data
    
    @classmethod
    def get_ranking(cls, rank_type='foreign_buy', top_n=10, date_str=None):
        """
        å–å¾—æ’è¡Œæ¦œ
        rank_type: foreign_buy, foreign_sell, trust_buy, trust_sell
        top_n: é¡¯ç¤ºå‰ N å
        """
        all_data = cls.get_all_institutional_data(date_str)
        
        if not all_data:
            return []
        
        # æ ¹æ“šé¡å‹æ’åº
        if rank_type == 'foreign_buy':
            sorted_data = sorted(all_data, key=lambda x: x['foreign_net'], reverse=True)
            sorted_data = [d for d in sorted_data if d['foreign_net'] > 0]
        elif rank_type == 'foreign_sell':
            sorted_data = sorted(all_data, key=lambda x: x['foreign_net'])
            sorted_data = [d for d in sorted_data if d['foreign_net'] < 0]
        elif rank_type == 'trust_buy':
            sorted_data = sorted(all_data, key=lambda x: x['trust_net'], reverse=True)
            sorted_data = [d for d in sorted_data if d['trust_net'] > 0]
        elif rank_type == 'trust_sell':
            sorted_data = sorted(all_data, key=lambda x: x['trust_net'])
            sorted_data = [d for d in sorted_data if d['trust_net'] < 0]
        else:
            return []
        
        return sorted_data[:top_n]
    
    @classmethod
    def fetch_stock_institutional_history(cls, stock_code, days=30):
        """
        ä½¿ç”¨ FinMind å–å¾—å€‹è‚¡æ³•äººæ­·å²è²·è³£è¶…è³‡æ–™
        å›å‚³: list of dict, æŒ‰æ—¥æœŸé™åºæ’åˆ—
        """
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=days)).strftime("%Y-%m-%d")
        
        try:
            params = {
                "dataset": "TaiwanStockInstitutionalInvestorsBuySell",
                "data_id": stock_code,
                "start_date": start_date,
                "end_date": end_date,
                "token": FINMIND_TOKEN
            }
            
            resp = requests.get(FINMIND_URL, params=params, timeout=15, verify=False)
            data = resp.json()
            
            if data.get('status') != 200 or 'data' not in data:
                return []
            
            # æ•´ç†è³‡æ–™ - å°‡åŒä¸€æ—¥çš„ä¸åŒæ³•äººè³‡æ–™åˆä½µ
            daily_data = {}
            for row in data['data']:
                date = row['date']
                name = row['name']
                buy = row.get('buy', 0) or 0
                sell = row.get('sell', 0) or 0
                net = buy - sell
                
                if date not in daily_data:
                    daily_data[date] = {'date': date, 'foreign_net': 0, 'trust_net': 0, 'dealer_net': 0}
                
                if 'Foreign' in name:
                    daily_data[date]['foreign_net'] += net
                elif 'Investment_Trust' in name:
                    daily_data[date]['trust_net'] += net
                elif 'Dealer' in name:
                    daily_data[date]['dealer_net'] += net
            
            # è½‰æ›ç‚º list ä¸¦æŒ‰æ—¥æœŸé™åºæ’åˆ—
            result = list(daily_data.values())
            result.sort(key=lambda x: x['date'], reverse=True)
            return result
            
        except Exception as e:
            print_flush(f"âš  å–å¾— {stock_code} æ³•äººæ­·å²è³‡æ–™å¤±æ•—: {e}")
            return []
    
    @classmethod
    def calculate_consecutive_days(cls, stock_code, investor_type='foreign'):
        """
        è¨ˆç®—é€£çºŒè²·è¶…/è³£è¶…å¤©æ•¸
        investor_type: 'foreign' æˆ– 'trust'
        å›å‚³: æ­£æ•¸=é€£çºŒè²·è¶…å¤©æ•¸, è² æ•¸=é€£çºŒè³£è¶…å¤©æ•¸
        """
        history = cls.fetch_stock_institutional_history(stock_code, days=60)
        
        if not history:
            return 0
        
        key = f'{investor_type}_net'
        consecutive = 0
        direction = None  # True=è²·è¶…, False=è³£è¶…
        
        for day_data in history:
            net = day_data.get(key, 0)
            
            if net == 0:
                break  # é‡åˆ° 0 å‰‡åœæ­¢è¨ˆç®—
            
            current_direction = net > 0
            
            if direction is None:
                direction = current_direction
                consecutive = 1 if direction else -1
            elif current_direction == direction:
                consecutive += 1 if direction else -1
            else:
                break  # æ–¹å‘æ”¹è®Šï¼Œåœæ­¢è¨ˆç®—
        
        return consecutive
    
    @classmethod
    def get_stock_institutional_signal(cls, stock_code):
        """
        å–å¾—å€‹è‚¡æ³•äººè¨Šè™Ÿ (ç”¨æ–¼é¡¯ç¤ºåœ¨è¨Šè™Ÿä¸­)
        å›å‚³: dict åŒ…å« foreign_days, trust_days, latest_foreign, latest_trust
        """
        foreign_days = cls.calculate_consecutive_days(stock_code, 'foreign')
        trust_days = cls.calculate_consecutive_days(stock_code, 'trust')
        
        # å–å¾—æœ€æ–°ä¸€æ—¥è³‡æ–™
        history = cls.fetch_stock_institutional_history(stock_code, days=5)
        latest_foreign = history[0].get('foreign_net', 0) // 1000 if history else 0  # è½‰æ›ç‚ºå¼µ
        latest_trust = history[0].get('trust_net', 0) // 1000 if history else 0
        
        return {
            'foreign_days': foreign_days,
            'trust_days': trust_days,
            'latest_foreign': latest_foreign,  # æœ€æ–°ä¸€æ—¥å¤–è³‡è²·è³£è¶…(å¼µ)
            'latest_trust': latest_trust        # æœ€æ–°ä¸€æ—¥æŠ•ä¿¡è²·è³£è¶…(å¼µ)
        }


# ==============================
# èè³‡èåˆ¸ API (OpenAPI)
# ==============================
class MarginDataAPI:
    """èè³‡èåˆ¸è³‡æ–™ API (ä½¿ç”¨å®˜æ–¹ OpenAPI)"""
    
    # TWSE èè³‡èåˆ¸ OpenAPI (JSON, ä¸­æ–‡æ¬„ä½)
    TWSE_MARGIN_URL = "https://openapi.twse.com.tw/v1/exchangeReport/MI_MARGN"
    # TPEx èè³‡èåˆ¸ OpenAPI (JSON, è‹±æ–‡æ¬„ä½)  
    TPEX_MARGIN_URL = "https://www.tpex.org.tw/openapi/v1/tpex_mainboard_margin_balance"
    
    # TWSE æ¬„ä½æ˜ å°„ (ä¸­æ–‡ -> è‹±æ–‡)
    TWSE_FIELD_MAP = {
        "è‚¡ç¥¨ä»£è™Ÿ": "code",
        "è‚¡ç¥¨åç¨±": "name",
        "èè³‡è²·é€²": "margin_buy",
        "èè³‡è³£å‡º": "margin_sell",
        "èè³‡ç¾é‡‘å„Ÿé‚„": "margin_redemp",
        "èè³‡å‰æ—¥é¤˜é¡": "margin_balance_prev",
        "èè³‡ä»Šæ—¥é¤˜é¡": "margin_balance",
        "èè³‡é™é¡": "margin_quota",
        "èåˆ¸è²·é€²": "short_buy",
        "èåˆ¸è³£å‡º": "short_sell",
        "èåˆ¸ç¾åˆ¸å„Ÿé‚„": "short_redemp",
        "èåˆ¸å‰æ—¥é¤˜é¡": "short_balance_prev",
        "èåˆ¸ä»Šæ—¥é¤˜é¡": "short_balance",
        "èåˆ¸é™é¡": "short_quota",
        "è³‡åˆ¸äº’æŠµ": "offsetting",
        "è¨»è¨˜": "note"
    }
    
    # TPEx æ¬„ä½æ˜ å°„ (è‹±æ–‡ -> æ¨™æº–åŒ–)
    TPEX_FIELD_MAP = {
        "Date": "date",
        "SecuritiesCompanyCode": "code",
        "CompanyName": "name",
        "MarginPurchase": "margin_buy",
        "MarginSales": "margin_sell",
        "CashRedemption": "margin_redemp",
        "MarginPurchaseBalancePreviousDay": "margin_balance_prev",
        "MarginPurchaseBalance": "margin_balance",
        "MarginPurchaseQuota": "margin_quota",
        "MarginPurchaseUtilizationRate": "margin_util_rate",
        "ShortConvering": "short_buy",
        "ShortSale": "short_sell",
        "StockRedemption": "short_redemp",
        "ShortSaleBalancePreviousDay": "short_balance_prev",
        "ShortSaleBalance": "short_balance",
        "ShortSaleQuota": "short_quota",
        "ShortSaleUtilizationRate": "short_util_rate",
        "Offsetting": "offsetting",
        "Note": "note"
    }
    
    @classmethod
    def _parse_number(cls, s):
        """è§£ææ•¸å­—å­—ä¸²ï¼Œç§»é™¤é€—è™Ÿä¸¦è½‰ç‚ºæ•´æ•¸"""
        if s is None or s == "" or s == "--":
            return 0
        try:
            return int(str(s).replace(",", "").replace(" ", ""))
        except (ValueError, TypeError):
            return 0
    
    @classmethod
    def _parse_float(cls, s):
        """è§£ææµ®é»æ•¸å­—ä¸²"""
        if s is None or s == "" or s == "--":
            return 0.0
        try:
            return float(str(s).replace(",", "").replace(" ", ""))
        except (ValueError, TypeError):
            return 0.0
    
    @classmethod
    def fetch_twse_margin(cls, progress=None):
        """å¾ TWSE å–å¾—èè³‡èåˆ¸è³‡æ–™ (ç¶²é ç‰ˆå„ªå…ˆï¼Œæ›´å³æ™‚)"""
        results = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        }
        
        today = datetime.now().strftime("%Y%m%d")
        
        # === ä¸»è¦ä¾†æº: MI_MARGN ç¶²é ç‰ˆ API (æ›´å³æ™‚) ===
        try:
            url = f"https://www.twse.com.tw/exchangeReport/MI_MARGN?response=json&date={today}&selectType=ALL"
            resp = requests.get(url, headers=headers, timeout=30, verify=False)
            data = resp.json()
            
            if data.get('stat') == 'OK' and data.get('data'):
                # è§£ææ—¥æœŸ
                date_str = data.get('date', today)
                if len(date_str) == 8:
                    date_int = int(date_str)
                else:
                    date_int = int(today)
                
                # æ¬„ä½: [ä»£è™Ÿ, åç¨±, èè³‡è²·é€², èè³‡è³£å‡º, èè³‡ç¾å„Ÿ, èè³‡å‰æ—¥é¤˜é¡, èè³‡ä»Šæ—¥é¤˜é¡, èè³‡é™é¡, ...]
                for row in data['data']:
                    if len(row) >= 16:
                        code = str(row[0]).strip()
                        if not code or not code.isdigit() or len(code) > 4:
                            continue
                        
                        record = {
                            "code": code,
                            "name": str(row[1]).strip(),
                            "date_int": date_int,
                            "margin_buy": cls._parse_number(row[2]),
                            "margin_sell": cls._parse_number(row[3]),
                            "margin_redemp": cls._parse_number(row[4]),
                            "margin_balance": cls._parse_number(row[6]),
                            "margin_quota": cls._parse_number(row[7]),
                            "short_buy": cls._parse_number(row[8]),
                            "short_sell": cls._parse_number(row[9]),
                            "short_redemp": cls._parse_number(row[10]),
                            "short_balance": cls._parse_number(row[12]),
                            "short_quota": cls._parse_number(row[13]),
                            "offsetting": cls._parse_number(row[14]) if len(row) > 14 else 0,
                            "margin_util_rate": 0.0,
                            "short_util_rate": 0.0,
                            "market": "TWSE"
                        }
                        
                        # è¨ˆç®—ä½¿ç”¨ç‡
                        if record["margin_quota"] > 0:
                            record["margin_util_rate"] = round(record["margin_balance"] / record["margin_quota"] * 100, 2)
                        if record["short_quota"] > 0:
                            record["short_util_rate"] = round(record["short_balance"] / record["short_quota"] * 100, 2)
                        
                        results.append(record)
                
                if results:
                    return results
                    
        except Exception as e:
            logger.debug(f"TWSE MI_MARGN ç¶²é ç‰ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨ OpenAPI å‚™æ´")
        
        # === å‚™æ´: OpenAPI ===
        try:
            resp = requests.get(cls.TWSE_MARGIN_URL, headers=headers, timeout=30, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            if not isinstance(data, list):
                return results
            
            today_int = int(datetime.now().strftime("%Y%m%d"))
            
            for item in data:
                code = item.get("è‚¡ç¥¨ä»£è™Ÿ", "").strip()
                if not code or not code.isdigit() or len(code) > 4:
                    continue
                
                record = {
                    "code": code,
                    "name": item.get("è‚¡ç¥¨åç¨±", "").strip(),
                    "date_int": today_int,
                    "margin_buy": cls._parse_number(item.get("èè³‡è²·é€²")),
                    "margin_sell": cls._parse_number(item.get("èè³‡è³£å‡º")),
                    "margin_redemp": cls._parse_number(item.get("èè³‡ç¾é‡‘å„Ÿé‚„")),
                    "margin_balance": cls._parse_number(item.get("èè³‡ä»Šæ—¥é¤˜é¡")),
                    "margin_quota": cls._parse_number(item.get("èè³‡é™é¡")),
                    "short_buy": cls._parse_number(item.get("èåˆ¸è²·é€²")),
                    "short_sell": cls._parse_number(item.get("èåˆ¸è³£å‡º")),
                    "short_redemp": cls._parse_number(item.get("èåˆ¸ç¾åˆ¸å„Ÿé‚„")),
                    "short_balance": cls._parse_number(item.get("èåˆ¸ä»Šæ—¥é¤˜é¡")),
                    "short_quota": cls._parse_number(item.get("èåˆ¸é™é¡")),
                    "offsetting": cls._parse_number(item.get("è³‡åˆ¸äº’æŠµ")),
                    "margin_util_rate": 0.0,
                    "short_util_rate": 0.0,
                    "market": "TWSE"
                }
                
                if record["margin_quota"] > 0:
                    record["margin_util_rate"] = round(record["margin_balance"] / record["margin_quota"] * 100, 2)
                if record["short_quota"] > 0:
                    record["short_util_rate"] = round(record["short_balance"] / record["short_quota"] * 100, 2)
                
                results.append(record)
                
        except Exception as e:
            logger.debug(f"TWSE èè³‡èåˆ¸ OpenAPI ä¹Ÿå¤±æ•—: {e}")
        
        return results
    
    @classmethod
    def fetch_tpex_margin(cls, progress=None):
        """
        å¾ TPEx OpenAPI å–å¾—èè³‡èåˆ¸è³‡æ–™
        å›å‚³: list of dict
        """
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨ä¸‹è¼‰ TPEx èè³‡èåˆ¸è³‡æ–™ (OpenAPI)...", level=2)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
            
            resp = requests.get(cls.TPEX_MARGIN_URL, headers=headers, timeout=30, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            if not isinstance(data, list):
                logger.warning("TPEx èè³‡èåˆ¸ API å›å‚³éé™£åˆ—æ ¼å¼")
                return results
            
            for item in data:
                code = item.get("SecuritiesCompanyCode", "").strip()
                if not code or not code.isdigit():
                    continue
                
                # åªè™•ç†æ™®é€šè‚¡ (4ç¢¼æ•¸å­—)
                if len(code) > 4:
                    continue
                
                # è§£ææ—¥æœŸ (æ°‘åœ‹æ ¼å¼: 1141217 -> 20251217)
                date_str = str(item.get("Date", "")).strip()
                if len(date_str) == 7:
                    year = int(date_str[:3]) + 1911
                    date_int = int(f"{year}{date_str[3:]}")
                else:
                    date_int = int(datetime.now().strftime("%Y%m%d"))
                
                record = {
                    "code": code,
                    "name": item.get("CompanyName", "").strip(),
                    "date_int": date_int,
                    "margin_buy": cls._parse_number(item.get("MarginPurchase")),
                    "margin_sell": cls._parse_number(item.get("MarginSales")),
                    "margin_redemp": cls._parse_number(item.get("CashRedemption")),
                    "margin_balance": cls._parse_number(item.get("MarginPurchaseBalance")),
                    "margin_quota": cls._parse_number(item.get("MarginPurchaseQuota")),
                    "short_buy": cls._parse_number(item.get("ShortConvering")),
                    "short_sell": cls._parse_number(item.get("ShortSale")),
                    "short_redemp": cls._parse_number(item.get("StockRedemption")),
                    "short_balance": cls._parse_number(item.get("ShortSaleBalance")),
                    "short_quota": cls._parse_number(item.get("ShortSaleQuota")),
                    "offsetting": cls._parse_number(item.get("Offsetting")),
                    "margin_util_rate": cls._parse_float(item.get("MarginPurchaseUtilizationRate")),
                    "short_util_rate": cls._parse_float(item.get("ShortSaleUtilizationRate")),
                    "market": "TPEx"
                }
                
                results.append(record)
            
            if progress:
                progress.success(f"âœ“ TPEx èè³‡èåˆ¸: {len(results)} ç­†", level=2)
            
        except requests.exceptions.RequestException as e:
            logger.error(f"TPEx èè³‡èåˆ¸ API è«‹æ±‚å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TPEx èè³‡èåˆ¸ API å¤±æ•—: {e}", level=2)
        except Exception as e:
            logger.error(f"TPEx èè³‡èåˆ¸è³‡æ–™è§£æå¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TPEx èè³‡èåˆ¸è§£æå¤±æ•—: {e}", level=2)
        
        return results
    
    @classmethod
    def save_to_db(cls, data_list):
        """
        å°‡èè³‡èåˆ¸è³‡æ–™å„²å­˜åˆ°è³‡æ–™åº«
        """
        if not data_list:
            return 0
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                insert_sql = """
                    INSERT OR REPLACE INTO margin_data 
                    (date_int, code, margin_buy, margin_sell, margin_redemp, 
                     margin_balance, margin_util_rate, short_buy, short_sell, 
                     short_redemp, short_balance, short_util_rate)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                records = [
                    (
                        d["date_int"], d["code"], d["margin_buy"], d["margin_sell"],
                        d["margin_redemp"], d["margin_balance"], d["margin_util_rate"],
                        d["short_buy"], d["short_sell"], d["short_redemp"],
                        d["short_balance"], d["short_util_rate"]
                    )
                    for d in data_list
                ]
                
                cur.executemany(insert_sql, records)
                
                # åŒæ­¥æ›´æ–° stock_snapshot (æœ€æ–°ä¸€ç­†)
                snapshot_updates = []
                for d in data_list:
                    snapshot_updates.append((
                        d["margin_balance"], d["margin_util_rate"],
                        d["short_balance"], d["short_util_rate"],
                        d["code"]
                    ))
                
                cur.executemany("""
                    UPDATE stock_snapshot 
                    SET margin_balance = ?, margin_util_rate = ?,
                        short_balance = ?, short_util_rate = ?
                    WHERE code = ?
                """, snapshot_updates)
                
                conn.commit()
                return len(records)
                
        except Exception as e:
            logger.error(f"å„²å­˜èè³‡èåˆ¸è³‡æ–™å¤±æ•—: {e}")
            return 0
    
    @classmethod
    def fetch_all_margin_data(cls, progress=None):
        """
        å–å¾—æ‰€æœ‰(ä¸Šå¸‚+ä¸Šæ«ƒ)èè³‡èåˆ¸è³‡æ–™ä¸¦å„²å­˜
        """
        twse_data = cls.fetch_twse_margin(progress)
        tpex_data = cls.fetch_tpex_margin(progress)
        
        all_data = twse_data + tpex_data
        
        if all_data:
            saved = cls.save_to_db(all_data)
            if progress:
                progress.success(f"âœ“ èè³‡èåˆ¸è³‡æ–™å·²å„²å­˜: {saved} ç­†", level=3)
            return saved
        
        return 0
    
    @classmethod
    def get_stock_margin_data(cls, stock_code, days=30):
        """
        å¾è³‡æ–™åº«å–å¾—å€‹è‚¡èè³‡èåˆ¸æ­·å²è³‡æ–™
        """
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                cur.execute("""
                    SELECT date_int, margin_balance, margin_util_rate,
                           short_balance, short_util_rate, 
                           margin_buy, margin_sell, short_buy, short_sell
                    FROM margin_data
                    WHERE code = ?
                    ORDER BY date_int DESC
                    LIMIT ?
                """, (stock_code, days))
                
                results = []
                for row in cur.fetchall():
                    results.append({
                        "date_int": row[0],
                        "margin_balance": row[1],
                        "margin_util_rate": row[2],
                        "short_balance": row[3],
                        "short_util_rate": row[4],
                        "margin_buy": row[5],
                        "margin_sell": row[6],
                        "short_buy": row[7],
                        "short_sell": row[8]
                    })
                
                return results
                
        except Exception as e:
            logger.error(f"æŸ¥è©¢èè³‡èåˆ¸è³‡æ–™å¤±æ•—: {e}")
            return []
    
    @classmethod
    def get_latest_margin_data(cls, stock_code):
        """
        å–å¾—å€‹è‚¡æœ€æ–°èè³‡èåˆ¸è³‡æ–™
        """
        history = cls.get_stock_margin_data(stock_code, days=1)
        return history[0] if history else None


# ==============================
# PE/PB ä¼°å€¼ API (OpenAPI)
# ==============================
class PePbDataAPI:
    """PE/PB ä¼°å€¼è³‡æ–™ API (ä½¿ç”¨å®˜æ–¹ OpenAPI)"""
    
    # TWSE PE/PB OpenAPI (JSON, ä¸­æ–‡æ¬„ä½)
    TWSE_PEPB_URL = "https://openapi.twse.com.tw/v1/exchangeReport/BWIBBU_d"
    # TPEx PE/PB OpenAPI (JSON, è‹±æ–‡æ¬„ä½)
    TPEX_PEPB_URL = "https://www.tpex.org.tw/openapi/v1/tpex_mainboard_peratio_analysis"
    
    @classmethod
    def _parse_float(cls, s):
        """è§£ææµ®é»æ•¸å­—ä¸²"""
        if s is None or s == "" or s == "--" or s == "-":
            return None
        try:
            return float(str(s).replace(",", "").replace(" ", ""))
        except (ValueError, TypeError):
            return None
    
    @classmethod
    def fetch_twse_pepb(cls, progress=None):
        """å¾ TWSE å–å¾—ä»Šæ—¥ PE/PB è³‡æ–™ (ç¶²é ç‰ˆå„ªå…ˆï¼Œæ›´å³æ™‚)"""
        results = []
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json'
        }
        
        # === ä¸»è¦ä¾†æº: BWIBBU_ALL ç¶²é ç‰ˆ API (æ›´å³æ™‚) ===
        try:
            url = "https://www.twse.com.tw/exchangeReport/BWIBBU_ALL?response=json"
            resp = requests.get(url, headers=headers, timeout=30, verify=False)
            data = resp.json()
            
            if data.get('stat') == 'OK' and data.get('data'):
                # è§£ææ—¥æœŸ
                date_str = data.get('date', '')
                if len(date_str) == 8:
                    date_int = int(date_str)
                else:
                    date_int = int(datetime.now().strftime("%Y%m%d"))
                
                # æ¬„ä½: [ä»£è™Ÿ, åç¨±, æ®–åˆ©ç‡, è‚¡åˆ©å¹´åº¦, æœ¬ç›Šæ¯”, è‚¡åƒ¹æ·¨å€¼æ¯”, ...]
                for row in data['data']:
                    if len(row) >= 6:
                        code = str(row[0]).strip()
                        if not code or not code.isdigit() or len(code) > 4:
                            continue
                        
                        results.append({
                            'code': code,
                            'name': str(row[1]).strip(),
                            'date_int': date_int,
                            'yield_rate': cls._parse_float(row[2]),
                            'pe': cls._parse_float(row[4]),
                            'pb': cls._parse_float(row[5]),
                            'market': 'TWSE'
                        })
                
                if results:
                    return results
                    
        except Exception as e:
            logger.debug(f"TWSE BWIBBU_ALL ç¶²é ç‰ˆå¤±æ•—: {e}ï¼Œä½¿ç”¨ OpenAPI å‚™æ´")
        
        # === å‚™æ´: OpenAPI ===
        try:
            resp = requests.get(cls.TWSE_PEPB_URL, headers=headers, timeout=30, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            if not isinstance(data, list):
                return results
            
            today_int = int(datetime.now().strftime("%Y%m%d"))
            
            for item in data:
                code = str(item.get("è­‰åˆ¸ä»£è™Ÿ", "")).strip()
                if not code or not code.isdigit() or len(code) > 4:
                    continue
                
                results.append({
                    'code': code,
                    'name': str(item.get("è­‰åˆ¸åç¨±", "")).strip(),
                    'date_int': today_int,
                    'pe': cls._parse_float(item.get("æœ¬ç›Šæ¯”")),
                    'pb': cls._parse_float(item.get("è‚¡åƒ¹æ·¨å€¼æ¯”")),
                    'yield_rate': cls._parse_float(item.get("æ®–åˆ©ç‡(%)")),
                    'market': 'TWSE'
                })
                
        except Exception as e:
            logger.debug(f"TWSE PE/PB OpenAPI ä¹Ÿå¤±æ•—: {e}")
        
        return results
    
    @classmethod
    def fetch_tpex_pepb(cls, progress=None):
        """å¾ TPEx OpenAPI å–å¾—ä»Šæ—¥ PE/PB è³‡æ–™"""
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ TPEx OpenAPI å–å¾— PE/PB è³‡æ–™...", level=2)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Accept': 'application/json'
            }
            
            resp = requests.get(cls.TPEX_PEPB_URL, headers=headers, timeout=30, verify=False)
            resp.raise_for_status()
            
            data = resp.json()
            if not isinstance(data, list):
                logger.warning("TPEx PE/PB OpenAPI å›å‚³éé™£åˆ—æ ¼å¼")
                return results
            
            for item in data:
                code = str(item.get("SecuritiesCompanyCode", "")).strip()
                if not code or not code.isdigit() or len(code) > 4:
                    continue
                
                # è§£ææ—¥æœŸ
                date_str = str(item.get("Date", "")).strip()
                if len(date_str) == 7:
                    year = int(date_str[:3]) + 1911
                    date_int = int(f"{year}{date_str[3:]}")
                else:
                    date_int = int(datetime.now().strftime("%Y%m%d"))
                
                results.append({
                    'code': code,
                    'name': str(item.get("CompanyName", "")).strip(),
                    'date_int': date_int,
                    'pe': cls._parse_float(item.get("PriceEarningRatio")),
                    'pb': cls._parse_float(item.get("PriceBookRatio")),
                    'yield_rate': cls._parse_float(item.get("DividendYield")),
                    'market': 'TPEx'
                })
            
            if progress:
                progress.success(f"âœ“ TPEx PE/PB: {len(results)} ç­†", level=2)
                
        except Exception as e:
            logger.error(f"TPEx PE/PB OpenAPI å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TPEx PE/PB OpenAPI å¤±æ•—: {e}", level=2)
        
        return results
    
    @classmethod
    def save_to_db(cls, data_list):
        """å°‡ PE/PB è³‡æ–™å„²å­˜åˆ°è³‡æ–™åº«ä¸¦åŒæ­¥è‡³ stock_snapshot"""
        if not data_list:
            return 0
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                # åŒæ­¥æ›´æ–° stock_snapshot
                for d in data_list:
                    cur.execute("""
                        UPDATE stock_snapshot 
                        SET pe = ?, pb = ?, yield = ?
                        WHERE code = ?
                    """, (d['pe'], d['pb'], d.get('yield_rate'), d['code']))
                
                conn.commit()
                return len(data_list)
                
        except Exception as e:
            logger.error(f"å„²å­˜ PE/PB è³‡æ–™å¤±æ•—: {e}")
            return 0
    
    @classmethod
    def fetch_from_finmind(cls, progress=None):
        """å¾ FinMind å–å¾— PE/PB è³‡æ–™ (å‚™æ´)"""
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ FinMind å–å¾— PE/PB è³‡æ–™ (å‚™æ´)...", level=2)
            
            # å–å¾—ä»Šå¤©çš„è³‡æ–™
            today = datetime.now().strftime("%Y-%m-%d")
            
            params = {
                'dataset': 'TaiwanStockPER',
                'start_date': today,
                'end_date': today,
                'token': FINMIND_TOKEN
            }
            
            resp = requests.get("https://api.finmindtrade.com/api/v4/data", 
                              params=params, timeout=30)
            resp.raise_for_status()
            
            data = resp.json()
            if data.get('msg') != 'success' or not data.get('data'):
                logger.warning("FinMind PE/PB å›å‚³ç„¡è³‡æ–™")
                return results
            
            today_int = int(today.replace('-', ''))
            
            for item in data['data']:
                code = str(item.get('stock_id', '')).strip()
                if not code or len(code) > 4:
                    continue
                
                results.append({
                    'code': code,
                    'name': '',
                    'date_int': today_int,
                    'pe': cls._parse_float(item.get('PER')),
                    'pb': cls._parse_float(item.get('PBR')),
                    'yield_rate': cls._parse_float(item.get('dividend_yield')),
                    'market': 'FinMind'
                })
            
            if progress:
                progress.success(f"âœ“ FinMind PE/PB: {len(results)} ç­†", level=2)
                
        except Exception as e:
            logger.error(f"FinMind PE/PB å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— FinMind PE/PB å¤±æ•—: {e}", level=2)
        
        return results
    
    @classmethod
    def fetch_all_pepb(cls, progress=None):
        """å–å¾—æ‰€æœ‰ PE/PB è³‡æ–™ä¸¦å„²å­˜ (OpenAPI å„ªå…ˆï¼ŒFinMind å‚™æ´)"""
        # å…ˆå˜—è©¦å®˜æ–¹ OpenAPI
        twse_data = cls.fetch_twse_pepb(progress)
        tpex_data = cls.fetch_tpex_pepb(progress)
        
        all_data = twse_data + tpex_data
        
        # è‹¥ OpenAPI å¤±æ•—æˆ–ç„¡è³‡æ–™ï¼Œä½¿ç”¨ FinMind å‚™æ´
        if not all_data:
            if progress:
                progress.info("OpenAPI ç„¡è³‡æ–™ï¼Œåˆ‡æ›è‡³ FinMind å‚™æ´...", level=1)
            all_data = cls.fetch_from_finmind(progress)
        
        if all_data:
            saved = cls.save_to_db(all_data)
            if progress:
                progress.success(f"âœ“ PE/PB è³‡æ–™å·²å„²å­˜: {saved} ç­†", level=3)
            return saved
        
        return 0


# ==============================
# é›†ä¿æˆ¶æ•¸ API (FinMind + TDCC å‚™æ´)
# ==============================
class ShareholderDataAPI:
    """é›†ä¿æˆ¶æ•¸è³‡æ–™ API (FinMind ç‚ºä¸»ï¼ŒTDCC CSV ç‚ºå‚™æ´)"""
    
    # FinMind API
    FINMIND_URL = "https://api.finmindtrade.com/api/v4/data"
    # TDCC CSV (å‚™æ´)
    TDCC_CSV_URL = "https://smart.tdcc.com.tw/opendata/getOD.ashx?id=1-5"
    
    @classmethod
    def fetch_from_finmind(cls, progress=None):
        """å¾ FinMind å–å¾—é›†ä¿æˆ¶æ•¸è³‡æ–™"""
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ FinMind å–å¾—é›†ä¿æˆ¶æ•¸è³‡æ–™...", level=1)
            
            # å–å¾—æœ€è¿‘ä¸€é€±çš„è³‡æ–™
            end_date = datetime.now().strftime("%Y-%m-%d")
            start_date = (datetime.now() - timedelta(days=7)).strftime("%Y-%m-%d")
            
            params = {
                'dataset': 'TaiwanStockHoldingSharesPer',
                'start_date': start_date,
                'end_date': end_date,
                'token': FINMIND_TOKEN
            }
            
            resp = requests.get(cls.FINMIND_URL, params=params, timeout=30)
            resp.raise_for_status()
            
            data = resp.json()
            if data.get('msg') != 'success' or not data.get('data'):
                logger.warning("FinMind é›†ä¿æˆ¶æ•¸å›å‚³ç„¡è³‡æ–™")
                return results
            
            # è™•ç†è³‡æ–™ï¼šè¨ˆç®—åƒå¼µå¤§æˆ¶æŒè‚¡æ¯”ä¾‹ (æŒè‚¡ç´šè· 15: 1000å¼µä»¥ä¸Š)
            df = pd.DataFrame(data['data'])
            if df.empty:
                return results
            
            # å–æœ€æ–°æ—¥æœŸçš„è³‡æ–™
            latest_date = df['date'].max()
            df_latest = df[df['date'] == latest_date]
            
            # è¨ˆç®—åƒå¼µå¤§æˆ¶æ¯”ä¾‹ (HoldingSharesLevel == 15)
            major_holders = df_latest[df_latest['HoldingSharesLevel'] == 15].groupby('stock_id')['percent'].sum()
            
            date_int = int(latest_date.replace('-', ''))
            
            for code, pct in major_holders.items():
                results.append({
                    'code': str(code),
                    'date_int': date_int,
                    'major_holders_pct': round(pct, 2),
                    'source': 'FinMind'
                })
            
            if progress:
                progress.success(f"âœ“ FinMind é›†ä¿æˆ¶æ•¸: {len(results)} ç­†", level=1)
                
        except Exception as e:
            logger.error(f"FinMind é›†ä¿æˆ¶æ•¸å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— FinMind é›†ä¿æˆ¶æ•¸å¤±æ•—: {e}", level=1)
        
        return results
    
    @classmethod
    def fetch_from_tdcc_csv(cls, progress=None):
        """å¾ TDCC CSV å–å¾—é›†ä¿æˆ¶æ•¸è³‡æ–™ (å‚™æ´)"""
        results = []
        try:
            if progress:
                progress.info("æ­£åœ¨å¾ TDCC CSV å–å¾—é›†ä¿æˆ¶æ•¸è³‡æ–™ (å‚™æ´)...", level=2)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            # æ³¨æ„ï¼šTDCC ç¶²ç«™æœ‰é‡å®šå‘å•é¡Œï¼Œéœ€ç¦æ­¢è‡ªå‹•é‡å®šå‘
            resp = requests.get(cls.TDCC_CSV_URL, headers=headers, timeout=30, verify=False, allow_redirects=False)
            resp.raise_for_status()
            
            # è§£æ CSV
            from io import StringIO
            df = pd.read_csv(StringIO(resp.text))
            
            if df.empty:
                logger.warning("TDCC CSV ç„¡è³‡æ–™")
                return results
            
            # å¿…è¦æ¬„ä½æª¢æŸ¥
            if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns or 'æŒè‚¡åˆ†ç´š' not in df.columns:
                logger.warning("TDCC CSV æ ¼å¼ä¸ç¬¦")
                return results
            
            # è™•ç†è³‡æ–™
            df['æŒè‚¡åˆ†ç´š'] = pd.to_numeric(df['æŒè‚¡åˆ†ç´š'], errors='coerce')
            df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].astype(str)
            
            # åƒå¼µå¤§æˆ¶ (æŒè‚¡åˆ†ç´š 15)
            df_major = df[df['æŒè‚¡åˆ†ç´š'] == 15].copy()
            
            if 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%' in df.columns:
                pct_col = 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%'
            elif 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹' in df.columns:
                pct_col = 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹'
            else:
                logger.warning("TDCC CSV æ‰¾ä¸åˆ°æ¯”ä¾‹æ¬„ä½")
                return results
            
            major_holders = df_major.groupby('è­‰åˆ¸ä»£è™Ÿ')[pct_col].sum()
            
            today_int = int(datetime.now().strftime("%Y%m%d"))
            
            for code, pct in major_holders.items():
                if len(str(code)) > 4:
                    continue
                results.append({
                    'code': str(code),
                    'date_int': today_int,
                    'major_holders_pct': round(float(pct), 2),
                    'source': 'TDCC'
                })
            
            if progress:
                progress.success(f"âœ“ TDCC CSV é›†ä¿æˆ¶æ•¸: {len(results)} ç­†", level=2)
                
        except Exception as e:
            logger.error(f"TDCC CSV é›†ä¿æˆ¶æ•¸å¤±æ•—: {e}")
            if progress:
                progress.error(f"âœ— TDCC CSV å¤±æ•—: {e}", level=2)
        
        return results
    
    @classmethod
    def save_to_db(cls, data_list):
        """å°‡é›†ä¿æˆ¶æ•¸è³‡æ–™åŒæ­¥è‡³ stock_snapshot"""
        if not data_list:
            return 0
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                for d in data_list:
                    cur.execute("""
                        UPDATE stock_snapshot 
                        SET major_holders_pct = ?
                        WHERE code = ?
                    """, (d['major_holders_pct'], d['code']))
                
                conn.commit()
                return len(data_list)
                
        except Exception as e:
            logger.error(f"å„²å­˜é›†ä¿æˆ¶æ•¸å¤±æ•—: {e}")
            return 0
    
    @classmethod
    def fetch_all_shareholder(cls, progress=None):
        """å–å¾—æ‰€æœ‰é›†ä¿æˆ¶æ•¸è³‡æ–™ (ç›´æ¥ä½¿ç”¨ TDCC CSV)"""
        # ç›´æ¥ä½¿ç”¨ TDCC CSV (FinMind å¸³è™Ÿç­‰ç´šé™åˆ¶ï¼Œå·²ç§»é™¤)
        data = cls.fetch_from_tdcc_csv(progress)
        
        if data:
            saved = cls.save_to_db(data)
            if progress:
                progress.success(f"âœ“ é›†ä¿æˆ¶æ•¸å·²å„²å­˜: {saved} ç­†", level=3)
            return saved
        
        return 0


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                   DOMAIN/INDICATORS                           â•‘
# â•‘  ç´”å‡½æ•¸æŒ‡æ¨™è¨ˆç®—ï¼šcalc_indicators(df) -> List[Dict]             â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# ==============================
# ç´”å‡½æ•¸æŒ‡æ¨™è¨ˆç®— (Phase 3)
# ==============================
def calc_indicators_pure(df: pd.DataFrame, display_days: int = 30) -> List[Dict]:
    """
    ç´”å‡½æ•¸ï¼šè¨ˆç®—æŠ€è¡“æŒ‡æ¨™ (ç„¡ DB å‰¯ä½œç”¨)
    
    Args:
        df: åŒ…å« date, open, high, low, close, volume, amount çš„ DataFrame
        display_days: è¿”å›æœ€è¿‘ N å¤©çš„æŒ‡æ¨™
        
    Returns:
        List[Dict]: æ¯æ—¥æŒ‡æ¨™å­—å…¸åˆ—è¡¨
    """
    # [Guard Clause] è¡›èªå¥ - è³‡æ–™ä¸è¶³å‰‡æ—©é€€
    if df is None or df.empty:
        return []
    if len(df) < 20:
        return []
    
    # ç¢ºä¿æ—¥æœŸæ’åº
    if 'date' in df.columns:
        df = df.sort_values('date').reset_index(drop=True)
    elif 'date_int' in df.columns:
        df = df.sort_values('date_int').reset_index(drop=True)
    
    # è¨ˆç®—å‡ç·š
    df = df.copy()
    df['ma5'] = df['close'].rolling(5).mean()
    df['ma20'] = df['close'].rolling(20).mean()
    df['ma60'] = df['close'].rolling(60).mean()
    df['ma120'] = df['close'].rolling(120).mean()
    df['ma200'] = df['close'].rolling(200).mean()
    
    # è¨ˆç®—æˆäº¤é‡å‡ç·š
    df['vol_ma5'] = df['volume'].rolling(5).mean()
    df['vol_ma20'] = df['volume'].rolling(20).mean()
    df['vol_ma60'] = df['volume'].rolling(60).mean()
    
    # è¨ˆç®— RSI (ä½¿ç”¨ç°¡åŒ–ç‰ˆ)
    delta = df['close'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))
    
    # è¨ˆç®— MFI (ç°¡åŒ–ç‰ˆ)
    typical_price = (df['high'] + df['low'] + df['close']) / 3
    money_flow = typical_price * df['volume']
    positive_flow = money_flow.where(typical_price > typical_price.shift(1), 0)
    negative_flow = money_flow.where(typical_price < typical_price.shift(1), 0)
    mfi_ratio = positive_flow.rolling(14).sum() / negative_flow.rolling(14).sum()
    df['mfi14'] = 100 - (100 / (1 + mfi_ratio))
    
    # è¨ˆç®—ä¹–é›¢ç‡
    df['bias20'] = ((df['close'] - df['ma20']) / df['ma20'] * 100) if 'ma20' in df.columns else None
    
    # è½‰æ›ç‚º List[Dict]
    results = []
    for i in range(max(0, len(df) - display_days), len(df)):
        row = df.iloc[i]
        results.append({
            'date_int': int(row.get('date_int', 0)) if 'date_int' in df.columns else None,
            'close': round(row['close'], 2) if pd.notna(row['close']) else None,
            'volume': int(row['volume']) if pd.notna(row['volume']) else 0,
            'ma5': round(row['ma5'], 2) if pd.notna(row.get('ma5')) else None,
            'ma20': round(row['ma20'], 2) if pd.notna(row.get('ma20')) else None,
            'ma60': round(row['ma60'], 2) if pd.notna(row.get('ma60')) else None,
            'ma120': round(row['ma120'], 2) if pd.notna(row.get('ma120')) else None,
            'ma200': round(row['ma200'], 2) if pd.notna(row.get('ma200')) else None,
            'rsi': round(row['rsi'], 2) if pd.notna(row.get('rsi')) else None,
            'mfi14': round(row['mfi14'], 2) if pd.notna(row.get('mfi14')) else None,
            'bias20': round(row['bias20'], 2) if pd.notna(row.get('bias20')) else None,
        })
    
    return results


# ==============================
# æŒ‡æ¨™è¨ˆç®—é¡åˆ¥
# ==============================
class IndicatorCalculator:
    @staticmethod
    def calculate_wma(series, period):
        """å‘é‡åŒ– WMA è¨ˆç®—"""
        if len(series) < period:
            return np.full(len(series), np.nan)
        
        weights = np.arange(1, period + 1)
        wma_valid = np.convolve(series, weights[::-1], mode='valid') / weights.sum()
        
        nans = np.full(period - 1, np.nan)
        return np.concatenate((nans, wma_valid))

    @staticmethod
    def calculate_wma_for_df(df, period):
        """è¨ˆç®— DataFrame çš„ WMA"""
        if df.empty or len(df) < period:
            return None
        
        try:
            vals = df['close'].dropna().values
            wma = IndicatorCalculator.calculate_wma(vals, period)
            return round(wma[-1], 2) if not np.isnan(wma[-1]) else None
        except:
            return None

    @staticmethod
    def calculate_macd_series(df, fast=12, slow=26, signal=9):
        """è¨ˆç®— MACD æŒ‡æ¨™åºåˆ—"""
        if df.empty or len(df) < slow:
            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)
        
        try:
            close_prices = df['close'].values
            wma_fast = IndicatorCalculator.calculate_wma(close_prices, fast)
            wma_slow = IndicatorCalculator.calculate_wma(close_prices, slow)
            
            macd_line = wma_fast - wma_slow
            signal_line = IndicatorCalculator.calculate_wma(macd_line, signal)
            
            return pd.Series(macd_line, index=df.index), pd.Series(signal_line, index=df.index)
        except:
            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)

    @staticmethod
    def calculate_ma(df, period):
        """è¨ˆç®—ç§»å‹•å¹³å‡ç·š"""
        if df.empty or len(df) < period:
            return None
        
        ma = df['close'].rolling(window=period).mean().iloc[-1]
        return round(ma, 2) if not pd.isna(ma) else None

    @staticmethod
    def calculate_rsi(df, period=14):
        """è¨ˆç®— RSI"""
        if df.empty or len(df) < period + 1:
            return None
        
        try:
            deltas = np.diff(df['close'].values)
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            
            avg_gain = IndicatorCalculator.calculate_wma(gains, period)[-1]
            avg_loss = IndicatorCalculator.calculate_wma(losses, period)[-1]
            
            if avg_loss == 0:
                return 100.0 if avg_gain > 0 else 50.0
            
            rs = avg_gain / avg_loss
            return round(100 - (100 / (1 + rs)), 2)
        except:
            return None

    @staticmethod
    def calculate_rsi_series(df, period=14):
        """è¨ˆç®— RSI æŒ‡æ¨™åºåˆ—"""
        if df.empty or len(df) < period + 1:
            return pd.Series(np.nan, index=df.index)
        
        try:
            deltas = np.diff(df['close'].values)
            gains = np.where(deltas > 0, deltas, 0)
            losses = np.where(deltas < 0, -deltas, 0)
            
            gains = np.insert(gains, 0, 0)
            losses = np.insert(losses, 0, 0)
            
            avg_gains = IndicatorCalculator.calculate_wma(gains, period)
            avg_losses = IndicatorCalculator.calculate_wma(losses, period)
            
            with np.errstate(divide='ignore', invalid='ignore'):
                rs = avg_gains / avg_losses
                rsi_values = 100 - (100 / (1 + rs))
            
            rsi_values = np.where(avg_losses == 0, 
                                  np.where(avg_gains > 0, 100.0, 50.0), 
                                  rsi_values)
            
            rsi_values = np.where(np.isnan(avg_gains) | np.isnan(avg_losses), np.nan, rsi_values)
            
            return pd.Series(rsi_values, index=df.index)
        except Exception as e:
            return pd.Series(np.nan, index=df.index)

    @staticmethod
    def calculate_macd(df, fast=12, slow=26, signal=9):
        """è¨ˆç®— MACD"""
        if df.empty or len(df) < slow:
            return None, None
        
        try:
            closes = df['close'].values
            wma_f = IndicatorCalculator.calculate_wma(closes, fast)
            wma_s = IndicatorCalculator.calculate_wma(closes, slow)
            
            macd_line = wma_f - wma_s
            valid_macd = macd_line[slow-1:]
            
            if len(valid_macd) < signal:
                return round(macd_line[-1], 2), None
            
            sig_vals = IndicatorCalculator.calculate_wma(valid_macd, signal)
            return round(macd_line[-1], 2), round(sig_vals[-1], 2)
        except:
            return None, None

    @staticmethod
    def calculate_mfi(df, period=14):
        """è¨ˆç®— MFI"""
        if df.empty or len(df) < period:
            return pd.Series(np.nan, index=df.index)
        
        try:
            tp = (df['high'] + df['low'] + df['close']) / 3
            mf = tp * df['volume']
            
            pos = np.where(tp > tp.shift(1), mf, 0)
            neg = np.where(tp < tp.shift(1), mf, 0)
            
            pos_wma = IndicatorCalculator.calculate_wma(pos, period)
            neg_wma = IndicatorCalculator.calculate_wma(neg, period)
            
            with np.errstate(divide='ignore', invalid='ignore'):
                ratio = pos_wma / neg_wma
                mfi = 100 - (100 / (1 + ratio))
            
            mfi = np.where(neg_wma == 0, 
                           np.where(pos_wma > 0, 100.0, 50.0), 
                           mfi)
            
            mfi = np.where(np.isnan(pos_wma) | np.isnan(neg_wma), 50.0, mfi)
            
            return pd.Series(mfi, index=df.index)
        except:
            return pd.Series(np.full(len(df), 50.0), index=df.index)

    @staticmethod
    def calculate_vwap_series(df, lookback=20):
        """è¨ˆç®— VWAP åºåˆ—"""
        if df.empty or len(df) < lookback:
            return pd.Series(np.nan, index=df.index)
        
        try:
            tp = (df['high'] + df['low'] + df['close']) / 3
            vwap_values = (tp * df['volume']).rolling(lookback).sum() / df['volume'].rolling(lookback).sum()
            return vwap_values.fillna(method='bfill')
        except:
            return pd.Series(np.nan, index=df.index)


    @staticmethod
    def calculate_chg14_series(df):
        """è¨ˆç®—14æ—¥è®ŠåŒ–ç‡åºåˆ—"""
        if df.empty or len(df) < 14:
            return pd.Series(np.nan, index=df.index)
        
        try:
            chg = (df['close'] - df['close'].shift(14)) / df['close'].shift(14) * 100
            return chg.fillna(0)
        except:
            return pd.Series(np.nan, index=df.index)

    @staticmethod
    def calculate_monthly_kd_series(df, k_period=9, d_period=3):
        """è¨ˆç®—æœˆKDåºåˆ—"""
        if df.empty or len(df) < k_period:
            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)
        
        try:
            low_min = df['low'].rolling(k_period).min()
            high_max = df['high'].rolling(k_period).max()
            rsv = (df['close'] - low_min) / (high_max - low_min) * 100
            rsv = rsv.fillna(50)
            
            k_vals = rsv.ewm(span=d_period, adjust=False).mean()
            d_vals = k_vals.ewm(span=d_period, adjust=False).mean()
            
            return k_vals.fillna(50), d_vals.fillna(50)
        except:
            return pd.Series(50.0, index=df.index), pd.Series(50.0, index=df.index)

    @staticmethod
    def calculate_daily_kd_series(df, k_period=9, d_period=3):
        """è¨ˆç®—æ—¥KDåºåˆ—"""
        return IndicatorCalculator.calculate_monthly_kd_series(df, k_period, d_period)

    @staticmethod
    def calculate_weekly_kd_series(df, k_period=9, d_period=3):
        """è¨ˆç®—é€±KDåºåˆ—"""
        return IndicatorCalculator.calculate_monthly_kd_series(df, k_period * 5, d_period)

    @staticmethod
    def calculate_smart_score_series(df):
        """è¨ˆç®—æ™ºæ…§åˆ†æ•¸åºåˆ—"""
        if df.empty:
            empty = pd.Series(0, index=df.index)
            return empty, empty, empty, empty, empty, empty, empty
        
        try:
            # Simplified smart score calculation
            score = pd.Series(50, index=df.index)
            smi_sig = pd.Series(0, index=df.index)
            nvi_sig = pd.Series(0, index=df.index)
            vsa_sig = pd.Series(0, index=df.index)
            svi_sig = pd.Series(0, index=df.index)
            vol_div_sig = pd.Series(0, index=df.index)
            weekly_nvi_sig = pd.Series(0, index=df.index)
            
            return score, smi_sig, nvi_sig, vsa_sig, svi_sig, vol_div_sig, weekly_nvi_sig
        except:
            empty = pd.Series(0, index=df.index)
            return empty, empty, empty, empty, empty, empty, empty

    @staticmethod
    def calculate_smi_series(df, period=14):
        """è¨ˆç®—SMIåºåˆ—"""
        if df.empty or len(df) < period:
            return pd.Series(np.nan, index=df.index)
        
        try:
            high_low_avg = (df['high'].rolling(period).max() + df['low'].rolling(period).min()) / 2
            smi = (df['close'] - high_low_avg) / (df['high'].rolling(period).max() - df['low'].rolling(period).min()) * 100
            return smi.fillna(0)
        except:
            return pd.Series(0, index=df.index)

    @staticmethod
    def calculate_nvi_series(df):
        """è¨ˆç®—NVIåºåˆ—"""
        if df.empty or len(df) < 2:
            return pd.Series(1000, index=df.index), pd.Series(1000, index=df.index)
        
        try:
            nvi = pd.Series(1000.0, index=df.index)
            for i in range(1, len(df)):
                if df['volume'].iloc[i] < df['volume'].iloc[i-1]:
                    pct_change = (df['close'].iloc[i] - df['close'].iloc[i-1]) / df['close'].iloc[i-1]
                    nvi.iloc[i] = nvi.iloc[i-1] * (1 + pct_change)
                else:
                    nvi.iloc[i] = nvi.iloc[i-1]
            
            nvi_ma = nvi.rolling(50).mean()
            return nvi, nvi_ma
        except:
            return pd.Series(1000, index=df.index), pd.Series(1000, index=df.index)

    @staticmethod
    def calculate_adl_series(df):
        """è¨ˆç®—ADLåºåˆ—"""
        if df.empty:
            return pd.Series(0, index=df.index)
        
        try:
            mfm = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low'])
            mfm = mfm.fillna(0)
            mfv = mfm * df['volume']
            adl = mfv.cumsum()
            return adl
        except:
            return pd.Series(0, index=df.index)

    @staticmethod
    def calculate_rs_series(df, period=14):
        """è¨ˆç®—ç›¸å°å¼·åº¦åºåˆ—"""
        if df.empty or len(df) < period:
            return pd.Series(50, index=df.index)
        
        try:
            returns = df['close'].pct_change()
            pos_returns = returns.where(returns > 0, 0)
            neg_returns = -returns.where(returns < 0, 0)
            
            avg_gain = pos_returns.rolling(period).mean()
            avg_loss = neg_returns.rolling(period).mean()
            
            rs = avg_gain / (avg_loss + 1e-10)
            rs_score = 100 - (100 / (1 + rs))
            return rs_score.fillna(50)
        except:
            return pd.Series(50, index=df.index)

    @staticmethod
    def calculate_pvi_series(df):
        """è¨ˆç®—PVIåºåˆ—"""
        if df.empty or len(df) < 2:
            return pd.Series(1000, index=df.index)
        
        try:
            pvi = pd.Series(1000.0, index=df.index)
            for i in range(1, len(df)):
                if df['volume'].iloc[i] > df['volume'].iloc[i-1]:
                    pct_change = (df['close'].iloc[i] - df['close'].iloc[i-1]) / df['close'].iloc[i-1]
                    pvi.iloc[i] = pvi.iloc[i-1] * (1 + pct_change)
                else:
                    pvi.iloc[i] = pvi.iloc[i-1]
            return pvi
        except:
            return pd.Series(1000, index=df.index)

    @staticmethod
    def calculate_clv_series(df):
        """è¨ˆç®—CLVåºåˆ—"""
        if df.empty:
            return pd.Series(0, index=df.index)
        
        try:
            clv = ((df['close'] - df['low']) - (df['high'] - df['close'])) / (df['high'] - df['low'])
            return clv.fillna(0)
        except:
            return pd.Series(0, index=df.index)

    @staticmethod
    def calculate_3day_divergence_series(df):
        """è¨ˆç®—3æ—¥èƒŒé›¢åºåˆ—"""
        if df.empty or len(df) < 3:
            return pd.Series(0, index=df.index), pd.Series(0, index=df.index)
        
        try:
            bull = pd.Series(0, index=df.index)
            bear = pd.Series(0, index=df.index)
            
            # Simple divergence: price down but volume up = bullish
            # price up but volume down = bearish
            price_change = df['close'].diff(3)
            vol_change = df['volume'].diff(3)
            
            bull = ((price_change < 0) & (vol_change > 0)).astype(int)
            bear = ((price_change > 0) & (vol_change < 0)).astype(int)
            
            return bull, bear
        except:
            return pd.Series(0, index=df.index), pd.Series(0, index=df.index)


    @staticmethod
    def calculate_vp_scheme3(df, lookback=20):
        """è¨ˆç®— Volume Profile (POC, VP_upper, VP_lower)"""
        result = {'POC': None, 'VP_upper': None, 'VP_lower': None}
        
        if df.empty or len(df) < 2:
            return result
        
        try:
            # Use recent data
            recent = df.tail(lookback) if len(df) >= lookback else df
            
            if len(recent) < 2:
                return result
                
            # Calculate typical price and volume profile
            high = recent['high'].max()
            low = recent['low'].min()
            
            if high == low:
                result['POC'] = high
                result['VP_upper'] = high
                result['VP_lower'] = low
                return result
            
            # Simple POC calculation - price with highest volume
            price_levels = 10
            step = (high - low) / price_levels
            
            volume_at_price = {}
            for i in range(price_levels):
                price_low = low + i * step
                price_high = low + (i + 1) * step
                mid_price = (price_low + price_high) / 2
                
                mask = (recent['close'] >= price_low) & (recent['close'] < price_high)
                vol = recent.loc[mask, 'volume'].sum()
                volume_at_price[mid_price] = vol
            
            if volume_at_price:
                poc_price = max(volume_at_price, key=volume_at_price.get)
                result['POC'] = round(poc_price, 2)
            else:
                result['POC'] = round(recent['close'].iloc[-1], 2)
            
            # Calculate value area (70% of volume)
            total_vol = sum(volume_at_price.values())
            if total_vol > 0:
                sorted_prices = sorted(volume_at_price.items(), key=lambda x: x[1], reverse=True)
                cumulative = 0
                value_area_prices = []
                
                for price, vol in sorted_prices:
                    cumulative += vol
                    value_area_prices.append(price)
                    if cumulative >= total_vol * 0.7:
                        break
                
                if value_area_prices:
                    result['VP_upper'] = round(max(value_area_prices) + step/2, 2)
                    result['VP_lower'] = round(min(value_area_prices) - step/2, 2)
                else:
                    result['VP_upper'] = round(high, 2)
                    result['VP_lower'] = round(low, 2)
            else:
                result['VP_upper'] = round(high, 2)
                result['VP_lower'] = round(low, 2)
            
            return result
        except Exception as e:
            return result


    @staticmethod
    def calculate_vsbc_bands(df, win=10):
        """è¨ˆç®— VSBC ä¸Šä¸‹é€šé“"""
        if df.empty or len(df) < win:
            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)
        
        try:
            # è¨ˆç®—æˆäº¤é‡æƒ…ç·’
            signed_vol = np.where(df['close'] >= df['open'], df['volume'], -df['volume'])
            signed_vol = pd.Series(signed_vol, index=df.index)

            # æƒ…ç·’æ¨åŠ›å¹³å‡ & å¹³å‡æˆäº¤é‡
            vs_force = signed_vol.rolling(win, min_periods=1).mean()
            vol_mean = df['volume'].rolling(win, min_periods=1).mean()

            # ç®±é«”åŸºç¤
            base_mid = (df['high'] + df['low']) / 2
            base_range = (df['high'] - df['low']).rolling(win, min_periods=1).mean().replace(0, 1e-9)

            # ä¸­ç·šä½ç§»ï¼ˆé˜²çˆ†ç¯„åœ -0.5 ~ 0.5ï¼‰
            shift = (vs_force / vol_mean).fillna(0).clip(-0.5, 0.5)

            vsbc_mid = base_mid + shift * base_range
            
            # å‡è¨­é€šé“å¯¬åº¦ç‚º 1 å€ base_range (ä¸Šä¸‹å„ 0.5)
            # æˆ–è€…æ ¹æ“šåŸå§‹é‚è¼¯ï¼ŒVSBC ä¸»è¦æ˜¯ä¸­ç·šï¼Œé€™è£¡æˆ‘å€‘å®šç¾©ä¸€å€‹é€šé“ä¾›åƒè€ƒ
            upper = vsbc_mid + base_range * 0.5
            lower = vsbc_mid - base_range * 0.5
            
            return upper, lower
        except:
            return pd.Series(np.nan, index=df.index), pd.Series(np.nan, index=df.index)

    @staticmethod
    def calculate_pattern_morning_star(df):
        """
        æ—©æ™¨ä¹‹æ˜Ÿ (Morning Star) - åº•éƒ¨åè½‰
        T-2: é•·é»‘ K
        T-1: æ˜Ÿç·š (å¯¦é«”å°, æ”¶ç›¤ < T-2 æ”¶ç›¤)
        T: é•·ç´… K (æ”¶ç›¤ > T-2 å¯¦é«”ä¸­é»)
        """
        if len(df) < 3:
            return pd.Series([False] * len(df), index=df.index)
            
        close = df['close']
        open_ = df['open']
        high = df['high']
        low = df['low']
        
        body = (close - open_).abs()
        candle_range = high - low
        
        c2 = close.shift(2)
        o2 = open_.shift(2)
        body2 = body.shift(2)
        range2 = candle_range.shift(2)
        
        c1 = close.shift(1)
        o1 = open_.shift(1)
        body1 = body.shift(1)
        
        c0 = close
        o0 = open_
        body0 = body
        range0 = candle_range
        
        is_long_black_2 = (c2 < o2) & (body2 > range2 * 0.6)
        is_star_1 = (body1 < body2 * 0.3) & (c1 < c2)
        mid_point_2 = (o2 + c2) / 2
        is_long_red_0 = (c0 > o0) & (c0 > mid_point_2) & (body0 > range0 * 0.6)
        
        return is_long_black_2 & is_star_1 & is_long_red_0

    @staticmethod
    def calculate_pattern_evening_star(df):
        """
        é»ƒæ˜ä¹‹æ˜Ÿ (Evening Star) - é ‚éƒ¨åè½‰
        T-2: é•·ç´… K
        T-1: æ˜Ÿç·š (å¯¦é«”å°, æ”¶ç›¤ > T-2 æ”¶ç›¤)
        T: é•·é»‘ K (æ”¶ç›¤ < T-2 å¯¦é«”ä¸­é»)
        """
        if len(df) < 3:
            return pd.Series([False] * len(df), index=df.index)
            
        close = df['close']
        open_ = df['open']
        high = df['high']
        low = df['low']
        
        body = (close - open_).abs()
        candle_range = high - low
        
        c2 = close.shift(2)
        o2 = open_.shift(2)
        body2 = body.shift(2)
        range2 = candle_range.shift(2)
        
        c1 = close.shift(1)
        o1 = open_.shift(1)
        body1 = body.shift(1)
        
        c0 = close
        o0 = open_
        body0 = body
        range0 = candle_range
        
        is_long_red_2 = (c2 > o2) & (body2 > range2 * 0.6)
        is_star_1 = (body1 < body2 * 0.3) & (c1 > c2)
        mid_point_2 = (o2 + c2) / 2
        is_long_black_0 = (c0 < o0) & (c0 < mid_point_2) & (body0 > range0 * 0.6)
        
        return is_long_red_2 & is_star_1 & is_long_black_0


class TaiwanStockScreenerAdvanced:
    """
    å°è‚¡äº”éšç¯©é¸å™¨ (å®Œæ•´ä¿®æ­£ç‰ˆ - æ•´åˆç‰ˆ)
    - å¸‚å ´ç’°å¢ƒä½œç‚ºå‹•æ…‹èª¿æ•´å› å­
    - æ°¸é å°‹æ‰¾å¸‚å ´ä¸­çš„ç›¸å°å¼·å‹¢è‚¡
    - é»‘å¤©éµé˜²è­·æ©Ÿåˆ¶
    """
    
    def __init__(self, db_conn):
        self.conn = db_conn
        self.base_params = {
            'min_relative_strength': 0.15,
        }
        self.current_params = self.base_params.copy()

    def calculate_technical_indicators(self, df):
        if df.empty or len(df) < 60: return df
        df = df.copy()
        
        # VWAP 60
        tp = (df['High'] + df['Low'] + df['Close']) / 3
        df['vwap_60'] = (tp * df['Volume']).rolling(60).sum() / df['Volume'].rolling(60).sum()
        
        # BBW
        df['ma20'] = df['Close'].rolling(20).mean()
        df['std20'] = df['Close'].rolling(20).std()
        df['upper_bb'] = df['ma20'] + 2 * df['std20']
        df['lower_bb'] = df['ma20'] - 2 * df['std20']
        df['bbw'] = (df['upper_bb'] - df['lower_bb']) / df['ma20']
        
        # KD
        lowest_low = df['Low'].rolling(9).min()
        highest_high = df['High'].rolling(9).max()
        df['rsv'] = 100 * (df['Close'] - lowest_low) / (highest_high - lowest_low)
        df['k'] = df['rsv'].ewm(alpha=1/3, adjust=False).mean()
        df['d'] = df['k'].ewm(alpha=1/3, adjust=False).mean()
        
        # Weekly (Resample)
        # Ensure index is datetime
        if not isinstance(df.index, pd.DatetimeIndex):
             # Try to convert if 'date' column exists, else return
             if 'date' in df.columns:
                 df['date'] = pd.to_datetime(df['date'])
                 df.set_index('date', inplace=True)
        
        try:
            weekly = df.resample('W').agg({'Open': 'first', 'Close': 'last'})
            df['weekly_close'] = weekly['Close'].reindex(df.index, method='ffill')
            df['weekly_open'] = weekly['Open'].reindex(df.index, method='ffill')
        except:
            df['weekly_close'] = df['Close']
            df['weekly_open'] = df['Open']
            
        return df

    def calculate_relative_strength(self, stock_df, twii_df):
        if len(stock_df) < 20 or len(twii_df) < 20: return 0.0
        try:
            s_ret = (stock_df['Close'].iloc[-1] / stock_df['Close'].iloc[-20]) - 1
            m_ret = (twii_df['Close'].iloc[-1] / twii_df['Close'].iloc[-20]) - 1
            rs = s_ret - m_ret
            if m_ret < 0 and s_ret > 0: rs += 0.3
            if m_ret > 0 and s_ret > m_ret * 1.5: rs += 0.2
            return rs
        except: return 0.0

    def market_filter(self, twii_df):
        if twii_df is None or len(twii_df) < 60:
            return {'market_score': 50, 'adjustment_factor': 1.0}
            
        score = 50
        ma60 = twii_df['Close'].rolling(60).mean().iloc[-1]
        if twii_df['Close'].iloc[-1] > ma60: score += 15
        else: score -= 15
        
        adj = 1.0
        if score > 70: adj = 1.2
        elif score < 40: adj = 0.8
        
        # Adjust params based on market
        self.current_params = self.base_params.copy()
        if score < 50:
            self.current_params['min_relative_strength'] = 0.25
        elif score > 70:
            self.current_params['min_relative_strength'] = 0.10
            
        return {'market_score': score, 'adjustment_factor': adj}

    def stock_strength_filter(self, df, adj=1.0):
        # Step 1: Strength (0-100)
        latest = df.iloc[-1]
        score = 0
        
        # Price > VWAP60 (20)
        if latest['Close'] > latest['vwap_60'] * adj: score += 20
        # Price > MA20 (20)
        if latest['Close'] > latest['ma20']: score += 20
        # Weekly Red (20)
        if latest['weekly_close'] > latest['weekly_open']: score += 20
        # Volume > 500 (20) - Basic check
        if latest['Volume'] > 500: score += 20
        # Trend (20)
        if latest['Close'] > df.iloc[-20]['Close']: score += 20
        
        return score >= 60, score

    def smart_money_validation(self, df, adj=1.0):
        # Step 2: Smart Money (0-100)
        latest = df.iloc[-1]
        score = 0
        
        # BBW Tight (< 0.15) (40)
        if latest['bbw'] < 0.15: score += 40
        # Volume > MA5 (30)
        vol_ma5 = df['Volume'].rolling(5).mean().iloc[-1]
        if latest['Volume'] > vol_ma5: score += 30
        # Price Stable (30)
        if abs(latest['Close'] - df.iloc[-5]['Close'])/df.iloc[-5]['Close'] < 0.05: score += 30
        
        return score >= 60, score

    def value_zone_filter(self, df):
        # Step 3: Value (0-100)
        latest = df.iloc[-1]
        score = 0
        
        # Near MA20 (< 5%) (40)
        dist = abs(latest['Close'] - latest['ma20']) / latest['ma20']
        if dist < 0.05: score += 40
        # KD Low (< 50) (30)
        if latest['k'] < 50: score += 30
        # Volume Shrink (30)
        if latest['Volume'] < df.iloc[-2]['Volume']: score += 30
        
        return score >= 50, score

    def entry_trigger(self, df):
        # Step 4: Trigger (0-100)
        latest = df.iloc[-1]
        prev = df.iloc[-2]
        score = 0
        
        # Bullish Candle (30)
        if latest['Close'] > latest['Open']: score += 30
        # Volume Surge (> 1.5x) (30)
        if latest['Volume'] > prev['Volume'] * 1.5: score += 30
        # KD Cross (20)
        if latest['k'] > latest['d']: score += 20
        # Price > Prev High (20)
        if latest['Close'] > prev['High']: score += 20
        
        return score >= 50, score


def run_five_stage_screener():
    """åŸ·è¡Œäº”éšç¯©é¸å™¨ (é¸å–®å…¥å£)"""
    print_flush("\n" + "="*60)
    print_flush("äº”éšç¯©é¸å™¨ (åƒå•ç‰ˆ - æœ¬åœ°è³‡æ–™åº«)")
    print_flush("="*60)
    
    date_str = get_latest_market_date()
    print_flush(f"ğŸ“… ä½¿ç”¨æœ€æ–°è³‡æ–™æ—¥æœŸ: {date_str}")
    
    # ç²å–ä½¿ç”¨è€…è¼¸å…¥çš„æƒæåƒæ•¸
    limit, min_vol = get_user_scan_params()
        
    try:
        screener = TaiwanStockScreenerAdvanced(db_path=Config.DB_PATH)
        results = screener.screen_all_stocks(date_str, max_stocks=None)
        print_flush(f"\nâœ¨ ç¯©é¸å®Œæˆ! å…±è™•ç† {len(results)} æª”è‚¡ç¥¨ã€‚")
    except Exception as e:
        print_flush(f"âŒ ç¯©é¸éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()

class InstitutionalValueStrategy:
    """
    æ©Ÿæ§‹åƒ¹å€¼å›æ­¸ç­–ç•¥ (geminiç‰ˆ)
    ç­–ç•¥æ ¸å¿ƒ:
    1. å¸‚å ´æ¿¾ç¶² (Step 0): å¤§ç›¤å¤šé ­ + å¸‚å ´é¨°è½ç·š (Market ADL) å¥åº·
    2. è¶¨å‹¢å¼·å¼± (Step 1): WMA200/VWAP200 å¤šé ­ + RS > RS_MA200 (å¼·æ–¼å¤§ç›¤) + é€±KDå‘ä¸Š
    3. ç±Œç¢¼é–å®š (Step 2): æŠ•ä¿¡å¤–è³‡è²·è¶… (çœŸä¸»åŠ›) + ADL åº•èƒŒé›¢ (éš±æ€§å¸ç±Œ)
    4. åƒ¹å€¼å›æ­¸ (Step 3): å›æ¸¬ Fib 0.618 / VP / WMA20
    5. å‹•èƒ½è§¸ç™¼ (Step 4): æ—¥KDä½æª”é‡‘å‰ + é‡å¢
    """
    
    def __init__(self):
        self.market_df = None
        self.market_adl_status = False
        
    def get_connection(self):
        return db_manager.get_connection()

    def _wma(self, series, period):
        """åŠ æ¬Šç§»å‹•å¹³å‡ (Weighted Moving Average)"""
        if len(series) < period: return pd.Series(np.nan, index=series.index)
        weights = np.arange(1, period + 1)
        return series.rolling(period).apply(lambda x: np.dot(x, weights) / weights.sum(), raw=True)

    def _vwap(self, df, period=200):
        """æˆäº¤é‡åŠ æ¬Šå¹³å‡åƒ¹ (VWAP)"""
        try:
            typical_price = (df['High'] + df['Low'] + df['Close']) / 3
            pv = typical_price * df['Volume']
            cum_pv = pv.rolling(period).sum()
            cum_vol = df['Volume'].rolling(period).sum()
            return cum_pv / cum_vol
        except:
            return pd.Series(np.nan, index=df.index)

    def _mansfield_rs(self, stock_close, market_close, period=200):
        """æ›¼æ–¯è²çˆ¾å¾·ç›¸å°å¼·åº¦ (Standardized RS)"""
        try:
            # ç¢ºä¿ç´¢å¼•å°é½Š
            m_aligned = market_close.reindex(stock_close.index).fillna(method='ffill')
            # åŸå§‹æ¯”ç‡
            raw_rs = stock_close / m_aligned
            # åŸºæº–ç·š (MA200 of Ratio)
            base = raw_rs.rolling(period).mean()
            return raw_rs, base
        except:
            return pd.Series(0, index=stock_close.index), pd.Series(0, index=stock_close.index)

    def _stock_adl(self, df):
        """Chaikin A/D Line (ç´¯ç©æ´¾ç™¼ç·š)"""
        try:
            clv = ((df['Close'] - df['Low']) - (df['High'] - df['Close'])) / (df['High'] - df['Low'])
            clv = clv.fillna(0)
            return (clv * df['Volume']).cumsum()
        except:
            return pd.Series(0, index=df.index)

    def _fibonacci_pivots(self, df, lookback=60):
        """è¨ˆç®—æ³¢æ®µé«˜ä½é»èˆ‡è²»æ³¢é‚£å¥‘å›æ’¤"""
        if len(df) < lookback: return None
        recent = df.iloc[-lookback:]
        high = recent['High'].max()
        low = recent['Low'].min()
        diff = high - low
        if diff == 0: return None
        return {
            '0.618': high - (diff * 0.618) # é»ƒé‡‘è²·é»
        }

    def _kd(self, df, period=9):
        """KDæŒ‡æ¨™"""
        try:
            low_min = df['Low'].rolling(period).min()
            high_max = df['High'].rolling(period).max()
            rsv = (df['Close'] - low_min) / (high_max - low_min) * 100
            rsv = rsv.fillna(50)
            k = rsv.ewm(com=2, adjust=False).mean()
            d = k.ewm(com=2, adjust=False).mean()
            return k, d
        except:
            return pd.Series(50, index=df.index), pd.Series(50, index=df.index)

    def _weekly_kd(self, df):
        """é€±ç·š KD"""
        try:
            w_df = df.resample('W-FRI').agg({
                'Open': 'first', 'High': 'max', 'Low': 'min', 'Close': 'last'
            }).dropna()
            if len(w_df) < 9: return pd.Series(0, index=df.index), pd.Series(0, index=df.index)
            wk, wd = self._kd(w_df)
            return wk.reindex(df.index, method='ffill'), wd.reindex(df.index, method='ffill')
        except:
            return pd.Series(0, index=df.index), pd.Series(0, index=df.index)

    def calculate_market_adl(self):
        """è¨ˆç®—å…¨å¸‚å ´ ADL (Market Breadth)"""
        print_flush("\n[Step 1] è¨ˆç®—å…¨å¸‚å ´é¨°è½ç·š (Market ADL)...")
        try:
            with self.get_connection() as conn:
                # ç°¡å–®è¨ˆç®—æ¯æ—¥æ¼²è·Œå®¶æ•¸
                df = pd.read_sql("""
                    SELECT date_int, 
                           SUM(CASE WHEN close > open THEN 1 ELSE 0 END) as rise,
                           SUM(CASE WHEN close < open THEN 1 ELSE 0 END) as fall
                    FROM stock_history 
                    WHERE code != '0000' AND code != 'TAIEX'
                    GROUP BY date_int
                    ORDER BY date_int
                """, conn)
                
                if df.empty: return False
                
                df['net'] = df['rise'] - df['fall']
                df['adl'] = df['net'].cumsum()
                
                # åˆ¤æ–·ç‹€æ…‹ (ADL > MA20)
                curr_adl = df['adl'].iloc[-1]
                ma20_adl = df['adl'].rolling(20).mean().iloc[-1]
                self.market_adl_status = curr_adl > ma20_adl
                
                status = "å¥åº· (ADL > MA20)" if self.market_adl_status else "è­¦æˆ’ (ADL < MA20)"
                print_flush(f"Market ADL ç‹€æ…‹: {status}")
                return self.market_adl_status
        except Exception as e:
            print_flush(f"âš  Market ADL è¨ˆç®—å¤±æ•—: {e}")
            return False

    def load_market_index(self):
        """è¼‰å…¥å¤§ç›¤æŒ‡æ•¸ (0000 æˆ– TAIEX)"""
        try:
            with self.get_connection() as conn:
                df = pd.read_sql("SELECT date_int, close FROM stock_history WHERE code='0000' OR code='TAIEX' ORDER BY date_int", conn)
                if df.empty: return False
                
                df['date'] = pd.to_datetime(df['date_int'].astype(str), format='%Y%m%d')
                df.set_index('date', inplace=True)
                self.market_df = df['close']
                return True
        except:
            return False

    def scan(self):
        """åŸ·è¡Œç­–ç•¥æƒæ"""
        print_flush("\n[æ©Ÿæ§‹åƒ¹å€¼å›æ­¸ç­–ç•¥] V2.0")
        
        # ç²å–ä½¿ç”¨è€…è¼¸å…¥çš„æƒæåƒæ•¸
        limit, min_vol = get_user_scan_params()
        
        print_flush("\n[Step 1] æº–å‚™å¸‚å ´æ•¸æ“š...")
        
        # 1. æº–å‚™æ•¸æ“š
        if not self.load_market_index():
            print_flush("âŒ ç„¡æ³•è¼‰å…¥å¤§ç›¤æ•¸æ“š")
            return
            
        self.calculate_market_adl()
        
        # 2. ç²å–è‚¡ç¥¨æ¸…å–®
        with self.get_connection() as conn:
            codes = [r[0] for r in conn.execute("SELECT DISTINCT code FROM stock_history WHERE code NOT LIKE '0%' AND LENGTH(code)=4").fetchall()]
            # Fetch names
            try:
                names = {r[0]: r[1] for r in conn.execute("SELECT code, name FROM stock_meta").fetchall()}
            except:
                names = {}
        
        print_flush(f"ç›®æ¨™: {len(codes)} æª”è‚¡ç¥¨ (åˆ†æéœ€æ™‚è¼ƒé•·ï¼Œè«‹ç¨å€™...)")
        
        candidates = []
        processed = 0
        
        for code in codes:
            processed += 1
            if processed % 50 == 0:
                print_flush(f"  é€²åº¦: {processed}/{len(codes)}")
                
            try:
                # è¼‰å…¥å€‹è‚¡æ•¸æ“š
                with self.get_connection() as conn:
                    df = pd.read_sql(f"SELECT * FROM stock_history WHERE code='{code}' ORDER BY date_int", conn)
                    inst = pd.read_sql(f"SELECT date_int, foreign_buy-foreign_sell as f_net, trust_buy-trust_sell as t_net FROM institutional_investors WHERE code='{code}'", conn)
                
                if len(df) < 250: continue
                
                df['date'] = pd.to_datetime(df['date_int'].astype(str), format='%Y%m%d')
                df.set_index('date', inplace=True)
                
                df.rename(columns={'open':'Open', 'high':'High', 'low':'Low', 'close':'Close', 'volume':'Volume'}, inplace=True)
                
                # --- Step 1: è¶¨å‹¢èˆ‡å¼·å¼± ---
                df['wma20'] = self._wma(df['Close'], 20)
                df['wma200'] = self._wma(df['Close'], 200)
                df['vwap200'] = self._vwap(df, 200)
                
                # RS
                rs_val, rs_ma = self._mansfield_rs(df['Close'], self.market_df)
                
                # é€± KD
                wk, wd = self._weekly_kd(df)
                
                cur = df.iloc[-1]
                
                # 1. å¤šé ­çµæ§‹
                if not (cur['Close'] > cur['wma200'] and cur['Close'] > cur['vwap200']): continue
                # 2. å¼·æ–¼å¤§ç›¤
                if not (rs_val.iloc[-1] > rs_ma.iloc[-1]): continue
                # 3. é€±ç·šä¿è­·
                if not (wk.iloc[-1] > wd.iloc[-1]): continue
                
                # --- Step 2: ç±Œç¢¼ ---
                # Stock ADL
                df['stock_adl'] = self._stock_adl(df)
                
                # æ³•äººè¿‘5æ—¥
                inst_5d_net = 0
                if not inst.empty:
                    inst['date'] = pd.to_datetime(inst['date_int'].astype(str), format='%Y%m%d')
                    inst.set_index('date', inplace=True)
                    # Join
                    df_inst = df.join(inst, how='left').fillna(0)
                    inst_5d_net = (df_inst['f_net'] + df_inst['t_net']).rolling(5).sum().iloc[-1]
                
                # ADL åº•èƒŒé›¢ (è‚¡åƒ¹è·Œ ADL æ¼²) - ç°¡å–®åˆ¤æ–·è¿‘5æ—¥
                price_trend = df['Close'].iloc[-1] < df['Close'].iloc[-5]
                adl_trend = df['stock_adl'].iloc[-1] > df['stock_adl'].iloc[-5]
                adl_div = price_trend and adl_trend
                
                if not (inst_5d_net > 0 or adl_div): continue
                
                # --- Step 3: åƒ¹å€¼å€é–“ ---
                fibs = self._fibonacci_pivots(df)
                fib_0618 = fibs['0.618'] if fibs else 0
                
                dist_fib = abs(cur['Close'] - fib_0618) / cur['Close'] if fib_0618 else 1.0
                dist_wma = abs(cur['Close'] - cur['wma20']) / cur['Close']
                
                in_value_zone = (dist_fib < 0.05) or (dist_wma < 0.05) # æ”¾å¯¬è‡³ 5%
                
                # --- Step 4: å‹•èƒ½è§¸ç™¼ ---
                dk, dd = self._kd(df)
                kd_cross = (dk.iloc[-1] > dd.iloc[-1]) and (dk.iloc[-2] <= dd.iloc[-2])
                kd_low = dk.iloc[-1] < 60
                vol_up = cur['Volume'] > df['Volume'].iloc[-2]
                
                is_triggered = kd_cross and kd_low and vol_up
                
                if in_value_zone:
                    candidates.append({
                        'code': code,
                        'name': names.get(code, code),
                        'price': cur['Close'],
                        'inst': inst_5d_net,
                        'fib': fib_0618,
                        'status': "TRIGGERED" if is_triggered else "WAITING"
                    })
                    
            except Exception as e:
                continue
                
        # è¼¸å‡ºçµæœ
        print_flush(f"\n{'='*80}")
        print_flush(f"ã€æƒæçµæœã€‘ æ©Ÿæ§‹åƒ¹å€¼å›æ­¸ V2.0 (RS+ADL+Fib)")
        print_flush(f"ç¯©é¸æ¨™æº–: å¼·æ–¼å¤§ç›¤(RS) + ä¸»åŠ›è²·è¶…/èƒŒé›¢ + å›æ¸¬åƒ¹å€¼å€")
        print_flush(f"{'-'*80}")
        # Header: ä»£è™Ÿ åç¨± æ”¶ç›¤ æ©Ÿæ§‹ç±Œç¢¼ å›æ¸¬ä½ç½® ç‹€æ…‹
        print_flush(f"{'ä»£è™Ÿ':<6} {'åç¨±':<8} {'æ”¶ç›¤':<10} {'æ©Ÿæ§‹ç±Œç¢¼':<12} {'å›æ¸¬ä½ç½®':<12} {'ç‹€æ…‹':<10}")
        print_flush(f"{'-'*80}")
        
        triggered = [c for c in candidates if c['status'] == "TRIGGERED"]
        waiting = [c for c in candidates if c['status'] == "WAITING"]
        
        reset = reset_color()
        
        for c in triggered:
            # Color logic
            c_price = Colors.RED # Default
            price_str = f"{c_price}{c['price']:.2f}{reset}"
            inst_str = f"{c['inst']:,.0f}"
            fib_str = f"{c['fib']:.2f}"
            status_str = f"{Colors.RED}â˜…è§¸ç™¼è²·é»{reset}"
            
            print_flush(f"{c['code']:<6} {c['name']:<8} {price_str:<19} {inst_str:<14} {fib_str:<14} {status_str:<19}")
            
        if not triggered:
            print_flush(" (ç„¡è§¸ç™¼æ¨™çš„)")
            
        print_flush(f"\n--- è§€å¯Ÿåå–® (é€²å…¥åƒ¹å€¼å€ä½†æœªè§¸ç™¼) ---")
        for c in waiting[:10]:
            price_str = f"{c['price']:.2f}"
            inst_str = f"{c['inst']:,.0f}"
            fib_str = f"{c['fib']:.2f}"
            print_flush(f"{c['code']:<6} {c['name']:<8} {price_str:<10} {inst_str:<12} {fib_str:<12} ç­‰å¾…è½‰æŠ˜")
            
        print_flush(f"{'='*80}\n")

def run_institutional_value_strategy():
    """åŸ·è¡Œæ©Ÿæ§‹åƒ¹å€¼å›æ­¸ç­–ç•¥ (é¸å–®å…¥å£)"""
    try:
        strategy = InstitutionalValueStrategy()
        strategy.scan()
    except Exception as e:
        print_flush(f"âŒ ç­–ç•¥åŸ·è¡Œå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()


# åƒ¹é‡ç‹€æ…‹æŸ¥è¡¨ (Tuple Lookup)
PRICE_VOLUME_STATUS = {
    (1, 1): "åƒ¹æ¼²é‡å¢",
    (1, -1): "åƒ¹æ¼²é‡ç¸®",
    (-1, 1): "åƒ¹è·Œé‡å¢",
    (-1, -1): "åƒ¹è·Œé‡ç¸®"
}

def calculate_trade_setup(close, vp_upper, vp_lower, ma20, tp=0, sl=0):
    """
    è¨ˆç®—æ­¢ç›ˆæ­¢æ (è·è²¬åˆ†é›¢)
    :return: (tp, sl)
    """
    if tp == 0:
        if vp_upper and vp_upper > close:
            tp = vp_upper
        else:
            tp = close * 1.1
            
    if sl == 0:
        if vp_lower and vp_lower < close:
            sl = vp_lower
        elif ma20 and close > ma20:
            sl = ma20
        else:
            sl = close * 0.95
            
    return tp, sl


# ANSI Color Codes
class Colors:
    RESET = "\033[0m"
    RED = "\033[91m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    MAGENTA = "\033[95m"
    CYAN = "\033[96m"
    WHITE = "\033[97m"
    # æ¼²åœ/è·ŒåœèƒŒæ™¯è‰² (æ»¿ç‰ˆ)
    LIMIT_UP = "\033[97;41m"    # ç™½å­—ç´…åº• (æ¼²åœ)
    LIMIT_DOWN = "\033[97;42m"  # ç™½å­—ç¶ åº• (è·Œåœ)

def reset_color():
    return Colors.RESET

def get_color_code(val):
    """æ¼²è·Œé¡è‰²ï¼šæ¼²åœ=ç´…åº•ã€è·Œåœ=ç¶ åº•ã€æ¼²=ç´…å­—ã€è·Œ=ç¶ å­—"""
    if val >= 9.5: return Colors.LIMIT_UP    # æ¼²åœ (ç´…åº•ç™½å­—)
    elif val <= -9.5: return Colors.LIMIT_DOWN  # è·Œåœ (ç¶ åº•ç™½å­—)
    elif val > 0: return Colors.RED          # ä¸Šæ¼² (ç´…å­—)
    elif val < 0: return Colors.GREEN        # ä¸‹è·Œ (ç¶ å­—)
    return Colors.RESET

def get_arrow(curr, prev):
    """Get arrow symbol based on trend"""
    if curr is None or prev is None: return ""
    if curr > prev: return "â†‘"
    elif curr < prev: return "â†“"
    return "-"

def get_volume_color(ratio):
    """æˆäº¤é‡é¡è‰²ï¼šé‡å¢=ç´…ã€é‡ç¸®=ç¶ ã€çˆ†é‡=ç´«"""
    if ratio >= 2.0: return Colors.MAGENTA  # çˆ†é‡ (ç´«è‰²ç‰¹åˆ¥æ¨™ç¤º)
    elif ratio > 1.0: return Colors.RED     # é‡å¢ = ç´…è‰²
    elif ratio < 1.0: return Colors.GREEN   # é‡ç¸® = ç¶ è‰²
    return Colors.RESET  # æŒå¹³ = ç™½è‰²

def get_trend_color(curr, prev):
    """Get color for trend"""
    if curr is None or prev is None: return Colors.RESET
    if curr > prev: return Colors.RED
    elif curr < prev: return Colors.GREEN
    return Colors.RESET

def get_colored_value(text, change, arrow=""):
    """Format value with color and arrow"""
    color = get_color_code(change)
    return f"{color}{text}{arrow}{Colors.RESET}"

def calculate_trade_setup(close, vp_upper, vp_lower, ma20, tp_raw=None, sl_raw=None):
    """Calculate Take Profit and Stop Loss"""
    # Default logic if not provided
    if tp_raw: tp = tp_raw
    else: tp = vp_upper if vp_upper and vp_upper > close else close * 1.1
    
    if sl_raw: sl = sl_raw
    else: sl = vp_lower if vp_lower and vp_lower < close else (ma20 if ma20 and ma20 < close else close * 0.9)
    
    return tp, sl

PRICE_VOLUME_STATUS = {
    (1, 1): "åƒ¹æ¼²é‡å¢",
    (1, -1): "åƒ¹æ¼²é‡ç¸®",
    (-1, 1): "åƒ¹è·Œé‡å¢",
    (-1, -1): "åƒ¹è·Œé‡ç¸®",
    (0, 1): "å¹³ç›¤é‡å¢",
    (0, -1): "å¹³ç›¤é‡ç¸®"
}

def format_scan_result(code, name, indicators, show_date=False):
    """æ ¼å¼åŒ–å–®æ—¥æŠ€è¡“æŒ‡æ¨™ (å®¢è£½åŒ–ç‰ˆ - ä¿®å¾© RSI/VSBC/é›†ä¿äººæ•¸)"""
    if not indicators:
        return ""
    
    # è¼”åŠ©å‡½æ•¸
    def get_val(keys, default=None):
        if isinstance(keys, str): keys = [keys]
        for k in keys:
            if k in indicators and indicators[k] is not None:
                return indicators[k]
        return default

    def safe_f(val, default=0.0):
        try: return float(val) if val is not None else default
        except: return default

    # è®€å–æ•¸æ“š
    date = indicators.get('date', '')
    close = safe_f(get_val('close'))
    close_prev = safe_f(get_val('close_prev'))
    volume = safe_f(get_val('volume'))
    vol_prev = safe_f(get_val('vol_prev'))
    
    # æŒ‡æ¨™
    mfi = safe_f(get_val(['mfi14', 'MFI']))
    mfi_prev = safe_f(get_val(['mfi14_prev', 'MFI_prev']))
    chg14 = safe_f(get_val(['chg14_pct', 'CHG14']))
    chg14_prev = safe_f(get_val(['chg14_pct_prev', 'CHG14_prev']))
    
    # RSI è¨ˆç®— (è‹¥ indicators ä¸­æ²’æœ‰ï¼Œå˜—è©¦è¨ˆç®—)
    rsi = safe_f(get_val(['rsi', 'RSI']))
    if rsi == 0 and 'close' in indicators:
        # é€™è£¡ç„¡æ³•å–å¾—æ­·å²æ•¸æ“šè¨ˆç®— RSIï¼Œåªèƒ½é¡¯ç¤º N/A æˆ–ä¾è³´å¤–éƒ¨è¨ˆç®—
        # ç‚ºäº†é¿å… N/Aï¼Œæˆ‘å€‘å‡è¨­ scan å‡½æ•¸æ‡‰è©²è¨ˆç®—å¥½ RSI
        # è‹¥çœŸçš„æ²’æœ‰ï¼Œé¡¯ç¤ºç©ºå­—ä¸²
        rsi_str = ""
    else:
        rsi_str = f"{rsi:.1f}"

    poc = safe_f(get_val(['vp_poc', 'POC']))
    vwap = safe_f(get_val(['vwap20', 'VWAP']))
    vwap_prev = safe_f(get_val(['vwap20_prev', 'VWAP_prev']))
    
    major_pct = safe_f(get_val(['major_holders_pct', 'Major_Holders']))
    total_holders = safe_int(get_val(['total_shareholders', 'Total_Shareholders']))
    
    f_buy = safe_int(get_val(['foreign_buy', 'Foreign_Buy']))
    t_buy = safe_int(get_val(['trust_buy', 'Trust_Buy']))
    d_buy = safe_int(get_val(['dealer_buy', 'Dealer_Buy']))
    
    # VSBC
    vsbc_up = safe_f(get_val(['vsbc_up', 'VSBC_Upper']))
    vsbc_low = safe_f(get_val(['vsbc_low', 'VSBC_Lower']))
    if vsbc_up == 0 and vsbc_low == 0:
        vsbc_str = "N/A"
    else:
        vsbc_str = f"{vsbc_up:.1f}/{vsbc_low:.1f}"
    
    # å‡ç·š
    mas = {}
    for p in [20, 60, 120, 200]:
        mas[p] = safe_f(get_val([f'ma{p}', f'MA{p}']))

    # è¨ˆç®—
    change_pct = (close - close_prev) / close_prev * 100 if close_prev else 0
    volume_ratio = volume / vol_prev if vol_prev > 0 else 1.0
    vol_in_lots = volume / 1000
    
    # æ ¼å¼åŒ–
    reset = reset_color()
    vol_text = f"{get_volume_color(volume_ratio)}{int(vol_in_lots):,}å¼µ({volume_ratio:.1f}å€){reset}"
    
    mfi_arrow = get_arrow(mfi, mfi_prev)
    chg14_arrow = get_arrow(chg14, chg14_prev)
    vwap_arrow = get_arrow(vwap, vwap_prev)
    
    colored_mfi = get_colored_value(f"{mfi:.1f}", mfi - mfi_prev, mfi_arrow)
    # 14æ—¥æ¼²è·Œå¹…ï¼šä½¿ç”¨ chg14 æœ¬èº«çš„æ­£è² ä¾†æ±ºå®šé¡è‰²ï¼ˆæ­£=ç´…ã€è² =ç¶ ï¼‰
    colored_chg14 = get_colored_value(f"{chg14:.1f}%", chg14, chg14_arrow)
    colored_vwap = get_colored_value(f"{vwap:.2f}", vwap - vwap_prev, vwap_arrow)
    
    # RSI é¡è‰²
    colored_rsi = f"{rsi_str}" # æš«ä¸åŠ è‰²ï¼Œæˆ–å¯ä¾ >70 ç´… <30 ç¶ 
    
    # è¨Šè™Ÿ
    sig_desc = []
    price_dir = 1 if change_pct > 0 else (-1 if change_pct < 0 else 0)
    vol_dir = 1 if volume_ratio >= 1.0 else -1
    status = PRICE_VOLUME_STATUS.get((price_dir, vol_dir))
    if status: sig_desc.append(status)
    
    if safe_int(get_val(['smi_signal', 'SMI_Signal'])) == 1: sig_desc.append("ä¸»åŠ›é€²å ´")
    if safe_int(get_val(['svi_signal', 'SVI_Signal'])) == 1: sig_desc.append("å¤šé ­æ’åˆ—")
    if safe_int(get_val(['nvi_signal', 'NVI_Signal'])) == 1: sig_desc.append("ç±Œç¢¼é–å®š")
    
    sig_str = f"[{','.join(sig_desc)}]" if sig_desc else "[-]"
    
    # æ­¢ç›ˆæ­¢æ
    tp_raw = safe_f(get_val(['take_profit', 'TP']))
    sl_raw = safe_f(get_val(['stop_loss', 'SL']))
    vp_lower = safe_f(get_val(['vp_lower', 'VP_lower']))
    vp_upper = safe_f(get_val(['vp_upper', 'VP_upper']))
    tp, sl = calculate_trade_setup(close, vp_upper, vp_lower, mas[20], tp_raw, sl_raw)

    # ç±Œç¢¼
    inst_str = ""
    if f_buy is not None:
        inst_str = f" å¤–:{f_buy//1000} æŠ•:{t_buy//1000} è‡ª:{d_buy//1000}"

    # é›†ä¿äººæ•¸é¡¯ç¤º
    holders_str = f" é›†ä¿:{total_holders:,}" if total_holders is not None else " é›†ä¿:N/A"

    # Line 1
    line1 = f"{date} {name}({code}) é‡:{vol_text} MFI:{colored_mfi} RSI:{colored_rsi}"
    
    # Line 2 - ä½¿ç”¨ change_pct çš„æ­£è² ä¾†æ±ºå®šé¡è‰²ï¼ˆæ¼²=ç´…ã€è·Œ=ç¶ ï¼‰
    close_color = get_color_code(change_pct)  # æ­£å€¼ç´…ï¼Œè² å€¼ç¶ 
    line2 = f"æ”¶ç›¤:{close_color}{close:.2f}({change_pct:+.2f}%){reset} 14æ—¥:{colored_chg14} VSBCä¸Š/ä¸‹:{vsbc_str} å¤§æˆ¶:{major_pct:.1f}%{holders_str}"
    
    # Line 3
    line3 = f"æ­¢ç›ˆ:{tp:.2f}   VWAP:{colored_vwap}   POC:{poc:.2f}   æ­¢æ:{sl:.2f}"
    
    # Line 4
    line4 = f"è¨Šè™Ÿ3/6:{sig_str}{inst_str}"
    
    # Line 5
    line5 = f"MA20:{mas[20]:.2f} MA60:{mas[60]:.2f} MA120:{mas[120]:.2f} MA200:{mas[200]:.2f}"
    
    return f"{line1}\n{line2}\n{line3}\n{line4}\n{line5}\n"


def reset_color():
    """é‡ç½®é¡è‰²"""
    return RESET_COLOR

def get_arrow(curr, prev):
    """æ ¹æ“šç•¶å‰å€¼å’Œå‰å€¼ç²å–ç®­é ­ (Table-Driven)"""
    if curr is None or prev is None: return ""
    
    # Table-Driven: (Condition) -> Symbol
    # Using a list of tuples for ordered evaluation
    rules = [
        (curr > prev, "â†‘"),
        (curr < prev, "â†“")
    ]
    
    for condition, symbol in rules:
        if condition: return symbol
    return "-"

def get_volume_color(ratio):
    """æˆäº¤é‡é¡è‰²ï¼šé‡å¢=ç´…ã€é‡ç¸®=ç¶ ã€çˆ†é‡=ç´«"""
    if ratio >= 2.0: return "\033[95m"  # çˆ†é‡ (ç´«è‰²)
    elif ratio > 1.0: return "\033[91m"  # é‡å¢ = ç´…è‰²
    elif ratio < 1.0: return "\033[92m"  # é‡ç¸® = ç¶ è‰²
    return "\033[97m"  # æŒå¹³ = ç™½è‰²

def get_trend_color(curr, prev):
    """æ ¹æ“šè¶¨å‹¢ç²å–é¡è‰² (Table-Driven)"""
    if curr is None or prev is None: return ""
    
    # Table-Driven: (Condition) -> Color
    rules = [
        (curr > prev, "\033[91m"), # Red
        (curr < prev, "\033[92m")  # Green
    ]
    
    for condition, color in rules:
        if condition: return color
    return "\033[97m" # White

def get_colored_value(text, change, arrow):
    """ç²å–å¸¶é¡è‰²çš„å€¼"""
    color = get_color_code(change)
    return f"{color}{text}{arrow}{reset_color()}"


def format_scan_result_list(code, name, indicators_list):
    """æ ¼å¼åŒ–å¤šå¤©æŠ€è¡“æŒ‡æ¨™çµæœ"""
    if not indicators_list:
        return ""
    
    output_lines = []
    for i, indicators in enumerate(indicators_list):
        output_lines.append(format_scan_result(code, name, indicators, show_date=True))
        if i < len(indicators_list) - 1:
            output_lines.append("-" * 80)
    
    return "\n".join(output_lines)

def display_scan_results(results, title, limit=20, extra_info_func=None, description=None):
    """çµ±ä¸€é¡¯ç¤ºæƒæçµæœçš„æ¨¡çµ„"""
    print_flush(f"\nã€{title}ã€‘")
    print_flush("â•" * 31)
    if description:
        print_flush(f"{description}")
        print_flush("â•" * 31)
    
    display_list = results[:limit]
    for i, item in enumerate(display_list):
        if len(item) == 2:
            code, ind = item
            value = None
        elif len(item) == 3:
            code, value, ind = item
        else:
            code, value, ind = item[0], item[1], item[2]
        
        name = get_correct_stock_name(code, ind.get('name', code))
        
        extra = ""
        if extra_info_func and value is not None:
            extra = f" {extra_info_func(code, value, ind)}"
        
        print_flush(f"{i+1}. {format_scan_result(code, name, ind, show_date=True)}{extra}")
        
        if i < len(display_list) - 1:
            print_flush("-" * 80)
    
    print_flush("=" * 80)
    print_flush(f"[é¡¯ç¤ºæª”æ•¸: {min(limit, len(results))}/{len(results)}]")
    print_flush("=" * 80)
    
    # Return codes for external use
    return [item[0] for item in results[:limit]]

def prompt_stock_detail_report(result_codes):
    """æç¤ºä½¿ç”¨è€…è¼¸å…¥è‚¡ç¥¨ä»£è™ŸæŸ¥çœ‹è©³ç´°å ±å‘Š"""
    if not result_codes:
        return
    
    while True:
        print_flush("\nè¼¸å…¥è‚¡ç¥¨ä»£è™ŸæŸ¥çœ‹è©³ç´°å ±å‘Š (è¼¸å…¥ 0 è¿”å›):")
        try:
            choice = input("è«‹è¼¸å…¥: ").strip()
        except EOFError:
            break
            
        if choice == '0' or choice == '':
            break
            
        if choice in result_codes:
            # Show detailed report
            name = get_correct_stock_name(choice)
            print_flush(f"\n{'='*80}")
            print_flush(f"ã€{choice} {name} è©³ç´°å ±å‘Šã€‘")
            print_flush('='*80)
            
            # Get historical data for analysis
            try:
                days_input = input("é¡¯ç¤ºå¤©æ•¸(é è¨­10å¤©): ").strip()
                days = int(days_input) if days_input.isdigit() else 10
            except:
                days = 10

            indicators_list = calculate_stock_history_indicators(choice, display_days=days, limit_days=max(400, days + 220))
            if indicators_list:
                # Show recent history
                for i, ind in enumerate(indicators_list[:10]):  # Show last 10 days
                    print_flush(format_scan_result(choice, name, ind, show_date=True))
                    if i < len(indicators_list[:10]) - 1:
                        print_flush("-" * 80)
            else:
                print_flush("âŒ ç„¡æ³•å–å¾—æ­·å²è³‡æ–™")
            
            print_flush('='*80)
        else:
            print_flush(f"âŒ æ‰¾ä¸åˆ°ä»£è™Ÿ {choice}ï¼Œè«‹å¾æƒæçµæœä¸­é¸æ“‡")


# ==============================
# æ­¥é©Ÿå‡½æ•¸
# ==============================
def get_latest_market_date():
    """ç²å–å¸‚å ´æœ€æ–°äº¤æ˜“æ—¥æœŸ"""
    dates = []
    
    # 1. Check TWSE
    try:
        url = f"{TWSE_STOCK_DAY_ALL_URL}&_={int(time.time())}"
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Referer': 'https://www.twse.com.tw/zh/page/trading/exchange/STOCK_DAY_ALL.html'
        })
        
        try:
            session.get('https://www.twse.com.tw/zh/page/trading/exchange/STOCK_DAY_ALL.html', timeout=5, verify=False)
        except:
            pass
        
        res = session.get(url, timeout=10, verify=False)
        if res.status_code == 200:
            data = res.json()
            if 'date' in data and len(data['date']) == 8:
                d = data['date']
                dates.append(f"{d[:4]}-{d[4:6]}-{d[6:]}")
    except:
        pass
    
    # 2. Check TPEx
    try:
        url = f"{TPEX_DAILY_TRADING_URL}?d=&stk_code=&o=json&_={int(time.time())}"
        res = requests.get(url, timeout=10, verify=False)
        if res.status_code == 200:
            data = res.json()
            if 'reportDate' in data:
                dates.append(roc_to_western_date(data['reportDate']))
    except:
        pass
    
    if not dates:
        # Fallback: Try to get max date from DB first
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT MAX(date) FROM stock_snapshot")
                db_date = cur.fetchone()[0]
                if db_date:
                    return db_date
        except:
            pass
            
        return datetime.now().strftime("%Y-%m-%d")
        
    return max(dates)

def step1_fetch_stock_list(silent_header=False):
    """æ­¥é©Ÿ1: æ›´æ–°ä¸Šå¸‚æ«ƒæ¸…å–® (ä½¿ç”¨ Open Data API)"""
    if not silent_header:
        print_flush("\n[Step 1] æ›´æ–°ä¸Šå¸‚æ«ƒæ¸…å–®...")
    stocks = []
    
    # 1. TWSE ä¸Šå¸‚ (OpenAPI) - åŸºæœ¬è³‡æ–™ + è¡Œæƒ…è¡¨
    twse_meta_map = {}
    twse_quote_data = {}  # å„²å­˜è¡Œæƒ…è³‡æ–™
    
    try:
        # 1a. ä¸Šå¸‚å…¬å¸åŸºæœ¬è³‡æ–™ (å«ä¸Šå¸‚æ—¥æœŸ)
        url_meta = "https://openapi.twse.com.tw/v1/opendata/t187ap03_L"
        if not silent_header:
            print_flush("  [TWSE] åŸºæœ¬è³‡æ–™ (ä¸Šå¸‚æ—¥æœŸ)...", end="")
        else:
            print_flush("  [TWSE] åŸºæœ¬è³‡æ–™ (ä¸Šå¸‚æ—¥æœŸ)...", end="")
        res = requests.get(url_meta, timeout=30, verify=False)
        data_meta = res.json()
        
        for item in data_meta:
            code = item.get('å…¬å¸ä»£è™Ÿ')
            l_date = item.get('ä¸Šå¸‚æ—¥æœŸ')  # Format: YYYYMMDD
            if code and l_date:
                twse_meta_map[code] = l_date
        print_flush(f" âœ“ (å–å¾— {len(twse_meta_map)} æª”)")
    except Exception as e:
        print_flush(f" âœ— ({e})")
    
    try:
        # 1b. TWSE è¡Œæƒ…è¡¨ (STOCK_DAY_ALL - å€‹è‚¡æ—¥æˆäº¤è³‡è¨Š)
        url_quote = "https://openapi.twse.com.tw/v1/exchangeReport/STOCK_DAY_ALL"
        if not silent_header:
            print_flush("  [TWSE] è¡Œæƒ…è¡¨ (è‚¡ç¥¨æ¸…å–®)...", end="")
        else:
            print_flush("  [TWSE] è¡Œæƒ…è¡¨ (è‚¡ç¥¨æ¸…å–®)...", end="")
        res = requests.get(url_quote, timeout=30, verify=False)
        data_quote = res.json()
        
        count = 0
        for item in data_quote:
            code = item.get('Code')
            name = item.get('Name')
            if code and name and len(code) == 4:
                l_date = twse_meta_map.get(code, '')
                # è½‰ç‚º YYYY-MM-DD
                if len(l_date) == 8:
                    l_date = f"{l_date[:4]}-{l_date[4:6]}-{l_date[6:]}"
                
                stocks.append({'code': code, 'name': name, 'market': 'TWSE', 'list_date': l_date})
                count += 1
        print_flush(f" âœ“ (å–å¾— {count} æª”)")
    except Exception as e:
        print_flush(f" âœ— ({e})")
        # Fallback: ä½¿ç”¨ BWIBBU_d (æœ¬ç›Šæ¯”æ¸…å–®) ä½œç‚ºå‚™æ´
        try:
            url_fallback = "https://openapi.twse.com.tw/v1/exchangeReport/BWIBBU_d"
            print_flush("  -> ä½¿ç”¨å‚™æ´ API (BWIBBU_d)...", end="")
            res = requests.get(url_fallback, timeout=30, verify=False)
            data = res.json()
            
            count = 0
            for item in data:
                code = item.get('Code')
                name = item.get('Name')
                if code and name and len(code) == 4:
                    l_date = twse_meta_map.get(code, '')
                    if len(l_date) == 8:
                        l_date = f"{l_date[:4]}-{l_date[4:6]}-{l_date[6:]}"
                    stocks.append({'code': code, 'name': name, 'market': 'TWSE', 'list_date': l_date})
                    count += 1
            print_flush(f" âœ“ (å–å¾— {count} æª”)")
        except Exception as e2:
            print_flush(f" âœ— ({e2})")
    
    # 2. TPEx ä¸Šæ«ƒ (Web API)
    # ä¸Šæ«ƒå…¬å¸åŸºæœ¬è³‡æ–™: https://www.tpex.org.tw/openapi/v1/t187ap03_O (ä¸Šæ«ƒå…¬å¸åŸºæœ¬è³‡æ–™)
    tpex_meta_map = {}
    try:
        url_meta_tpex = "https://www.tpex.org.tw/openapi/v1/mopsfin_t187ap03_O"
        if not silent_header:
            print_flush("  [TPEx] åŸºæœ¬è³‡æ–™ (ä¸Šå¸‚æ—¥æœŸ)...", end="")
        else:
            print_flush("  [TPEx] åŸºæœ¬è³‡æ–™ (ä¸Šå¸‚æ—¥æœŸ)...", end="")
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        res = requests.get(url_meta_tpex, timeout=30, verify=False, headers=headers)
        try:
            data_meta_tpex = res.json()
            for item in data_meta_tpex:
                code = item.get('SecuritiesCompanyCode')
                l_date = item.get('DateOfListing') # Format: YYYYMMDD
                if code and l_date:
                    tpex_meta_map[code] = l_date
            print_flush(" âœ“")
        except:
            print_flush(" âš  (ç„¡æ³•è§£æåŸºæœ¬è³‡æ–™ï¼Œå°‡ç•¥éä¸Šå¸‚æ—¥æœŸ)")
    except Exception as e:
        print_flush(f" âš  ({e})")

    try:
        url = "https://www.tpex.org.tw/web/stock/aftertrading/daily_close_quotes/stk_quote_result.php?l=zh-tw&d=&o=json"
        if not silent_header:
            print_flush("  [TPEx] è¡Œæƒ…è¡¨ (è‚¡ç¥¨æ¸…å–®)...", end="")
        else:
            print_flush("  [TPEx] è¡Œæƒ…è¡¨ (è‚¡ç¥¨æ¸…å–®)...", end="")
        res = requests.get(url, timeout=30, verify=False)
        data = res.json()
        
        count = 0
        if 'tables' in data:
            for table in data['tables']:
                if 'data' in table:
                    for row in table['data']:
                        if len(row) >= 2:
                            code = row[0]
                            name = row[1]
                            if len(code) == 4:
                                l_date = tpex_meta_map.get(code, '')
                                # è½‰ç‚º YYYY-MM-DD
                                if len(l_date) == 8:
                                    l_date = f"{l_date[:4]}-{l_date[4:6]}-{l_date[6:]}"
                            
                                stocks.append({'code': code, 'name': name, 'market': 'TPEx', 'list_date': l_date})
                                count += 1
        print_flush(f" âœ“ (å–å¾— {count} æª”)")
    except Exception as e:
        print_flush(f" âœ— ({e})")
        
    # 3. è£œå…¨ç¼ºå¤±çš„ä¸Šå¸‚æ—¥æœŸ (å¾ stock_meta è®€å–èˆŠè³‡æ–™)
    if not silent_header:
        print_flush("  [è£œå…¨] å¡«è£œç¼ºå¤±ä¸Šå¸‚æ—¥æœŸ...", end="")
    else:
        print_flush("  [è£œå…¨] å¡«è£œç¼ºå¤±ä¸Šå¸‚æ—¥æœŸ...", end="")
    fill_count = 0
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            cur.execute("SELECT code, list_date FROM stock_meta WHERE list_date IS NOT NULL")
            existing_dates = {row[0]: row[1] for row in cur.fetchall()}
            
            for s in stocks:
                if not s['list_date'] and s['code'] in existing_dates:
                    s['list_date'] = existing_dates[s['code']]
                    fill_count += 1
        print_flush(f" âœ“ (è£œå…¨ {fill_count} ç­†)")
    except:
        print_flush(" -")

    # 4. å¯«å…¥è³‡æ–™åº«
    if stocks:
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                # ä½¿ç”¨ INSERT OR REPLACE æ›´æ–°
                for s in stocks:
                    cur.execute("""
                        INSERT INTO stock_meta (code, name, market_type, list_date)
                        VALUES (?, ?, ?, ?)
                        ON CONFLICT(code) DO UPDATE SET
                            name=excluded.name,
                            market_type=excluded.market_type,
                            list_date=COALESCE(excluded.list_date, stock_meta.list_date)
                    """, (s['code'], s['name'], s['market'], s['list_date']))
                conn.commit()
            print_flush(f"âœ“ å·²æ›´æ–° {len(stocks)} æª”è‚¡ç¥¨è‡³æ¸…å–®")
            print_flush("âœ“ å·²å¯«å…¥è³‡æ–™åº« stock_meta")
        except Exception as e:
            print_flush(f"âŒ è³‡æ–™åº«å¯«å…¥å¤±æ•—: {e}")
    else:
        print_flush("âš  æœªå–å¾—ä»»ä½•è‚¡ç¥¨æ¸…å–®")

def sync_stock_names_to_supabase(stocks):
    """å°‡è‚¡ç¥¨æ¸…å–®åŒæ­¥è‡³ Supabase (ä½¿ç”¨ stock_list è¡¨)"""
    print_flush("â˜ æ­£åœ¨åŒæ­¥è‚¡åè‡³ Supabase...", end="")
    try:
        # ä½¿ç”¨ stock_list è¡¨ (å­˜æ”¾è‚¡ç¥¨ä»£ç¢¼èˆ‡åç¨±)
        url = f"{SUPABASE_URL}/rest/v1/stock_list"
        headers = {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json",
            "Prefer": "resolution=merge-duplicates"
        }
        
        batch_size = 200
        total = len(stocks)
        success_count = 0
        failed_batches = 0
        
        for i in range(0, total, batch_size):
            batch = stocks[i:i+batch_size]
            # stock_list è¡¨æ¬„ä½: code, name
            payload = [{"code": s['code'], "name": s['name']} for s in batch]
            
            try:
                res = requests.post(url, json=payload, headers=headers, timeout=15)
                if res.status_code in (200, 201):
                    success_count += len(batch)
                    print_flush(".", end="")
                else:
                    failed_batches += 1
                    # é¡¯ç¤ºè©³ç´°éŒ¯èª¤ä»¥ä¾¿é™¤éŒ¯
                    error_msg = res.text.replace('\n', ' ')
                    print_flush(f"x[{res.status_code}:{error_msg}]", end="")
            except requests.Timeout:
                failed_batches += 1
                print_flush("t", end="")
                
        if failed_batches > 0:
            print_flush(f" âš  ({success_count}/{total}, {failed_batches} æ‰¹å¤±æ•—)")
        else:
            print_flush(f" âœ“ ({success_count}/{total})")
    except Exception as e:
        print_flush(f" âœ— åŒæ­¥éŒ¯èª¤: {e}")

# ==============================
# å¸‚å ´è³‡æ–™æ›´æ–°æ¨¡æ¿ (Template Method)
# ==============================
def update_market_data(market_name, fetch_func, parse_func, silent_header=False):
    """
    é€šç”¨å¸‚å ´è³‡æ–™æ›´æ–°é‚è¼¯ (Template Method)
    :param market_name: å¸‚å ´åç¨± (TPEx/TWSE)
    :param fetch_func: è³‡æ–™ç²å–å‡½å¼ï¼Œå›å‚³ (trade_date, data_list)
    :param parse_func: è³‡æ–™è§£æå‡½å¼ï¼Œå›å‚³ (code, name, open, high, low, close, vol)
    :param silent_header: æ˜¯å¦éš±è—æ¨™é¡Œ (ç”¨æ–¼ä¸€éµæ›´æ–°æ™‚é¿å…é‡è¤‡)
    """
    if not silent_header:
        print_flush(f"\n[Step] ä¸‹è¼‰ {market_name} æœ¬æ—¥è¡Œæƒ…...")
    
    try:
        trade_date, data_list = fetch_func()
        if not data_list:
            return set()
            
        date_int = int(trade_date.replace('-', ''))
        print_flush(f"  -> æ—¥æœŸ: {trade_date}")
        print_flush("  -> æ­£åœ¨å¯«å…¥è³‡æ–™åº«: ", end="")
        
        new_count = 0
        update_count = 0
        skip_count = 0
        updated_codes = set()
        
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            conn.execute("PRAGMA synchronous = OFF;")
            
            for idx, item in enumerate(data_list):
                if idx % 100 == 0:
                    print_flush(".", end="")
                
                # 1. è§£æè³‡æ–™
                parsed = parse_func(item)
                if not parsed:
                    continue
                
                code, name, open_price, high, low, close, vol, amount = parsed
                
                # 2. æª¢æŸ¥èˆŠè³‡æ–™
                cur.execute("SELECT close, amount FROM stock_history WHERE code=? AND date_int=?", (code, date_int))
                existing = cur.fetchone()
                
                # è¡›èªå¥ï¼šè³‡æ–™å­˜åœ¨ä¸”ä¸€è‡´ (ä¸” amount ä¸ç‚ºç©º)
                # å¦‚æœè³‡æ–™åº«ä¸­ amount ç‚ºç©ºä½†æ–°è³‡æ–™æœ‰ amountï¼Œå‰‡å¼·åˆ¶æ›´æ–°
                need_update = False
                if existing:
                    if existing[0] != close:
                        need_update = True
                    elif (existing[1] is None or existing[1] == 0) and (amount is not None and amount > 0):
                        need_update = True
                    else:
                        skip_count += 1
                        continue
                
                if existing is None:
                    new_count += 1
                else:
                    update_count += 1
                
                updated_codes.add(code)
                
                # 3. å–å¾—å‰æ—¥æ”¶ç›¤åƒ¹ (è¨ˆç®—æ¼²è·Œå¹…ç”¨)
                cur.execute("""
                    SELECT close, volume FROM stock_history 
                    WHERE code=? AND date_int<? 
                    ORDER BY date_int DESC LIMIT 1
                """, (code, date_int))
                
                prev = cur.fetchone()
                pc, pv = (prev[0], prev[1]) if prev else (close, vol)
                
                # 4. å¯«å…¥æ­·å²è³‡æ–™ (å« amount)
                cur.execute("""
                    INSERT OR REPLACE INTO stock_history 
                    (code, date_int, open, high, low, close, volume, amount)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                """, (code, date_int, open_price, high, low, close, vol, amount))
                
                # 5. å¯«å…¥å¿«ç…§è³‡æ–™ (æ”¹ç”¨ UPSERT ä»¥ä¿ç•™ PE/Yield ç­‰è³‡æ–™)
                cur.execute("""
                    INSERT INTO stock_snapshot (code, name, date, close, volume, close_prev, vol_prev, amount)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    ON CONFLICT(code) DO UPDATE SET
                        name=excluded.name,
                        date=excluded.date,
                        close=excluded.close,
                        volume=excluded.volume,
                        close_prev=excluded.close_prev,
                        vol_prev=excluded.vol_prev,
                        amount=excluded.amount
                """, (code, name, trade_date, close, vol, pc, pv, amount))
            
            conn.commit()
            
            # [å·²ç§»é™¤] ä¸å†æ¯æ¬¡éƒ½åŒæ­¥ Supabaseï¼Œæ”¹ç”± step8_sync_supabase çµ±ä¸€è™•ç†
            
        print_flush(f"\nâœ“ {market_name} æ›´æ–°: æ–°å¢ {new_count} ç­† | æ›´æ–° {update_count} ç­† | è·³é {skip_count} ç­†")
        return updated_codes
        
    except Exception as e:
        print_flush(f"\nâŒ å¤±æ•—: {e}")
        return set()

# TPEx è¼”åŠ©å‡½å¼
def _fetch_tpex_data():
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    res = requests.get(TPEX_MAINBOARD_URL, timeout=Config.API_TIMEOUT, verify=False, headers=headers)
    data = res.json()
    if not data:
        return None, []
    
    raw_date = data[0].get('Date') or data[0].get('date')
    trade_date = roc_to_western_date(raw_date)
    return trade_date, data

def _parse_tpex_item(item):
    code = item.get('SecuritiesCompanyCode', '').strip()
    name = item.get('CompanyName', '').strip()
    
    # Guard Clause
    if not is_normal_stock(code, name):
        return None
        
    vol = safe_int(item.get('TradingShares'))
    if vol < 1: vol = 0
    
    return (
        code, name,
        safe_num(item.get('Open')),
        safe_num(item.get('High')),
        safe_num(item.get('Low')),
        safe_num(item.get('Close')),
        vol,
        safe_num(item.get('TransactionAmount'))  # [ä¿®æ­£] æˆäº¤é‡‘é¡ key ç‚º TransactionAmount
    )

# TWSE è¼”åŠ©å‡½å¼
def _fetch_twse_data():
    """ç²å– TWSE ä¸Šå¸‚è‚¡ç¥¨ä»Šæ—¥è¡Œæƒ… (ä½¿ç”¨ MI_INDEX ç¶²é ç‰ˆ API - æ›´å³æ™‚)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }
    
    # ä½¿ç”¨ä»Šå¤©æ—¥æœŸ
    today = datetime.now().strftime("%Y%m%d")
    
    # === ä¸»è¦ä¾†æº: MI_INDEX ç¶²é ç‰ˆ API (Table 8: æ¯æ—¥æ”¶ç›¤è¡Œæƒ…) ===
    url = f"https://www.twse.com.tw/exchangeReport/MI_INDEX?response=json&date={today}&type=ALLBUT0999"
    
    try:
        res = requests.get(url, timeout=30, verify=False, headers=headers)
        data = res.json()
        
        if data.get('stat') != 'OK':
            # å˜—è©¦ OpenAPI ä½œç‚ºå‚™æ´
            return _fetch_twse_data_openapi_fallback()
        
        trade_date = data.get('date', today)
        # è½‰æ›ç‚º YYYY-MM-DD æ ¼å¼
        if len(trade_date) == 8:
            trade_date = f"{trade_date[:4]}-{trade_date[4:6]}-{trade_date[6:]}"
        
        # æ‰¾åˆ° Table 8: æ¯æ—¥æ”¶ç›¤è¡Œæƒ…
        stock_data = []
        for table in data.get('tables', []):
            title = table.get('title', '')
            if 'æ¯æ—¥æ”¶ç›¤è¡Œæƒ…' in title:
                # æ ¼å¼: [ä»£è™Ÿ, åç¨±, æˆäº¤è‚¡æ•¸, æˆäº¤ç­†æ•¸, æˆäº¤é‡‘é¡, é–‹ç›¤åƒ¹, æœ€é«˜åƒ¹, æœ€ä½åƒ¹, æ”¶ç›¤åƒ¹, ...]
                for row in table.get('data', []):
                    if len(row) >= 9:
                        code = str(row[0]).strip()
                        # åªä¿ç•™ 4 ç¢¼æ™®é€šè‚¡
                        if len(code) == 4 and code.isdigit():
                            stock_data.append({
                                'Code': code,
                                'Name': str(row[1]).strip(),
                                'TradeVolume': str(row[2]).replace(',', ''),
                                'TradeValue': str(row[4]).replace(',', ''),
                                'OpeningPrice': str(row[5]).replace(',', '').replace('--', '0'),
                                'HighestPrice': str(row[6]).replace(',', '').replace('--', '0'),
                                'LowestPrice': str(row[7]).replace(',', '').replace('--', '0'),
                                'ClosingPrice': str(row[8]).replace(',', '').replace('--', '0'),
                            })
                break
        
        if stock_data:
            return trade_date, stock_data
        else:
            # å‚™æ´
            return _fetch_twse_data_openapi_fallback()
            
    except Exception as e:
        logger.debug(f"MI_INDEX API å¤±æ•—: {e}ï¼Œä½¿ç”¨ OpenAPI å‚™æ´")
        return _fetch_twse_data_openapi_fallback()

def _fetch_twse_data_openapi_fallback():
    """OpenAPI å‚™æ´ (è³‡æ–™æ›´æ–°è¼ƒæ…¢)"""
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    try:
        res = requests.get(TWSE_STOCK_DAY_ALL_URL, timeout=Config.API_TIMEOUT, verify=False, headers=headers)
        data = res.json()
        
        if not data or not isinstance(data, list):
            return None, []
        
        raw_date = data[0].get('Date', '')
        trade_date = roc_to_western_date(raw_date)
        return trade_date, data
    except:
        return None, []

def _parse_twse_item(item):
    code = item.get('Code', '').strip()
    name = item.get('Name', '').strip()
    
    # Guard Clause
    if not is_normal_stock(code, name):
        return None
        
    vol = safe_int(item.get('TradeVolume'))
    if vol < 1: vol = 0

    return (
        code, name,
        safe_num(item.get('OpeningPrice')),
        safe_num(item.get('HighestPrice')),
        safe_num(item.get('LowestPrice')),
        safe_num(item.get('ClosingPrice')),
        vol,
        safe_num(item.get('TradeValue'))  # [OpenAPI] æˆäº¤é‡‘é¡
    )

def _fetch_and_update_tpex_valuation():
    """ä¸‹è¼‰ä¸¦æ›´æ–° TPEx å€‹è‚¡æœ¬ç›Šæ¯”ã€æ®–åˆ©ç‡ã€è‚¡åƒ¹æ·¨å€¼æ¯”"""
    print_flush("\n[Step 2+] æ›´æ–° TPEx ä¼°å€¼è³‡æ–™ (PE/Yield/PB)...")
    try:
        url = "https://www.tpex.org.tw/web/stock/aftertrading/peratio_analysis/pera_result.php?l=zh-tw&o=json"
        res = requests.get(url, timeout=30, verify=False)
        data = res.json()
        
        # TPEx æ ¼å¼è®Šç•°å¤šï¼Œå˜—è©¦ä¸åŒæ¬„ä½
        rows = data.get('aaData') or data.get('data') or []
        if not rows and 'tables' in data:
             rows = data['tables'][0]['data']
             
        updates = []
        for item in rows:
            if len(item) < 7: continue
            
            code = item[0]
            # 0:Code, 1:Name, 2:PE, 3:Div, 4:Year, 5:Yield, 6:PB
            pe = safe_float_preserving_none(item[2])
            dy = safe_float_preserving_none(item[5])
            pb = safe_float_preserving_none(item[6])
            
            if len(code) == 4:
                updates.append((pe, dy, pb, code))
                
        if updates:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                cur.executemany("""
                    UPDATE stock_snapshot 
                    SET pe=?, yield=?, pb=?
                    WHERE code=?
                """, updates)
                conn.commit()
            today_display = datetime.now().strftime("%Y-%m-%d")
            print_flush(f"âœ“ å·²æ›´æ–° {len(updates)} ç­† TPEx ä¼°å€¼è³‡æ–™ ({today_display})")
    except Exception as e:
        print_flush(f"âŒ TPEx ä¼°å€¼æ›´æ–°å¤±æ•—: {e}")

def step2_download_tpex_daily(silent_header=False):
    """æ­¥é©Ÿ2: ä¸‹è¼‰ TPEx (ä¸Šæ«ƒ) æœ¬æ—¥è¡Œæƒ… (å«ä¼°å€¼)"""
    updated = update_market_data("TPEx (ä¸Šæ«ƒ)", _fetch_tpex_data, _parse_tpex_item, silent_header=silent_header)
    _fetch_and_update_tpex_valuation()
    return updated

def _fetch_and_update_twse_valuation():
    """ä¸‹è¼‰ä¸¦æ›´æ–° TWSE å€‹è‚¡æœ¬ç›Šæ¯”ã€æ®–åˆ©ç‡ã€è‚¡åƒ¹æ·¨å€¼æ¯”"""
    print_flush("\n[Step 3+] æ›´æ–° TWSE ä¼°å€¼è³‡æ–™ (PE/Yield/PB)...")
    try:
        url = "https://openapi.twse.com.tw/v1/exchangeReport/BWIBBU_d"
        res = requests.get(url, timeout=30, verify=False)
        data = res.json()
        
        updates = []
        for item in data:
            code = item.get('Code')
            # TWSE OpenAPI: PEratio, DividendYield, PBratio
            pe = safe_float_preserving_none(item.get('PEratio'))
            dy = safe_float_preserving_none(item.get('DividendYield'))
            pb = safe_float_preserving_none(item.get('PBratio'))
            
            if code:
                updates.append((pe, dy, pb, code))
                
        if updates:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                cur.executemany("""
                    UPDATE stock_snapshot 
                    SET pe=?, yield=?, pb=?
                    WHERE code=?
                """, updates)
                conn.commit()
            today_display = datetime.now().strftime("%Y-%m-%d")
            print_flush(f"âœ“ å·²æ›´æ–° {len(updates)} ç­† TWSE ä¼°å€¼è³‡æ–™ ({today_display})")
    except Exception as e:
        print_flush(f"âŒ TWSE ä¼°å€¼æ›´æ–°å¤±æ•—: {e}")

def step3_download_twse_daily(silent_header=False):
    """æ­¥é©Ÿ3: ä¸‹è¼‰ TWSE (ä¸Šå¸‚) æœ¬æ—¥è¡Œæƒ… (å«ä¼°å€¼)"""
    updated = update_market_data("TWSE (ä¸Šå¸‚)", _fetch_twse_data, _parse_twse_item, silent_header=silent_header)
    _fetch_and_update_twse_valuation()
    return updated

MIN_DATA_COUNT = 450 # 450ç­†
    
def step4_check_data_gaps():
    """æ­¥é©Ÿ4: æª¢æŸ¥æ•¸æ“šç¼ºå¤± (å«é‡‘é¡èˆ‡æ³•äºº) - æ”¯æ´ä¸Šå¸‚æ—¥æœŸåˆ¤æ–·"""
    print_flush("\n[Step 4] æª¢æŸ¥æ•¸æ“šç¼ºå¤±...")
    # MIN_DATA_COUNT = 400  # ç”¨æˆ¶æŒ‡å®šé–€æª»
    
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        
        # 0. é å…ˆè¼‰å…¥ä¸Šå¸‚æ—¥æœŸ (Source of Truth)
        list_date_map = {}
        try:
            cur.execute("SELECT code, list_date FROM stock_meta")
            for r in cur.fetchall():
                if r[1]: list_date_map[r[0]] = r[1]
        except:
            pass

        # 1. æª¢æŸ¥æ­·å²è³‡æ–™ç­†æ•¸èˆ‡é‡‘é¡ç¼ºå¤± (åªæœ‰ volume > 0 æ™‚ amount ç‚º 0/NULL æ‰ç®—ç¼ºå¤±)
        print_flush("æ­£åœ¨åˆ†ææ­·å²è³‡æ–™èˆ‡é‡‘é¡...")
        sql = """
            SELECT code, 
                   COUNT(*) as total_cnt,
                   SUM(CASE WHEN volume > 0 AND (amount IS NULL OR amount = 0) THEN 1 ELSE 0 END) as missing_amount_cnt,
                   MIN(date_int) as min_date
            FROM stock_history 
            GROUP BY code
        """
        rows = cur.execute(sql).fetchall()
        
        # 2. æª¢æŸ¥æ³•äººè³‡æ–™
        print_flush("æ­£åœ¨åˆ†ææ³•äººè³‡æ–™...")
        try:
            inst_rows = cur.execute("SELECT code, COUNT(*) FROM institutional_investors GROUP BY code").fetchall()
            inst_map = {r[0]: r[1] for r in inst_rows}
        except:
            inst_map = {}  # è¡¨æ ¼å¯èƒ½ä¸å­˜åœ¨
            
    # åˆ†æç¼ºå¤±
    count_gaps = []
    amount_gaps = []
    inst_gaps = []
    
    for r in rows:
        code = r[0]
        total = r[1]
        missing_amt = r[2] if r[2] else 0
        min_date_int = r[3]
        
        # æª¢æŸ¥è³‡æ–™ç­†æ•¸ (åŠ å…¥ä¸Šå¸‚æ—¥æœŸåˆ¤æ–·)
        if total < MIN_DATA_COUNT:
            is_new_stock = False
            l_date_str = list_date_map.get(code)
            
            if l_date_str:
                try:
                    l_date = datetime.strptime(l_date_str, '%Y-%m-%d')
                    # è¨ˆç®—ä¸Šå¸‚è‡³ä»Šçš„å¤©æ•¸
                    days_since = (datetime.now() - l_date).days
                    
                    # é æœŸäº¤æ˜“æ—¥ (ç´„ç¸½å¤©æ•¸çš„ 68%ï¼Œæ‰£é™¤å‡æ—¥)
                    expected_count = int(days_since * 0.68)
                    
                    # å¦‚æœè³‡æ–™é‡é”åˆ°é æœŸçš„ 90%ï¼Œè¦–ç‚ºå®Œæ•´ (é‡å°æ–°ä¸Šå¸‚è‚¡ç¥¨)
                    if total >= expected_count * 0.9:
                        is_new_stock = True
                    
                    # é›™é‡ç¢ºèª: å¦‚æœæœ€æ—©è³‡æ–™æ—¥æœŸæ¥è¿‘ä¸Šå¸‚æ—¥æœŸ (20å¤©å…§)ï¼Œä¹Ÿè¦–ç‚ºå®Œæ•´
                    if min_date_int:
                        min_date = datetime.strptime(str(min_date_int), '%Y%m%d')
                        if min_date <= l_date + timedelta(days=20):
                            is_new_stock = True
                            
                except Exception:
                    pass
            
            # åªæœ‰ç•¶ä¸æ˜¯æ–°ä¸Šå¸‚è‚¡ç¥¨ï¼Œä¸”ç­†æ•¸ä¸è¶³æ™‚ï¼Œæ‰åˆ—å…¥ç¼ºå¤±
            if not is_new_stock:
                count_gaps.append((code, total))
        
        if missing_amt > 0:
            amount_gaps.append((code, missing_amt))
            
        if code not in inst_map:
            inst_gaps.append(code)
            
    # é¡¯ç¤ºçµæœ
    if not count_gaps and not amount_gaps and not inst_gaps:
        print_flush(f"âœ“ æ‰€æœ‰è‚¡ç¥¨è³‡æ–™çš†å……è¶³ (>= {MIN_DATA_COUNT} ç­†æˆ–ç¬¦åˆä¸Šå¸‚å¤©æ•¸, é‡‘é¡/æ³•äººçš†å®Œæ•´)")
    else:
        if count_gaps:
            print_flush(f"\nâš  è³‡æ–™ç­†æ•¸ä¸è¶³ (<{MIN_DATA_COUNT}): {len(count_gaps)} æª”")
            for c, n in count_gaps[:5]:
                print_flush(f"  - {c}: {n} ç­†")
            if len(count_gaps) > 5:
                print_flush(f"  ... ç­‰å…± {len(count_gaps)} æª”")
                
        if amount_gaps:
            print_flush(f"\nâš  æˆäº¤é‡‘é¡ç¼ºå¤± (Amount=0/Null): {len(amount_gaps)} æª”")
            for c, n in amount_gaps[:5]:
                print_flush(f"  - {c}: ç¼º {n} ç­†")
            if len(amount_gaps) > 5:
                print_flush(f"  ... ç­‰å…± {len(amount_gaps)} æª”")
                
        if inst_gaps:
            print_flush(f"\nâš  æ³•äººè³‡æ–™ç¼ºå¤± (å®Œå…¨ç„¡è³‡æ–™): {len(inst_gaps)} æª”")
            for c in inst_gaps[:5]:
                print_flush(f"  - {c}")
            if len(inst_gaps) > 5:
                print_flush(f"  ... ç­‰å…± {len(inst_gaps)} æª”")

def step5_clean_delisted():
    """æ­¥é©Ÿ5: æ¸…ç†ä¸‹å¸‚è‚¡ç¥¨"""
    print_flush("\n[Step 5] æ¸…ç†ä¸‹å¸‚è‚¡ç¥¨...")
    
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            
            # ä½¿ç”¨ stock_meta è¡¨ä½œç‚ºæœ‰æ•ˆè‚¡ç¥¨åå†Š (åŒ…å«ä¸Šå¸‚+ä¸Šæ«ƒ)
            cur.execute("SELECT code FROM stock_meta WHERE delist_date IS NULL OR delist_date = ''")
            valid_codes = set(row[0] for row in cur.fetchall())
            
            if not valid_codes:
                print_flush("âš  stock_meta è¡¨ç‚ºç©ºï¼Œè·³éæ¸…ç† (é¿å…èª¤åˆª)")
                return
            
            # æŸ¥è©¢ stock_history ä¸­çš„è‚¡ç¥¨
            db_codes = set(row[0] for row in cur.execute("SELECT DISTINCT code FROM stock_history").fetchall())
            
            # æ‰¾å‡ºå·²ä¸‹å¸‚çš„è‚¡ç¥¨ (åœ¨ history ä¸­ä½†ä¸åœ¨æœ‰æ•ˆåå†Šä¸­)
            delisted = db_codes - valid_codes
            
            if delisted:
                # å®‰å…¨æ©Ÿåˆ¶ï¼šå¦‚æœåˆªé™¤æ•¸é‡è¶…éç¸½æ•¸ 10%ï¼Œéœ€è¦ç¢ºèª
                deletion_ratio = len(delisted) / max(len(db_codes), 1)
                if deletion_ratio > 0.1:
                    print_flush(f"âš  ç™¼ç¾ {len(delisted)} æª” ({deletion_ratio:.1%}) å¯èƒ½æ˜¯ä¸‹å¸‚è‚¡ç¥¨")
                    print_flush("  åˆªé™¤æ¯”ä¾‹éé«˜ï¼Œå¯èƒ½æ˜¯åå†Šä¸å®Œæ•´ï¼Œè·³éæ¸…ç†")
                    print_flush("  è«‹å…ˆåŸ·è¡Œ [1] æ›´æ–°ä¸Šå¸‚æ«ƒæ¸…å–® ç¢ºä¿åå†Šå®Œæ•´")
                    return
                
                print_flush(f"ç™¼ç¾ {len(delisted)} æª”ä¸‹å¸‚è‚¡ç¥¨ï¼Œæº–å‚™æ¸…ç†...")
                for code in delisted:
                    cur.execute("DELETE FROM stock_history WHERE code=?", (code,))
                    cur.execute("DELETE FROM stock_snapshot WHERE code=?", (code,))
                
                conn.commit()
                print_flush(f"âœ“ å·²æ¸…é™¤ {len(delisted)} æª”ä¸‹å¸‚è‚¡ç¥¨è³‡æ–™")
            else:
                print_flush("âœ“ ç„¡ä¸‹å¸‚è‚¡ç¥¨æ®˜ç•™")
                
    except Exception as e:
        print_flush(f"âŒ æ¸…ç†å¤±æ•—: {e}")

def step3_5_download_institutional(days=60, silent_header=False):
    """æ­¥é©Ÿ3.5: ä¸‹è¼‰ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™ (å®˜æ–¹ OpenAPI ç‚ºä¸»ï¼Œç¶²é ç‚ºå‚™æ´)"""
    if not silent_header:
        print_flush(f"\n[Step 3.5] ä¸‹è¼‰ä¸‰å¤§æ³•äººè²·è³£è¶…è³‡æ–™ (å®˜æ–¹ OpenAPI å„ªå…ˆ)...")
    
    try:
        from io import StringIO
        
        # === A. å®˜æ–¹ OpenAPI (ä¸»è¦ä¾†æº - åªæŠ“ä»Šå¤©) ===
        today_int = int(datetime.now().strftime("%Y%m%d"))
        openapi_success = False
        
        try:
            print_flush("æ­£åœ¨å¾å®˜æ–¹ OpenAPI å–å¾—ä»Šæ—¥æ³•äººè³‡æ–™...")
            saved = InstitutionalInvestorAPI.fetch_all_openapi()
            if saved > 0:
                today_display = datetime.now().strftime("%Y-%m-%d")
                print_flush(f"âœ“ å®˜æ–¹ OpenAPI: å·²å„²å­˜ {saved} ç­†æ³•äººè³‡æ–™ ({today_display})")
                openapi_success = True
        except Exception as e:
            print_flush(f"âš  å®˜æ–¹ OpenAPI å¤±æ•—: {e}ï¼Œåˆ‡æ›è‡³å‚™æ´ä¾†æº...")
        
        # === B. æ­·å²è³‡æ–™è£œæ¼ (ç¶²é çˆ¬èŸ²å‚™æ´) ===
        print_flush(f"æª¢æŸ¥è¿‘ {days} å¤©æ­·å²ç¼ºæ¼...")
        
        # 1. æº–å‚™æ—¥æœŸåˆ—è¡¨
        base_date = datetime.now()
        dates_to_check = []
        for i in range(days + 10): # å¤šæŠ“ä¸€é»ä»¥é˜²å‡æ—¥
            dt = base_date - timedelta(days=i)
            if dt.weekday() < 5: # åªå–å¹³æ—¥
                dates_to_check.append(dt)
            if len(dates_to_check) >= days: break
            
        # 2. æª¢æŸ¥è³‡æ–™åº«ç¾æœ‰è³‡æ–™
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            # ç¢ºä¿è¡¨æ ¼å­˜åœ¨
            cur.execute("""
                CREATE TABLE IF NOT EXISTS institutional_investors (
                    code TEXT NOT NULL,
                    date_int INTEGER NOT NULL,
                    foreign_buy INTEGER DEFAULT 0,
                    foreign_sell INTEGER DEFAULT 0,
                    trust_buy INTEGER DEFAULT 0,
                    trust_sell INTEGER DEFAULT 0,
                    dealer_buy INTEGER DEFAULT 0,
                    dealer_sell INTEGER DEFAULT 0,
                    PRIMARY KEY (code, date_int)
                )
            """)
            conn.commit()
            
            # å–å¾—å·²æœ‰çš„æ—¥æœŸ
            check_start = int(dates_to_check[-1].strftime("%Y%m%d"))
            cur.execute("SELECT DISTINCT date_int FROM institutional_investors WHERE date_int >= ?", (check_start,))
            existing_dates = {r[0] for r in cur.fetchall()}
            
        # 3. æ‰¾å‡ºç¼ºæ¼æ—¥æœŸ (æ’é™¤ä¼‘å¸‚æ—¥ï¼Œä»Šå¤©åªæœ‰åœ¨ 14:00 å¾Œæ‰å˜—è©¦å›è£œ)
        today_int = int(datetime.now().strftime("%Y%m%d"))
        current_hour = datetime.now().hour
        
        missing_dates = []
        for d in dates_to_check:
            d_int = int(d.strftime("%Y%m%d"))
            if d_int in existing_dates:
                continue
            if is_market_holiday(d_int):
                continue
            # ä»Šå¤©åªæœ‰åœ¨ 14:00 å¾Œæ‰å˜—è©¦å›è£œ (æ”¶ç›¤ 13:30ï¼Œç›¤å¾Œæ›´æ–°ç´„ 14:00)
            if d_int == today_int and current_hour < 14:
                continue
            missing_dates.append(d)
        
        if not missing_dates:
            print_flush("âœ“ æ³•äººè³‡æ–™å®Œæ•´ï¼Œç„¡é ˆè£œæ¼")
            return

        print_flush(f"ç™¼ç¾ {len(missing_dates)} å¤©ç¼ºæ¼ï¼Œé–‹å§‹å›è£œ...")
        
        # 4. åŸ·è¡Œå›è£œ
        total_inserted = 0
        for i, dt in enumerate(missing_dates):
            date_str = dt.strftime("%Y%m%d")
            date_int = int(date_str)
            print_flush(f"\r[{i+1}/{len(missing_dates)}] è™•ç† {dt.strftime('%Y-%m-%d')} ... ", end="")
            
            inst_data = []
            
            # --- TWSE (T86) ---
            try:
                url = f'https://www.twse.com.tw/rwd/zh/fund/T86?response=csv&date={date_str}&selectType=ALLBUT0999'
                # éš¨æ©Ÿå»¶é²
                time.sleep(random.uniform(2.0, 4.0))
                r = requests.get(url, timeout=15, verify=False)
                
                if r.status_code == 200 and len(r.text) > 100:
                    df = pd.read_csv(StringIO(r.text), header=1).dropna(how='all', axis=1).dropna(how='any')
                    df = df.astype(str).apply(lambda s: s.str.replace(',', ''))
                    if 'è­‰åˆ¸ä»£è™Ÿ' in df.columns:
                        df['code'] = df['è­‰åˆ¸ä»£è™Ÿ'].str.replace('=', '').str.replace('"', '').str.strip()
                        df = df[df['code'].str.len() == 4]
                        
                        for _, row in df.iterrows():
                            try:
                                code = row['code']
                                f_buy = safe_int(row.get('å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)è²·é€²è‚¡æ•¸', 0))
                                f_sell = safe_int(row.get('å¤–è³‡åŠé™¸è³‡(ä¸å«å¤–è³‡è‡ªç‡Ÿå•†)è³£å‡ºè‚¡æ•¸', 0))
                                t_buy = safe_int(row.get('æŠ•ä¿¡è²·é€²è‚¡æ•¸', 0))
                                t_sell = safe_int(row.get('æŠ•ä¿¡è³£å‡ºè‚¡æ•¸', 0))
                                d_buy = safe_int(row.get('è‡ªç‡Ÿå•†è²·é€²è‚¡æ•¸(è‡ªè¡Œè²·è³£)', 0))
                                d_sell = safe_int(row.get('è‡ªç‡Ÿå•†è³£å‡ºè‚¡æ•¸(è‡ªè¡Œè²·è³£)', 0))
                                inst_data.append((code, date_int, f_buy, f_sell, t_buy, t_sell, d_buy, d_sell))
                            except: pass
            except Exception as e:
                pass # TWSE å¤±æ•—
                
            # --- TPEx ---
            try:
                d_obj = dt
                roc_date = f'{d_obj.year - 1911}/{d_obj.month:02d}/{d_obj.day:02d}'
                url = f'https://www.tpex.org.tw/web/stock/3insti/daily_trade/3itrade_hedge_result.php?l=zh-tw&d={roc_date}&se=EW&t=D&o=json'
                time.sleep(random.uniform(2.0, 4.0))
                r = requests.get(url, timeout=15, verify=False)
                data = r.json()
                
                tables = data.get('tables', [])
                if tables and isinstance(tables, list) and len(tables) > 0:
                    table_data = tables[0].get('data', [])
                    for row in table_data:
                        try:
                            code = str(row[0]).strip()
                            if len(code) != 4: continue
                            f_buy = safe_int(row[2])
                            f_sell = safe_int(row[3])
                            t_buy = safe_int(row[5])
                            t_sell = safe_int(row[6])
                            d_buy = safe_int(row[8])
                            d_sell = safe_int(row[9])
                            inst_data.append((code, date_int, f_buy, f_sell, t_buy, t_sell, d_buy, d_sell))
                        except: pass
            except Exception as e:
                pass # TPEx å¤±æ•—
            
            # å¯«å…¥è³‡æ–™åº«
            if inst_data:
                with db_manager.get_connection() as conn:
                    cur = conn.cursor()
                    cur.executemany("""
                        INSERT OR REPLACE INTO institutional_investors 
                        (code, date_int, foreign_buy, foreign_sell, trust_buy, trust_sell, dealer_buy, dealer_sell)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?)
                    """, inst_data)
                    conn.commit()
                print_flush(f"æˆåŠŸ ({len(inst_data)} ç­†)")
                total_inserted += len(inst_data)
            else:
                print_flush("ç„¡è³‡æ–™")
                
        print_flush(f"âœ“ æ³•äººè³‡æ–™æ›´æ–°å®Œæˆï¼Œå…±æ–°å¢ {total_inserted} ç­†ç´€éŒ„")
        
        # [æ–°å¢] åŒæ­¥æœ€æ–°æ³•äººæ•¸æ“šåˆ° stock_snapshot
        print_flush("æ­£åœ¨åŒæ­¥æœ€æ–°æ³•äººæ•¸æ“šåˆ°å¿«ç…§è¡¨...")
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            cur.execute("""
                UPDATE stock_snapshot
                SET foreign_buy = (SELECT foreign_buy - foreign_sell FROM institutional_investors WHERE code = stock_snapshot.code ORDER BY date_int DESC LIMIT 1),
                    trust_buy = (SELECT trust_buy - trust_sell FROM institutional_investors WHERE code = stock_snapshot.code ORDER BY date_int DESC LIMIT 1),
                    dealer_buy = (SELECT dealer_buy - dealer_sell FROM institutional_investors WHERE code = stock_snapshot.code ORDER BY date_int DESC LIMIT 1)
                WHERE EXISTS (SELECT 1 FROM institutional_investors WHERE code = stock_snapshot.code)
            """)
            conn.commit()
        print_flush("âœ“ å¿«ç…§è¡¨æ³•äººæ•¸æ“šæ›´æ–°å®Œæˆ")
            
    except Exception as e:
        
        # ä½¿ç”¨ requests ä¸‹è¼‰ (é¿é–‹ SSL éŒ¯èª¤)
        import requests
        import io
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, verify=False)
        response.raise_for_status()
        
        df = pd.read_csv(io.StringIO(response.text))
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns or 'æŒè‚¡åˆ†ç´š' not in df.columns or 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%' not in df.columns:
            print_flush("âŒ CSV æ ¼å¼ä¸ç¬¦ï¼Œè·³é")
            return

        # è™•ç†è³‡æ–™
        df['æŒè‚¡åˆ†ç´š'] = pd.to_numeric(df['æŒè‚¡åˆ†ç´š'], errors='coerce')
        df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].astype(str)
        
        # 1. è¨ˆç®—åƒå¼µå¤§æˆ¶æŒè‚¡æ¯”ä¾‹ (æŒè‚¡åˆ†ç´š 15: 1,000,001è‚¡ä»¥ä¸Š)
        # æ³¨æ„: ç´šåˆ¥ 17 æ˜¯åˆè¨ˆï¼Œä¸èƒ½åŠ ç¸½ï¼
        # è‹¥è¦è¨ˆç®— 400å¼µä»¥ä¸Šï¼Œå¯ä½¿ç”¨ isin([12, 13, 14, 15])
        # é€™è£¡ä¾æ“šä½¿ç”¨è€…éœ€æ±‚ (1000å¼µä»¥ä¸Š)ï¼Œåªå–ç´šåˆ¥ 15
        df_major = df[df['æŒè‚¡åˆ†ç´š'] == 15].copy()
        major_holders = df_major.groupby('è­‰åˆ¸ä»£è™Ÿ')['å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%'].sum().to_dict()
        
        # 2. å–å¾—ç¸½è‚¡æ±äººæ•¸ (æŒè‚¡åˆ†ç´š 17: åˆè¨ˆ)
        df_total = df[df['æŒè‚¡åˆ†ç´š'] == 17].copy()
        # ç§»é™¤äººæ•¸ä¸­çš„é€—è™Ÿä¸¦è½‰ç‚ºæ•´æ•¸
        if df_total['äººæ•¸'].dtype == object:
            df_total['äººæ•¸'] = df_total['äººæ•¸'].astype(str).str.replace(',', '')
        df_total['äººæ•¸'] = pd.to_numeric(df_total['äººæ•¸'], errors='coerce').fillna(0).astype(int)
        total_shareholders = df_total.set_index('è­‰åˆ¸ä»£è™Ÿ')['äººæ•¸'].to_dict()
        
        if not major_holders:
            print_flush("âš  æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„å¤§æˆ¶è³‡æ–™")
            return
            
        print_flush(f"å–å¾— {len(major_holders)} æª”è‚¡ç¥¨çš„å¤§æˆ¶æŒè‚¡è³‡æ–™ï¼Œæ­£åœ¨æ›´æ–°è³‡æ–™åº«...")
        
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            count = 0
            for code, pct in major_holders.items():
                holders = total_shareholders.get(code, 0)
                # æ›´æ–°å¤§æˆ¶æ¯”ä¾‹èˆ‡ç¸½è‚¡æ±äººæ•¸
                cur.execute("""
                    UPDATE stock_snapshot 
                    SET major_holders_pct=?, total_shareholders=? 
                    WHERE code=?
                """, (pct, holders, code))
                count += 1
            conn.commit()
            
def step3_6_download_major_holders(force=False, silent_header=False):
    """æ­¥é©Ÿ3.6: ä¸‹è¼‰é›†ä¿æˆ¶è‚¡æ¬Šåˆ†æ•£è¡¨ (åƒå¼µå¤§æˆ¶ & ç¸½è‚¡æ±äººæ•¸) - æ¯é€±äº”æ›´æ–°"""
    if not silent_header:
        print_flush("\n[Step 3.6] ä¸‹è¼‰é›†ä¿æˆ¶è‚¡æ¬Šåˆ†æ•£è¡¨ (åƒå¼µå¤§æˆ¶ & ç¸½è‚¡æ±äººæ•¸)...")
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºé€±äº” (weekday 4)ã€é€±å…­ (5) æˆ–é€±æ—¥ (6) æˆ–è³‡æ–™ç¼ºå¤±
    # æ“´å±•ç‚ºé€±æœ«ä¸‰å¤©éƒ½å¯æ›´æ–°ï¼Œé¿å…é€±äº”å¿˜è¨˜
    should_run = force or datetime.now().weekday() in [4, 5, 6]
    
    if not should_run:
        # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å®Œå…¨ç¼ºå¤±é›†ä¿è³‡æ–™
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                cur.execute("SELECT COUNT(*) FROM stock_snapshot WHERE total_shareholders IS NULL OR total_shareholders = 0")
                missing_count = cur.fetchone()[0]
                if missing_count > 1000: # è‹¥å¤§é‡ç¼ºå¤±ï¼Œå¼·åˆ¶åŸ·è¡Œ
                    print_flush(f"âš  åµæ¸¬åˆ° {missing_count} ç­†é›†ä¿è³‡æ–™ç¼ºå¤±ï¼Œå¼·åˆ¶åŸ·è¡Œä¸‹è¼‰")
                    should_run = True
        except:
            pass
            
    if not should_run:
        print_flush("âš  éæ›´æ–°æ™‚æ®µ (é€±äº”è‡³é€±æ—¥) ä¸”è³‡æ–™å®Œæ•´ï¼Œè·³éé›†ä¿å¤§æˆ¶è³‡æ–™ä¸‹è¼‰")
        return
    
    url = "https://smart.tdcc.com.tw/opendata/getOD.ashx?id=1-5"
    
    try:
        print_flush("æ­£åœ¨ä¸‹è¼‰ CSV (è³‡æ–™é‡å¤§ï¼Œè«‹ç¨å€™)...")
        
        # ä½¿ç”¨ requests ä¸‹è¼‰ (é¿é–‹ SSL éŒ¯èª¤)
        import requests
        import io
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}
        response = requests.get(url, headers=headers, verify=False)
        response.raise_for_status()
        
        df = pd.read_csv(io.StringIO(response.text))
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        if 'è­‰åˆ¸ä»£è™Ÿ' not in df.columns or 'æŒè‚¡åˆ†ç´š' not in df.columns or 'å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%' not in df.columns:
            print_flush("âŒ CSV æ ¼å¼ä¸ç¬¦ï¼Œè·³é")
            return

        # è™•ç†è³‡æ–™
        df['æŒè‚¡åˆ†ç´š'] = pd.to_numeric(df['æŒè‚¡åˆ†ç´š'], errors='coerce')
        df['è­‰åˆ¸ä»£è™Ÿ'] = df['è­‰åˆ¸ä»£è™Ÿ'].astype(str).str.strip()
        
        # 1. è¨ˆç®—åƒå¼µå¤§æˆ¶æŒè‚¡æ¯”ä¾‹ (æŒè‚¡åˆ†ç´š 15: 1,000,001è‚¡ä»¥ä¸Š)
        # æ³¨æ„: ç´šåˆ¥ 17 æ˜¯åˆè¨ˆï¼Œä¸èƒ½åŠ ç¸½ï¼
        # è‹¥è¦è¨ˆç®— 400å¼µä»¥ä¸Šï¼Œå¯ä½¿ç”¨ isin([12, 13, 14, 15])
        # é€™è£¡ä¾æ“šä½¿ç”¨è€…éœ€æ±‚ (1000å¼µä»¥ä¸Š)ï¼Œåªå–ç´šåˆ¥ 15
        df_major = df[df['æŒè‚¡åˆ†ç´š'] == 15].copy()
        major_holders = df_major.groupby('è­‰åˆ¸ä»£è™Ÿ')['å é›†ä¿åº«å­˜æ•¸æ¯”ä¾‹%'].sum().to_dict()
        
        # 2. å–å¾—ç¸½è‚¡æ±äººæ•¸ (æŒè‚¡åˆ†ç´š 17: åˆè¨ˆ)
        df_total = df[df['æŒè‚¡åˆ†ç´š'] == 17].copy()
        # ç§»é™¤äººæ•¸ä¸­çš„é€—è™Ÿä¸¦è½‰ç‚ºæ•´æ•¸
        if df_total['äººæ•¸'].dtype == object:
            df_total['äººæ•¸'] = df_total['äººæ•¸'].astype(str).str.replace(',', '')
        df_total['äººæ•¸'] = pd.to_numeric(df_total['äººæ•¸'], errors='coerce').fillna(0).astype(int)
        total_shareholders = df_total.set_index('è­‰åˆ¸ä»£è™Ÿ')['äººæ•¸'].to_dict()
        
        if not major_holders:
            print_flush("âš  æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„å¤§æˆ¶è³‡æ–™")
            return
            
        print_flush(f"å–å¾— {len(major_holders)} æª”è‚¡ç¥¨çš„å¤§æˆ¶æŒè‚¡è³‡æ–™ï¼Œæ­£åœ¨æ›´æ–°è³‡æ–™åº«...")
        
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            count = 0
            for code, pct in major_holders.items():
                holders = total_shareholders.get(code, 0)
                # æ›´æ–°å¤§æˆ¶æ¯”ä¾‹èˆ‡ç¸½è‚¡æ±äººæ•¸
                cur.execute("""
                    UPDATE stock_snapshot 
                    SET major_holders_pct=?, total_shareholders=? 
                    WHERE code=?
                """, (pct, holders, code))
                count += 1
            conn.commit()
            
        today_display = datetime.now().strftime("%Y-%m-%d")
        print_flush(f"âœ“ å·²æ›´æ–° {count} æª”å¤§æˆ¶æŒè‚¡æ¯”ä¾‹èˆ‡ç¸½è‚¡æ±äººæ•¸ ({today_display})")
        
    except Exception as e:
        print_flush(f"âŒ ä¸‹è¼‰æˆ–è™•ç†å¤±æ•—: {e}")


def step3_7_fetch_margin_data(days=60, silent_header=False):
    """æ­¥é©Ÿ3.7: ä¸‹è¼‰èè³‡èåˆ¸è³‡æ–™ (å®˜æ–¹ OpenAPI ç‚ºä¸»ï¼ŒFinMind/ç¶²é ç‚ºå‚™æ´)"""
    if not silent_header:
        print_flush(f"\n[Step 3.7] ä¸‹è¼‰èè³‡èåˆ¸è³‡æ–™ (å®˜æ–¹ OpenAPI å„ªå…ˆ)...")
    
    try:
        # === A. å®˜æ–¹ OpenAPI (ä¸»è¦ä¾†æº - åªæŠ“ä»Šå¤©) ===
        today_int = int(datetime.now().strftime("%Y%m%d"))
        openapi_success = False
        
        try:
            print_flush("æ­£åœ¨å¾å®˜æ–¹ OpenAPI å–å¾—ä»Šæ—¥èè³‡èåˆ¸...")
            saved = MarginDataAPI.fetch_all_margin_data()
            if saved > 0:
                today_display = datetime.now().strftime("%Y-%m-%d")
                print_flush(f"âœ“ å®˜æ–¹ OpenAPI: å·²å„²å­˜ {saved} ç­†èè³‡èåˆ¸è³‡æ–™ ({today_display})")
                openapi_success = True
        except Exception as e:
            print_flush(f"âš  å®˜æ–¹ OpenAPI å¤±æ•—: {e}ï¼Œåˆ‡æ›è‡³å‚™æ´ä¾†æº...")
        
        # === B. æ­·å²è³‡æ–™è£œæ¼ (FinMind/ç¶²é å‚™æ´) ===
        print_flush(f"æª¢æŸ¥è¿‘ {days} å¤©æ­·å²ç¼ºæ¼...")
        
        # 1. æº–å‚™æ—¥æœŸåˆ—è¡¨
        base_date = datetime.now()
        dates_to_check = []
        for i in range(days + 10):
            dt = base_date - timedelta(days=i)
            if dt.weekday() < 5:
                dates_to_check.append(dt)
            if len(dates_to_check) >= days: break
            
        # 2. æª¢æŸ¥è³‡æ–™åº«ç¾æœ‰è³‡æ–™
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            check_start = int(dates_to_check[-1].strftime("%Y%m%d"))
            try:
                cur.execute("SELECT DISTINCT date_int FROM margin_data WHERE date_int >= ?", (check_start,))
                existing_dates = {r[0] for r in cur.fetchall()}
            except:
                existing_dates = set()
                
        # 3. æ‰¾å‡ºç¼ºæ¼æ—¥æœŸ (æ’é™¤ä¼‘å¸‚æ—¥ï¼Œä»Šå¤©åªæœ‰åœ¨ 14:00 å¾Œæ‰å˜—è©¦å›è£œ)
        current_hour = datetime.now().hour
        missing_dates = []
        for d in dates_to_check:
            d_int = int(d.strftime("%Y%m%d"))
            if d_int in existing_dates:
                continue
            if is_market_holiday(d_int):
                continue
            # ä»Šå¤©åªæœ‰åœ¨ 14:00 å¾Œæ‰å˜—è©¦å›è£œ (æ”¶ç›¤ 13:30ï¼Œç›¤å¾Œæ›´æ–°ç´„ 14:00)
            if d_int == today_int and current_hour < 14:
                continue
            missing_dates.append(d)
        
        if not missing_dates:
            print_flush("âœ“ èè³‡èåˆ¸è³‡æ–™å®Œæ•´ï¼Œç„¡é ˆè£œæ¼")
            return

        print_flush(f"ç™¼ç¾ {len(missing_dates)} å¤©ç¼ºæ¼ï¼Œé–‹å§‹å›è£œ (FinMind å„ªå…ˆ)...")
        
        finmind_limit_hit = False
        
        for i, dt in enumerate(missing_dates):
            d_dash = dt.strftime("%Y-%m-%d")
            d_nodash = dt.strftime("%Y%m%d")
            d_int = int(d_nodash)
            
            print_flush(f"\r[{i+1}/{len(missing_dates)}] è™•ç† {d_dash} ... ", end="")
            
            margin_data = None
            
            # --- B1. FinMind (å‚™æ´) ---
            if not finmind_limit_hit:
                try:
                    dataset = "TaiwanStockMarginPurchaseShortSale"
                    url = f"{FINMIND_URL}?dataset={dataset}&date={d_dash}&token={FINMIND_TOKEN}"
                    r = requests.get(url, timeout=10)

                    
                    if r.status_code == 429:
                        print_flush("â›” FinMind é™æµ! åˆ‡æ›è‡³ TWSE... ", end="")
                        finmind_limit_hit = True
                    elif r.status_code == 200:
                        data = r.json()
                        if data.get('msg') == 'success' and data.get('data'):
                            batch = []
                            for d in data['data']:
                                # FinMind å›å‚³çš„æ˜¯ Limitï¼Œé€™è£¡è½‰æ›ç‚º Rate (Balance / Limit * 100) ä»¥ç¬¦åˆ schema
                                # è‹¥ Limit ç‚º 0ï¼Œå‰‡ Rate ç‚º 0
                                m_bal = safe_int(d.get('MarginPurchaseTodayBalance'))
                                m_lim = safe_float(d.get('MarginPurchaseLimit'))
                                m_rate = round(m_bal / m_lim * 100, 2) if m_lim > 0 else 0.0
                                
                                s_bal = safe_int(d.get('ShortSaleTodayBalance'))
                                s_lim = safe_float(d.get('ShortSaleLimit'))
                                s_rate = round(s_bal / s_lim * 100, 2) if s_lim > 0 else 0.0
                                
                                batch.append((
                                    to_date_int(d.get('date')), d.get('stock_id'),
                                    safe_int(d.get('MarginPurchaseBuy')), safe_int(d.get('MarginPurchaseSell')), 
                                    safe_int(d.get('MarginPurchaseCashRepayment')), m_bal, m_rate,
                                    safe_int(d.get('ShortSaleBuy')), safe_int(d.get('ShortSaleSell')), 
                                    safe_int(d.get('ShortSaleCashRepayment')), s_bal, s_rate
                                ))
                            margin_data = batch
                            print_flush(f"FinMind({len(batch)}) ", end="")
                except Exception as e:
                    pass

            # --- B. TWSE/TPEx (å‚™æ´) ---
            if not margin_data:
                # TWSE
                try:
                    url = f"https://www.twse.com.tw/exchangeReport/MI_MARGN?response=json&date={d_nodash}&selectType=ALL"
                    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                    time.sleep(random.uniform(2.0, 4.0))
                    r = requests.get(url, headers=headers, timeout=15)
                    data = r.json()
                    
                    if data.get('stat') == 'OK':
                        raw_data = data.get('data', [])
                        batch = []
                        for row in raw_data:
                            code = row[0]
                            if len(code) != 4: continue
                            # TWSE row[8] æ˜¯èè³‡ä½¿ç”¨ç‡, row[15] æ˜¯èåˆ¸ä½¿ç”¨ç‡
                            batch.append((
                                int(d_nodash), code,
                                safe_int(row[2]), safe_int(row[3]), safe_int(row[4]), safe_int(row[6]), safe_float(row[8]), 
                                safe_int(row[9]), safe_int(row[10]), safe_int(row[11]), safe_int(row[13]), safe_float(row[15])
                            ))
                        if batch:
                            margin_data = batch
                            print_flush(f"TWSE({len(batch)}) ", end="")
                except: pass
                
                # TPEx (è‹¥ TWSE æ²’æŠ“åˆ°æˆ–éœ€è¦è£œ TPExï¼Œé€™è£¡ç°¡å–®èµ·è¦‹è‹¥ TWSE æœ‰å°±ä¸æŠ“ TPEx? ä¸ï¼Œæ‡‰è©²éƒ½è¦æŠ“)
                # ä½† fix.py çš„ fetch_margin_from_twse ä¼¼ä¹åªæŠ“ TWSE? 
                # æœ€çµ‚ä¿®æ­£.py åŸæœ¬æœ‰æŠ“ TPExã€‚
                # ç‚ºäº†å®Œæ•´æ€§ï¼Œæˆ‘å€‘ä¹ŸæŠ“ TPEx
                try:
                    d_obj = dt
                    roc_date = f"{d_obj.year - 1911}/{d_obj.month:02d}/{d_obj.day:02d}"
                    url = f"https://www.tpex.org.tw/web/stock/margin_trading/margin_balance/margin_bal_result.php?l=zh-tw&o=json&d={roc_date}&s=0,asc,0"
                    time.sleep(random.uniform(1.5, 3.0))
                    r = requests.get(url, timeout=10, verify=False)
                    data = r.json()
                    
                    if data.get('tables'):
                        tpex_batch = []
                        for row in data['tables'][0]['data']:
                            code = row[0]
                            if len(code) != 4: continue
                            # TPEx æ ¼å¼: ä»£è™Ÿ, åç¨±, èè³‡å‰æ—¥é¤˜é¡, èè³‡è²·é€², èè³‡è³£å‡º, èè³‡ç¾é‡‘å„Ÿé‚„, èè³‡ä»Šæ—¥é¤˜é¡, èè³‡ä½¿ç”¨ç‡, ...
                            # row[7] æ˜¯èè³‡ä½¿ç”¨ç‡, row[14] æ˜¯èåˆ¸ä½¿ç”¨ç‡
                            tpex_batch.append((
                                int(d_nodash), code,
                                safe_int(row[3]), safe_int(row[4]), safe_int(row[5]), safe_int(row[6]), safe_num(row[7]),
                                safe_int(row[10]), safe_int(row[11]), safe_int(row[12]), safe_int(row[13]), safe_num(row[14])
                            ))
                        if tpex_batch:
                            if margin_data is None: margin_data = []
                            margin_data.extend(tpex_batch)
                            print_flush(f"TPEx({len(tpex_batch)}) ", end="")
                except: pass

            # å¯«å…¥è³‡æ–™åº«
            if margin_data:
                with db_manager.get_connection() as conn:
                    cur = conn.cursor()
                    cur.executemany("""
                        INSERT OR REPLACE INTO margin_data 
                        (date_int, code, margin_buy, margin_sell, margin_redemp, margin_balance, margin_util_rate,
                         short_buy, short_sell, short_redemp, short_balance, short_util_rate)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, margin_data)
                    conn.commit()
                
                # åŒæ­¥åˆ° snapshot (åªåŒæ­¥æœ€æ–°çš„ä¸€å¤©)
                if d_int == int(datetime.now().strftime("%Y%m%d")):
                    print_flush("åŒæ­¥è‡³å¿«ç…§... ", end="")
                    with db_manager.get_connection() as conn:
                        cur = conn.cursor()
                        for rec in margin_data:
                            # rec: date_int, code, m_buy, m_sell, m_redemp, m_bal, m_rate, s_buy, s_sell, s_redemp, s_bal, s_rate
                            cur.execute("""
                                UPDATE stock_snapshot 
                                SET margin_balance=?, margin_util_rate=?, short_balance=?, short_util_rate=?
                                WHERE code=?
                            """, (rec[5], rec[6], rec[10], rec[11], rec[1]))
                        conn.commit()
            else:
                print_flush("ç„¡è³‡æ–™", end="")
            
            print_flush("") # Newline

    except Exception as e:
        print_flush(f"âŒ èè³‡èåˆ¸ä¸‹è¼‰å¤±æ•—: {e}")

def to_date_int(d):
    """è¼”åŠ©å‡½å¼: è½‰æ—¥æœŸæ•´æ•¸"""
    if isinstance(d, int): return d
    if isinstance(d, str):
        s = d.replace('-', '').replace('/', '').split(' ')[0]
        return int(s)
    return 0

def step3_8_fetch_market_index(date_str=None, silent_header=False):
    """æ­¥é©Ÿ3.8: ä¸‹è¼‰å¤§ç›¤æŒ‡æ•¸ (TWSE + TPEx)"""
    if not silent_header:
        print_flush("\n[Step 3.8] ä¸‹è¼‰å¤§ç›¤æŒ‡æ•¸...")
    
    if date_str is None:
        date_str = datetime.now().strftime('%Y%m%d')
        
    date_int = int(date_str)
    records = []
    
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
    
    # TWSE Index - ä½¿ç”¨ FMTQIK (æ¯æ—¥å¸‚å ´æˆäº¤è³‡è¨Š) APIï¼Œæ›´ç©©å®š
    try:
        url_twse = f"https://www.twse.com.tw/exchangeReport/FMTQIK?response=json&date={date_str}"
        r = requests.get(url_twse, headers=headers, timeout=15, verify=False)
        data = r.json()
        
        if data.get('stat') == 'OK' and data.get('data'):
            # æ‰¾ç•¶å¤©çš„è³‡æ–™
            for row in data['data']:
                # row[0] = æ—¥æœŸ (æ°‘åœ‹å¹´), row[1] = é–‹ç›¤, row[2] = æœ€é«˜, row[3] = æœ€ä½, row[4] = æ”¶ç›¤
                try:
                    parts = row[0].split('/')
                    western_year = int(parts[0]) + 1911
                    row_date_int = int(f"{western_year}{parts[1]}{parts[2]}")
                    
                    if row_date_int == date_int:
                        open_val = safe_num(row[1])
                        high_val = safe_num(row[2])
                        low_val = safe_num(row[3])
                        close_val = safe_num(row[4])
                        volume = safe_int(row[5]) if len(row) > 5 else 0
                        
                        if close_val > 0:
                            records.append((date_int, 'TAIEX', close_val, open_val, high_val, low_val, volume))
                        break
                except:
                    pass
            
            # å¦‚æœä»Šå¤©æ²’è³‡æ–™ï¼Œå–æœ€å¾Œä¸€ç­†
            if not records and data['data']:
                row = data['data'][-1]
                try:
                    parts = row[0].split('/')
                    western_year = int(parts[0]) + 1911
                    row_date_int = int(f"{western_year}{parts[1]}{parts[2]}")
                    open_val = safe_num(row[1])
                    high_val = safe_num(row[2])
                    low_val = safe_num(row[3])
                    close_val = safe_num(row[4])
                    volume = safe_int(row[5]) if len(row) > 5 else 0
                    
                    if close_val > 0:
                        records.append((row_date_int, 'TAIEX', close_val, open_val, high_val, low_val, volume))
                except:
                    pass
    except Exception as e:
        print_flush(f"âš  TWSE æŒ‡æ•¸ä¸‹è¼‰å¤±æ•—: {e}")

    # TPEx Index - ä½¿ç”¨ aftertrading API
    try:
        time.sleep(0.5)
        d_obj = datetime.strptime(date_str, '%Y%m%d')
        roc_date = f"{d_obj.year - 1911}/{d_obj.month:02d}/{d_obj.day:02d}"
        url_tpex = f"https://www.tpex.org.tw/web/stock/aftertrading/otc_index_summary/OTC_index_summary_result.php?l=zh-tw&d={roc_date}&o=json"
        
        r = requests.get(url_tpex, headers=headers, timeout=15, verify=False)
        data_tpex = r.json()
        
        if data_tpex.get('aaData'):
            # aaData[0] é€šå¸¸æ˜¯æ«ƒè²·æŒ‡æ•¸
            for row in data_tpex['aaData']:
                if 'æ«ƒè²·æŒ‡æ•¸' in str(row[0]) or 'OTC' in str(row[0]).upper():
                    close_val = safe_num(row[1]) if len(row) > 1 else 0
                    if close_val > 0:
                        records.append((date_int, 'TPEX', close_val, 0, 0, 0, 0))
                    break
    except Exception as e:
        # TPEx æŒ‡æ•¸ API ä¸ç©©å®šï¼Œéœé»˜è™•ç†
        pass
                    
    if records:
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                # æª¢æŸ¥æ˜¯å¦å·²æœ‰ä»Šæ—¥è³‡æ–™
                first_date = records[0][0]
                cur.execute("SELECT COUNT(*) FROM market_index WHERE date_int = ?", (first_date,))
                existing_count = cur.fetchone()[0]
                
                if existing_count > 0:
                    # æ ¼å¼åŒ–æ—¥æœŸé¡¯ç¤º
                    date_display = f"{str(first_date)[:4]}-{str(first_date)[4:6]}-{str(first_date)[6:]}"
                    print_flush(f"âœ“ å¤§ç›¤æŒ‡æ•¸ ({date_display}) å·²æ˜¯æœ€æ–°")
                else:
                    cur.executemany("""
                        INSERT OR REPLACE INTO market_index (
                            date_int, index_id, close, open, high, low, volume
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, records)
                    conn.commit()
                    date_display = f"{str(first_date)[:4]}-{str(first_date)[4:6]}-{str(first_date)[6:]}"
                    print_flush(f"âœ“ å·²æ›´æ–°å¤§ç›¤æŒ‡æ•¸è³‡æ–™ ({date_display}, {len(records)} ç­†)")
        except Exception as e:
            print_flush(f"âŒ å¤§ç›¤æŒ‡æ•¸å„²å­˜å¤±æ•—: {e}")
    else:
        print_flush("âš  ä»Šæ—¥å°šç„¡å¤§ç›¤æŒ‡æ•¸è³‡æ–™ (å¯èƒ½å°šæœªæ”¶ç›¤)")

def step4_load_data():
    """æ­¥é©Ÿ4: è¼‰å…¥åˆ†æè³‡æ–™ (æ–°ä¸‰è¡¨æ¶æ§‹)"""
    print_flush("\n[Step 4] è¼‰å…¥åˆ†æè³‡æ–™...")
    data = {}
    
    with db_manager.get_connection() as conn:
        conn.row_factory = sqlite3.Row
        cur = conn.cursor()
        
        # å¾å¿«ç…§è¡¨è®€å– (æ–°ä¸‰è¡¨æ¶æ§‹ï¼Œä¸å† Fallback åˆ°èˆŠè¡¨)
        try:
            rows = cur.execute("SELECT * FROM stock_snapshot").fetchall()
            for row in rows:
                data[row['code']] = dict(row)
        except Exception as e:
            print_flush(f"âš  å¿«ç…§è¡¨è®€å–å¤±æ•—: {e}")
            
    print_flush(f"âœ“ å·²è¼‰å…¥ {len(data)} æª”è‚¡ç¥¨è³‡æ–™")
    return data

def _auto_fix_missing_amount():
    """
    è‡ªå‹•ä¿®å¾©ç¼ºå¤±çš„æˆäº¤é‡‘é¡/æ”¶ç›¤åƒ¹/æˆäº¤é‡
    
    ä¿®å¾©é‚è¼¯çŸ©é™£ï¼š
    1. æœ‰é‡ã€æœ‰åƒ¹ã€æœ‰é¡ â†’ æ­£å¸¸
    2. æœ‰é‡ã€æœ‰åƒ¹ã€ç„¡é¡ â†’ é¡ = é‡ Ã— åƒ¹
    3. æœ‰é‡ã€ç„¡åƒ¹ã€æœ‰é¡ â†’ åƒ¹ = é¡ Ã· é‡
    4. æœ‰é‡ã€ç„¡åƒ¹ã€ç„¡é¡ â†’ éœ€è¦çˆ¬èŸ² (å…ˆè·³é)
    5. ç„¡é‡ã€æœ‰åƒ¹ã€æœ‰é¡ â†’ é‡ = é¡ Ã· åƒ¹
    6. ç„¡é‡ã€æœ‰åƒ¹ã€ç„¡é¡ â†’ éœ€è¦çˆ¬èŸ² (å…ˆè·³é)
    7. ç„¡é‡ã€ç„¡åƒ¹ã€æœ‰é¡ â†’ éœ€è¦çˆ¬èŸ² (å…ˆè·³é)
    8. ç„¡é‡ã€ç„¡åƒ¹ã€ç„¡é¡ â†’ å¯èƒ½åœç‰Œæˆ–ä¸‹å¸‚ (æ¨™è¨˜ç‚ºé›¶æˆäº¤)
    """
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            
            # æƒ…æ³ 2: æœ‰é‡ã€æœ‰åƒ¹ã€ç„¡é¡ â†’ é¡ = é‡ Ã— åƒ¹
            cur.execute("""
                SELECT code, date_int, close, volume 
                FROM stock_history 
                WHERE volume > 0 AND close > 0 AND (amount IS NULL OR amount = 0)
            """)
            case2 = cur.fetchall()
            if case2:
                updates = [(int(close * volume), code, date_int) for code, date_int, close, volume in case2]
                cur.executemany("UPDATE stock_history SET amount = ? WHERE code = ? AND date_int = ?", updates)
                print_flush(f"  [ä¿®å¾©] æƒ…æ³2 (æœ‰é‡æœ‰åƒ¹ç„¡é¡): {len(case2)} ç­† â†’ é¡ = é‡ Ã— åƒ¹")
            
            # æƒ…æ³ 3: æœ‰é‡ã€ç„¡åƒ¹ã€æœ‰é¡ â†’ åƒ¹ = é¡ Ã· é‡
            cur.execute("""
                SELECT code, date_int, amount, volume 
                FROM stock_history 
                WHERE volume > 0 AND (close IS NULL OR close = 0) AND amount > 0
            """)
            case3 = cur.fetchall()
            if case3:
                updates = [(round(amount / volume, 2), code, date_int) for code, date_int, amount, volume in case3]
                cur.executemany("UPDATE stock_history SET close = ? WHERE code = ? AND date_int = ?", updates)
                print_flush(f"  [ä¿®å¾©] æƒ…æ³3 (æœ‰é‡ç„¡åƒ¹æœ‰é¡): {len(case3)} ç­† â†’ åƒ¹ = é¡ Ã· é‡")
            
            # æƒ…æ³ 5: ç„¡é‡ã€æœ‰åƒ¹ã€æœ‰é¡ â†’ é‡ = é¡ Ã· åƒ¹
            cur.execute("""
                SELECT code, date_int, amount, close 
                FROM stock_history 
                WHERE (volume IS NULL OR volume = 0) AND close > 0 AND amount > 0
            """)
            case5 = cur.fetchall()
            if case5:
                updates = [(int(amount / close), code, date_int) for code, date_int, amount, close in case5]
                cur.executemany("UPDATE stock_history SET volume = ? WHERE code = ? AND date_int = ?", updates)
                print_flush(f"  [ä¿®å¾©] æƒ…æ³5 (ç„¡é‡æœ‰åƒ¹æœ‰é¡): {len(case5)} ç­† â†’ é‡ = é¡ Ã· åƒ¹")
            
            # æƒ…æ³ 4, 6, 7: éœ€è¦çˆ¬èŸ²æŠ“å–
            cur.execute("""
                SELECT code, date_int, close, volume, amount 
                FROM stock_history 
                WHERE (volume > 0 AND (close IS NULL OR close = 0) AND (amount IS NULL OR amount = 0))
                   OR ((volume IS NULL OR volume = 0) AND close > 0 AND (amount IS NULL OR amount = 0))
                   OR ((volume IS NULL OR volume = 0) AND (close IS NULL OR close = 0) AND amount > 0)
                ORDER BY code, date_int
            """)
            need_crawl = cur.fetchall()
            
            if need_crawl:
                fixed_by_crawl = 0
                fixed_by_prev = 0
                
                # æŒ‰æ—¥æœŸåˆ†çµ„
                from collections import defaultdict
                by_date = defaultdict(list)
                for code, date_int, close, volume, amount in need_crawl:
                    by_date[date_int].append((code, close, volume, amount))
                
                for date_int, stocks in by_date.items():
                    # å˜—è©¦å¾ TWSE/TPEx æŠ“å–è©²æ—¥è³‡æ–™
                    try:
                        date_str = str(date_int)
                        url_twse = f"https://www.twse.com.tw/exchangeReport/MI_INDEX?response=json&date={date_str}&type=ALLBUT0999"
                        url_tpex = f"https://www.tpex.org.tw/openapi/v1/tpex_mainboard_daily_close_quotes"
                        
                        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                        
                        # æŠ“å– TWSE è³‡æ–™
                        crawled_data = {}
                        try:
                            resp = requests.get(url_twse, headers=headers, timeout=15, verify=False)
                            data = resp.json()
                            if data.get('stat') == 'OK':
                                # æ‰¾åˆ°å€‹è‚¡è³‡æ–™ (é€šå¸¸åœ¨ tables[8] æˆ–é¡ä¼¼ä½ç½®)
                                for table in data.get('tables', []):
                                    if table.get('title') and 'æ¯æ—¥æ”¶ç›¤è¡Œæƒ…' in table.get('title', ''):
                                        for row in table.get('data', []):
                                            if len(row) >= 9:
                                                c = str(row[0]).strip()
                                                if len(c) == 4 and c.isdigit():
                                                    try:
                                                        crawled_data[c] = {
                                                            'close': safe_num(row[8]),
                                                            'volume': safe_int(row[2]),
                                                            'amount': safe_int(row[4])
                                                        }
                                                    except:
                                                        pass
                        except:
                            pass
                        
                        # æŠ“å– TPEx è³‡æ–™
                        try:
                            # è½‰æ›æ—¥æœŸæ ¼å¼ç‚ºæ°‘åœ‹å¹´
                            d_obj = datetime.strptime(date_str, '%Y%m%d')
                            roc_date = f"{d_obj.year - 1911}/{d_obj.month:02d}/{d_obj.day:02d}"
                            url_tpex = f"https://www.tpex.org.tw/web/stock/aftertrading/otc_quotes_no1430/stk_wn1430_result.php?l=zh-tw&d={roc_date}&o=json"
                            
                            resp = requests.get(url_tpex, headers=headers, timeout=15, verify=False)
                            data = resp.json()
                            
                            if data.get('aaData'):
                                for row in data['aaData']:
                                    if len(row) >= 6:
                                        c = str(row[0]).strip()
                                        if len(c) == 4 and c.isdigit():
                                            try:
                                                crawled_data[c] = {
                                                    'close': safe_num(row[2]),  # æ”¶ç›¤
                                                    'volume': safe_int(row[8]) if len(row) > 8 else 0,  # æˆäº¤é‡
                                                    'amount': safe_int(row[9]) if len(row) > 9 else 0   # æˆäº¤é‡‘é¡
                                                }
                                            except:
                                                pass
                        except:
                            pass
                        
                        # ç”¨çˆ¬å–çš„è³‡æ–™æ›´æ–°
                        for code, old_close, old_volume, old_amount in stocks:
                            if code in crawled_data:
                                cdata = crawled_data[code]
                                new_close = cdata.get('close') or old_close
                                new_volume = cdata.get('volume') or old_volume
                                new_amount = cdata.get('amount') or old_amount
                                
                                # å¦‚æœé‚„æ˜¯ç¼ºï¼Œç”¨è¨ˆç®—è£œé½Š
                                if new_volume and new_close and not new_amount:
                                    new_amount = int(new_volume * new_close)
                                if new_amount and new_close and not new_volume:
                                    new_volume = int(new_amount / new_close) if new_close > 0 else 0
                                if new_amount and new_volume and not new_close:
                                    new_close = round(new_amount / new_volume, 2) if new_volume > 0 else 0
                                
                                if new_close and new_volume and new_amount:
                                    cur.execute("UPDATE stock_history SET close=?, volume=?, amount=? WHERE code=? AND date_int=?",
                                               (new_close, new_volume, new_amount, code, date_int))
                                    fixed_by_crawl += 1
                            else:
                                # æ²’æŠ“åˆ°ï¼Œç”¨å‰ä¸€å¤©ä¼°ç®—
                                cur.execute("""
                                    SELECT close FROM stock_history 
                                    WHERE code = ? AND date_int < ? AND close > 0
                                    ORDER BY date_int DESC LIMIT 1
                                """, (code, date_int))
                                prev = cur.fetchone()
                                if prev and prev[0] > 0:
                                    prev_close = prev[0]
                                    if old_volume and old_volume > 0:
                                        est_amount = int(prev_close * old_volume)
                                        cur.execute("UPDATE stock_history SET close=?, amount=? WHERE code=? AND date_int=?",
                                                   (prev_close, est_amount, code, date_int))
                                        fixed_by_prev += 1
                        
                        time.sleep(0.3)  # é¿å…è«‹æ±‚éå¿«
                        
                    except Exception as e:
                        pass
                
                if fixed_by_crawl > 0:
                    print_flush(f"  [ä¿®å¾©] æƒ…æ³4/6/7 (çˆ¬èŸ²): {fixed_by_crawl} ç­† â†’ å¾ TWSE/TPEx æŠ“å–")
                if fixed_by_prev > 0:
                    print_flush(f"  [ä¿®å¾©] æƒ…æ³4/6/7 (ä¼°ç®—): {fixed_by_prev} ç­† â†’ ç”¨å‰æ—¥åƒ¹æ ¼ä¼°ç®—")
            
            # æƒ…æ³ 8: ç„¡é‡ã€ç„¡åƒ¹ã€ç„¡é¡ â†’ ä¿æŒä¸è®Š (å¯èƒ½åœç‰Œæˆ–ä¸‹å¸‚)
            cur.execute("""
                SELECT COUNT(*) FROM stock_history 
                WHERE (volume IS NULL OR volume = 0) 
                  AND (close IS NULL OR close = 0) 
                  AND (amount IS NULL OR amount = 0)
            """)
            case8_count = cur.fetchone()[0]
            if case8_count > 0:
                print_flush(f"  [ç•¥é] æƒ…æ³8 (å…¨ç„¡): {case8_count} ç­† (å¯èƒ½åœç‰Œ/ä¸‹å¸‚)")
            
            conn.commit()
            
    except Exception as e:
        print_flush(f"  âš  è‡ªå‹•ä¿®å¾©å¤±æ•—: {e}")

def step6_verify_and_backfill(data=None, resume=False, skip_downloads=False):
    """æ­¥é©Ÿ6: é©—è­‰è³‡æ–™å®Œæ•´æ€§èˆ‡å›è£œ (å« amount èˆ‡æ³•äººè³‡æ–™)"""
    print_flush("\n[Step 6] é©—è­‰è³‡æ–™å®Œæ•´æ€§èˆ‡å›è£œ...")
    
    # 0. è‡ªå‹•ä¿®å¾©ç¼ºé‡‘é¡è¨˜éŒ„ (ç”¨ close * volume ä¼°ç®—)
    _auto_fix_missing_amount()
    
    if not skip_downloads:
        # 1. æª¢æŸ¥ä¸¦è£œé½Šæ³•äººè³‡æ–™ (æ™ºæ…§æ¨¡å¼)
        step3_5_download_institutional(days=3)
        
        # 2. ä¸‹è¼‰é›†ä¿å¤§æˆ¶è³‡æ–™ (æ¯é€±ä¸€æ¬¡ï¼Œé€™è£¡æ¯æ¬¡æª¢æŸ¥æ›´æ–°)
        step3_6_download_major_holders()
    
    if data is None:
        data = step4_load_data()
    
    # æ”¶é›†éœ€è¦å›è£œçš„è‚¡ç¥¨ (ä½¿ç”¨æ–°ä¸‰è¡¨æ¶æ§‹)
    tasks = []
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        
        # å–å¾—å¸‚å ´æœ€æ–°äº¤æ˜“æ—¥ (ä½œç‚ºåŸºæº–)
        latest_market_date_str = get_latest_market_date()
        latest_market_date_int = int(latest_market_date_str.replace('-', ''))
        print_flush(f"åŸºæº–æœ€æ–°æ—¥æœŸ: {latest_market_date_int}")

        # Pre-load listing dates from stock_meta (Source of Truth)
        list_date_map = {}
        try:
            cur.execute("SELECT code, list_date FROM stock_meta")
            for r in cur.fetchall():
                if r[1]: list_date_map[r[0]] = r[1]
        except:
            pass

        # å„ªåŒ–: ä½¿ç”¨ GROUP BY ä¸€æ¬¡æŸ¥è©¢æ‰€æœ‰è‚¡ç¥¨çš„æ­·å²è³‡æ–™ç­†æ•¸èˆ‡ amount ç¼ºå¤±æ•¸ (åƒ…æª¢æŸ¥æœ€è¿‘ 3 å¹´)
        cutoff_date = (datetime.now() - timedelta(days=Config.HISTORY_DAYS_LOOKBACK)).strftime("%Y%m%d")
        cutoff_int = int(cutoff_date)
        
        print_flush("æ­£åœ¨åˆ†æè³‡æ–™åº«ç‹€æ…‹ (å«æˆäº¤é‡‘é¡èˆ‡æ™‚æ•ˆæª¢æŸ¥)...")
        cur.execute(f"""
            SELECT code, COUNT(*), MIN(date_int), MAX(date_int),
                   SUM(CASE WHEN volume > 0 AND (amount IS NULL OR amount = 0) AND date_int >= {cutoff_int} THEN 1 ELSE 0 END)
            FROM stock_history 
            GROUP BY code
        """)
        history_stats = {row[0]: {'count': row[1], 'min_date': row[2], 'max_date': row[3], 'missing_amount': row[4]} for row in cur.fetchall()}
        
        for code, info in data.items():
            stats = history_stats.get(code)
            
            # Guard Clause: No history data
            if not stats:
                tasks.append((code, info['name'], 0, "ç„¡æ­·å²è³‡æ–™"))
                continue
                
            count = stats['count']
            min_date_int = stats['min_date']
            max_date_int = stats['max_date'] or 0
            missing_amount = stats['missing_amount'] or 0
            
            # Guard Clause: Data is outdated
            if max_date_int < latest_market_date_int:
                tasks.append((code, info['name'], count, f"è³‡æ–™éèˆŠ(è‡³{max_date_int})"))
                continue
                
            # Guard Clause: Missing amount (Strict Check)
            # ç”¨æˆ¶å¼·èª¿: åªè¦å°‘ä¸€å¼µï¼ŒæŒ‡æ¨™éƒ½æœƒéŒ¯ï¼Œå› æ­¤å¿…é ˆåš´æ ¼æª¢æŸ¥
            if missing_amount > 0:
                tasks.append((code, info['name'], count, f"ç¼ºé‡‘é¡({missing_amount}ç­†)"))
                continue
            
            # Guard Clause: Insufficient count
            if count < MIN_DATA_COUNT:
                # Check if it's a new stock (listed recently) using stock_meta
                is_new_stock = False
                l_date_str = list_date_map.get(code)
                
                if l_date_str:
                    try:
                        l_date = datetime.strptime(l_date_str, '%Y-%m-%d')
                        # Calculate theoretical max market days since listing (approx 5/7 of total days)
                        # Or simply check if listing date is recent enough
                        days_since = (datetime.now() - l_date).days
                        # If listed less than MIN_DATA_COUNT * 1.5 days ago (approx), and we have most of the data
                        # expected_market_days approx days_since * 0.68 (taking holidays into account)
                        expected_count = int(days_since * 0.68)
                        
                        # If we have at least 90% of expected data, consider it complete
                        if count >= expected_count * 0.9:
                            is_new_stock = True
                        
                        # Also check if min_date is close to list_date (within 20 days)
                        if min_date_int:
                            min_date = datetime.strptime(str(min_date_int), '%Y%m%d')
                            if min_date <= l_date + timedelta(days=20):
                                is_new_stock = True
                                
                    except Exception as e:
                        # print_flush(f"Date parse error: {e}")
                        pass
                
                # Fallback to twstock if stock_meta missing (Legacy logic)
                if not is_new_stock and not l_date_str:
                    if min_date_int:
                        try:
                            stock_info = twstock.codes.get(code)
                            if stock_info and stock_info.start:
                                list_date = datetime.strptime(stock_info.start, '%Y/%m/%d')
                                min_date = datetime.strptime(str(min_date_int), '%Y%m%d')
                                if min_date <= list_date + timedelta(days=10):
                                    is_new_stock = True
                        except:
                            pass
                
                if not is_new_stock:
                    tasks.append((code, info['name'], count, f"ç­†æ•¸ä¸è¶³({count})"))
                    continue

            # If we reached here, data is considered complete
            pass
    
    if not tasks:
        print_flush(f"âœ“ æ‰€æœ‰è‚¡ç¥¨è³‡æ–™å®Œæ•´ (ç­†æ•¸å……è¶³ä¸”ç„¡ç¼ºå¤±é‡‘é¡)")
        return set()

    # è®€å–é€²åº¦
    progress = load_progress()
    start_idx = progress.get("last_code_index", 0) if resume else 0
    failed_stocks = set(progress.get("failed_stocks", []))
    
    # é‡ç½®é€²åº¦
    if not resume:
        save_progress(last_idx=0, failed_stocks=[])
        start_idx = 0
        failed_stocks = set()
    
    # Filter out failed stocks (Avoid infinite loop on same day)
    if resume or start_idx > 0:
        original_count = len(tasks)
        tasks = [t for t in tasks if t[0] not in failed_stocks]
        if len(tasks) < original_count:
            print_flush(f"âš  å·²ç•¥é {original_count - len(tasks)} æª”å…ˆå‰å¤±æ•—çš„è‚¡ç¥¨")

    if start_idx >= len(tasks):
        print_flush(f"âš  é€²åº¦ç´€éŒ„ ({start_idx}) è¶…å‡ºç•¶å‰ä»»å‹™ç¯„åœ ({len(tasks)})ï¼Œé‡ç½®é€²åº¦å¾é ­é–‹å§‹...")
        start_idx = 0
        save_progress(last_idx=0)
    
    print_flush(f"âš  ç™¼ç¾ {len(tasks)} æª”è‚¡ç¥¨è³‡æ–™ä¸è¶³ï¼Œé–‹å§‹å›è£œ...")
    
    if start_idx > 0:
        print_flush(f"ğŸ“ å¾ç¬¬ {start_idx+1} æª”ç¹¼çºŒ(å·²å®Œæˆ {start_idx} æª”)")
    
    tracker = ProgressTracker(total_lines=4)
    data_source_manager = DataSourceManager(progress_tracker=tracker, silent=False)
    
    success_count = 0
    verified_count = 0
    updated_codes = set()
    
    # é å…ˆè¼‰å…¥ä¸Šå¸‚æ—¥æœŸ Map
    list_date_map = {}
    try:
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            cur.execute("SELECT code, list_date FROM stock_meta")
            for r in cur.fetchall():
                if r[1]: list_date_map[r[0]] = r[1]
    except:
        pass

    with tracker:
        latest_date = get_latest_market_date()
        end_date = latest_date
        default_start_date = (datetime.strptime(latest_date, "%Y-%m-%d") - timedelta(days=1095)).strftime("%Y-%m-%d")
        
        for i in range(start_idx, len(tasks)):
            code, name, count, reason = tasks[i]
            
            # å‹•æ…‹èª¿æ•´ Start Date (ä¾æ“šä¸Šå¸‚æ—¥æœŸ)
            start_date = default_start_date
            l_date_str = list_date_map.get(code)
            if l_date_str:
                try:
                    # å‡è¨­ list_date æ ¼å¼ç‚º YYYY-MM-DD
                    if l_date_str > start_date:
                        start_date = l_date_str
                        # å¦‚æœä¸Šå¸‚æ—¥æœŸæ¯” end_date é‚„æ™š(ç†è«–ä¸Šä¸å¯èƒ½ï¼Œé™¤éè³‡æ–™éŒ¯)ï¼Œå‰‡ç„¡éœ€è£œ
                        if start_date > end_date:
                            tracker.update_lines(f"è·³é {code} {name}: ä¸Šå¸‚æ—¥æœŸ {l_date_str} æ™šæ–¼ {end_date}")
                            continue
                except:
                    pass

            tracker.update_lines(
                f"æ­£åœ¨å›è£œ: {code} {name} (è‡ª {start_date})",
                f"åŸå› : {reason}",
                f"é€²åº¦: {i+1}/{len(tasks)} | æˆåŠŸ: {success_count}",
                "æ­£åœ¨é€£æ¥ API..."
            )
            
            df = data_source_manager.fetch_history(code, start_date, end_date)
            
            if df is not None and not df.empty:
                try:
                    with db_manager.get_connection() as conn:
                        cur = conn.cursor()
                        
                        for _, row in df.iterrows():
                            # å¯«å…¥ stock_history (æ–°ä¸‰è¡¨æ¶æ§‹) - å«æˆäº¤é‡‘é¡
                            # ä½¿ç”¨ REPLACE ç¢ºä¿æ›´æ–° amount æ¬„ä½
                            date_int = int(str(row['date']).replace('-', ''))
                            cur.execute("""
                                INSERT OR REPLACE INTO stock_history 
                                (code, date_int, open, high, low, close, volume, amount)
                                VALUES (?,?,?,?,?,?,?,?)
                            """, (code, date_int, row.get('open'), row.get('high'), 
                                  row.get('low'), row.get('close'), row.get('volume'),
                                  row.get('amount')))
                        
                        conn.commit()
                        success_count += 1
                        updated_codes.add(code)
                        
                        # Remove from failed_stocks if it was there
                        if code in failed_stocks:
                            failed_stocks.remove(code)
                        
                except Exception:
                    pass
            else:
                # Mark as failed
                failed_stocks.add(code)
            
            # å„²å­˜é€²åº¦ (åŒ…å« failed_stocks)
            if (i + 1) % 10 == 0:
                save_progress(last_idx=i + 1, failed_stocks=list(failed_stocks))
                
            time.sleep(1)  # é¿å…éå¿«è«‹æ±‚
            
    # å®Œæˆå¾Œæ¸…é™¤é€²åº¦
    if os.path.exists(PROGRESS_FILE):
        os.remove(PROGRESS_FILE)
        
    print_flush(f"\nâœ“ å›è£œå®Œæˆ - æˆåŠŸ: {success_count}")
    print_flush(f"\nâœ“ å›è£œå®Œæˆ - æˆåŠŸ: {success_count}")
    return updated_codes



def step8_sync_supabase():
    """æ­¥é©Ÿ8: åŒæ­¥è³‡æ–™åˆ° Supabase"""
    print_flush("\n[Step 8] åŒæ­¥è³‡æ–™åˆ° Supabase (å·²åœç”¨)")
    return
    
    if not HAS_SUPABASE:
        print_flush("âŒ æœªå®‰è£ supabase å¥—ä»¶ï¼Œç„¡æ³•åŒæ­¥ (pip install supabase)")
        return

    print_flush("\n[Step 8] åŒæ­¥è³‡æ–™åˆ° Supabase...")
    
    # Supabase è¨­å®š
    url = "https://gqiyvefcldxslrqpqlri.supabase.co"
    key = "sb_secret_XSeaHx_76CRxA6j8nZ3qDg_nzgFgTAN"
    
    try:
        supabase: Client = create_client(url, key)
        
        with db_manager.get_connection() as conn:
            conn.row_factory = sqlite3.Row
            cur = conn.cursor()
            
            # 1. åŒæ­¥ institutional_investors
            cur.execute("SELECT COUNT(*) FROM institutional_investors")
            total = cur.fetchone()[0]
            
            if total > 0:
                print_flush(f"æ­£åœ¨åŒæ­¥æ³•äººè³‡æ–™ ({total} ç­†)...")
                BATCH_SIZE = 1000
                total_batches = math.ceil(total / BATCH_SIZE)
                
                cur.execute("SELECT * FROM institutional_investors")
                
                success_count = 0
                for i in range(total_batches):
                    rows = cur.fetchmany(BATCH_SIZE)
                    if not rows: break
                    
                    data = [dict(row) for row in rows]
                    try:
                        supabase.table("institutional_investors").upsert(data).execute()
                        success_count += len(data)
                        if (i+1) % 10 == 0:
                            print(f"\r  é€²åº¦: {i+1}/{total_batches}", end="")
                    except Exception as e:
                        if "Could not find the table" in str(e):
                            print_flush(f"\nâŒ éŒ¯èª¤: è¡¨æ ¼ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œ update_supabase.sql")
                            return
                        # å…¶ä»–éŒ¯èª¤å¿½ç•¥ï¼Œç¹¼çºŒä¸‹ä¸€æ‰¹
                        pass
                print_flush(f"\nâœ“ æ³•äººè³‡æ–™åŒæ­¥å®Œæˆ ({success_count}/{total})")
            else:
                print_flush("æ³•äººè³‡æ–™ç‚ºç©ºï¼Œè·³é")

            # 2. åŒæ­¥ stock_history (å¯é¸ï¼Œå› ç‚ºè³‡æ–™é‡å¤ªå¤§ï¼Œé€™è£¡å…ˆåªåŒæ­¥æ³•äºº)
            # è‹¥è¦åŒæ­¥ stock_historyï¼Œå»ºè­°åªåŒæ­¥æœ€è¿‘ N å¤©
            
    except Exception as e:
        print_flush(f"âŒ åŒæ­¥å¤±æ•—: {e}")



def _build_history_query(limit_days=None):
    """å»ºæ§‹æ­·å²è³‡æ–™æŸ¥è©¢èªå¥ (Extract Method)"""
    if limit_days:
        return """
            SELECT * FROM (
                SELECT 
                    CAST(date_int/10000 AS TEXT) || '-' || 
                    SUBSTR('0'||CAST((date_int/100)%100 AS TEXT),-2) || '-' ||
                    SUBSTR('0'||CAST(date_int%100 AS TEXT),-2) as date,
                    open, high, low, close, volume, amount
                FROM stock_history 
                WHERE code = ? 
                ORDER BY date_int DESC
                LIMIT ?
            ) ORDER BY date ASC
        """
    else:
        return """
            SELECT 
                CAST(date_int/10000 AS TEXT) || '-' || 
                SUBSTR('0'||CAST((date_int/100)%100 AS TEXT),-2) || '-' ||
                SUBSTR('0'||CAST(date_int%100 AS TEXT),-2) as date,
                open, high, low, close, volume, amount
            FROM stock_history 
            WHERE code = ? 
            ORDER BY date_int ASC
        """

def calculate_stock_history_indicators(code, display_days=30, limit_days=None, conn=None, preloaded_df=None):
    """è¨ˆç®—è‚¡ç¥¨æ­·å²æŠ€è¡“æŒ‡æ¨™"""
    try:
        # ç²å–é›†ä¿äººæ•¸
        total_shareholders = 0
        try:
            if conn:
                cur = conn.cursor()
                cur.execute("SELECT total_shareholders FROM stock_snapshot WHERE code = ?", (code,))
                res = cur.fetchone()
                if res and res[0]: total_shareholders = res[0]
            else:
                with db_manager.get_connection() as tmp_conn:
                    cur = tmp_conn.cursor()
                    cur.execute("SELECT total_shareholders FROM stock_snapshot WHERE code = ?", (code,))
                    res = cur.fetchone()
                    if res and res[0]: total_shareholders = res[0]
        except:
            pass

        # å…§éƒ¨å‡½æ•¸: åŸ·è¡ŒæŸ¥è©¢ (æ–°ä¸‰è¡¨æ¶æ§‹)
        def execute_query(connection):
            query = _build_history_query(limit_days)
            
            # åƒæ•¸è™•ç†
            params = [code]
            if limit_days:
                params.append(limit_days + 250) # å¤šæŠ“ä¸€äº›ä»¥è¨ˆç®— MA200
                
            df = pd.read_sql_query(query, connection, params=params)
            return df

        t_start = time.time()
        
        if preloaded_df is not None:
            df = preloaded_df.copy()
            # å¦‚æœæœ‰ limit_daysï¼Œæˆªå–æœ€å¾Œ N ç­†
            if limit_days and len(df) > limit_days:
                df = df.iloc[-limit_days:].reset_index(drop=True)
        elif conn:
            df = execute_query(conn)
        else:
            with db_manager.get_connection() as new_conn:
                df = execute_query(new_conn)
        
        if df.empty or len(df) < 20:
            return None
        
        # ç¢ºä¿æ—¥æœŸæ ¼å¼æ­£ç¢º
        df['date'] = pd.to_datetime(df['date'])
        df = df.sort_values('date').reset_index(drop=True)
        
        # è¨ˆç®—æŒ‡æ¨™
        df['MA3'] = df['close'].rolling(3).mean().round(2)
        df['MA20'] = df['close'].rolling(20).mean().round(2)
        df['MA60'] = df['close'].rolling(60).mean().round(2)
        df['MA120'] = df['close'].rolling(120).mean().round(2)
        df['MA200'] = df['close'].rolling(200).mean().round(2)
        
        # æˆäº¤é‡å‡ç·š
        df['Vol_MA3'] = df['volume'].rolling(3).mean().round(2)
        
        df['WMA3'] = pd.Series(IndicatorCalculator.calculate_wma(df['close'].values, 3), index=df.index).round(2)
        df['WMA20'] = pd.Series(IndicatorCalculator.calculate_wma(df['close'].values, 20), index=df.index).round(2)
        df['WMA60'] = pd.Series(IndicatorCalculator.calculate_wma(df['close'].values, 60), index=df.index).round(2)
        df['WMA120'] = pd.Series(IndicatorCalculator.calculate_wma(df['close'].values, 120), index=df.index).round(2)
        df['WMA200'] = pd.Series(IndicatorCalculator.calculate_wma(df['close'].values, 200), index=df.index).round(2)
        
        df['MFI'] = IndicatorCalculator.calculate_mfi(df, 14).round(2)
        df['VWAP'] = IndicatorCalculator.calculate_vwap_series(df, lookback=20).round(2)
        df['CHG14'] = IndicatorCalculator.calculate_chg14_series(df).round(2)
        df['RSI'] = IndicatorCalculator.calculate_rsi_series(df, 14).round(2)
        
        macd, signal = IndicatorCalculator.calculate_macd_series(df)
        df['MACD'] = macd.round(2)
        df['SIGNAL'] = signal.round(2)
        
        # [New] Six-Dim Resonance Indicators
        # 1. BBI (Bullish Bearish Indicator) - (MA3 + MA6 + MA12 + MA24) / 4
        # Using existing MAs or calculating new ones if needed. Standard BBI uses 3, 6, 12, 24.
        # We have MA3, MA20, MA60, MA120. Let's calculate specific ones for BBI.
        ma3 = df['close'].rolling(3).mean()
        ma6 = df['close'].rolling(6).mean()
        ma12 = df['close'].rolling(12).mean()
        ma24 = df['close'].rolling(24).mean()
        df['BBI'] = ((ma3 + ma6 + ma12 + ma24) / 4).round(2)

        # 2. MTM (Momentum) - Close - Close(N), usually N=12
        df['MTM'] = (df['close'] - df['close'].shift(12)).round(2)
        df['MTM_MA'] = df['MTM'].rolling(6).mean().round(2) # MTM Signal line

        # 3. LWR (Williams %R) - usually 9 days
        # Formula: (Highest High - Close) / (Highest High - Lowest Low) * -100
        low_min = df['low'].rolling(9).min()
        high_max = df['high'].rolling(9).max()
        df['LWR'] = (((high_max - df['close']) / (high_max - low_min)) * -100).round(2)
        
        k_series, d_series = IndicatorCalculator.calculate_monthly_kd_series(df)
        daily_k, daily_d = IndicatorCalculator.calculate_daily_kd_series(df)
        week_k, week_d = IndicatorCalculator.calculate_weekly_kd_series(df)
        
        smart_score, smi_sig, nvi_sig, vsa_sig, svi_sig, vol_div_sig, weekly_nvi_sig = IndicatorCalculator.calculate_smart_score_series(df)
        
        # è¨ˆç®—ä¸¦å„²å­˜åŸå§‹æ•¸å€¼
        df['SMI'] = IndicatorCalculator.calculate_smi_series(df).round(2)
        nvi, _ = IndicatorCalculator.calculate_nvi_series(df)
        df['NVI'] = nvi.round(2)
        
        # [Restored] SVI, RSI, MACD
        df['SVI'] = ((df['close'] - df['MA200']) / df['MA200'] * 100).round(2)
        
        # [Added] ADL, RS
        df['ADL'] = IndicatorCalculator.calculate_adl_series(df).round(2)
        df['RS'] = IndicatorCalculator.calculate_rs_series(df).round(2)
        
        df['Smart_Score'] = smart_score
        df['SMI_Signal'] = smi_sig
        df['NVI_Signal'] = nvi_sig
        df['VSA_Signal'] = vsa_sig
        df['SVI_Signal'] = svi_sig
        df['Vol_Div_Signal'] = vol_div_sig
        df['Weekly_NVI_Signal'] = weekly_nvi_sig
        
        df['PVI'] = IndicatorCalculator.calculate_pvi_series(df).round(2)
        
        # [Fix] è£œä¸Šç¼ºå¤±çš„ CLV è¨ˆç®—
        df['clv'] = IndicatorCalculator.calculate_clv_series(df).round(2)
        
        # [Fix] è£œä¸Šç¼ºå¤±çš„ 3æ—¥èƒŒé›¢è¨Šè™Ÿè¨ˆç®—
        div_bull, div_bear = IndicatorCalculator.calculate_3day_divergence_series(df)
        df['div_3day_bull'] = div_bull
        df['div_3day_bear'] = div_bear
        
        df['Month_K'] = k_series.round(2)
        df['Month_D'] = d_series.round(2)
        df['Daily_K'] = daily_k.round(2)
        df['Daily_D'] = daily_d.round(2)
        df['Week_K'] = pd.Series(week_k, index=df.index).round(2)
        df['Week_D'] = pd.Series(week_d, index=df.index).round(2)
        
        df['close_prev'] = df['close'].shift(1)
        df['vol_prev'] = df['volume'].shift(1)
        
        # [New] VWAP 60
        df['VWAP60'] = IndicatorCalculator.calculate_vwap_series(df, lookback=60).round(2)
        
        # [New] BBW (Bollinger Band Width)
        # Using simple calculation here as IndicatorCalculator might not have a dedicated series method for BBW
        
        # [New] VSBC Bands
        vsbc_u, vsbc_l = IndicatorCalculator.calculate_vsbc_bands(df)
        df['VSBC_Upper'] = vsbc_u.round(2)
        df['VSBC_Lower'] = vsbc_l.round(2)
        ma20_for_bb = df['close'].rolling(20).mean()
        std20_for_bb = df['close'].rolling(20).std()
        upper_bb = ma20_for_bb + 2 * std20_for_bb
        lower_bb = ma20_for_bb - 2 * std20_for_bb
        df['BBW'] = ((upper_bb - lower_bb) / ma20_for_bb).round(4)
        
        # [New] Fibonacci 0.618 (Recent 60 days)
        # We need a rolling calculation for this to be correct for each day in history
        # For efficiency, we can use rolling max/min
        roll_high_60 = df['high'].rolling(60).max()
        roll_low_60 = df['low'].rolling(60).min()
        diff_60 = roll_high_60 - roll_low_60
        df['Fib_0618'] = (roll_high_60 - (diff_60 * 0.618)).round(2)
        
        # [New] VWAP 200
        df['VWAP200'] = IndicatorCalculator.calculate_vwap_series(df, lookback=200).round(2)
        
        # [New] Weekly/Monthly Data (Resampled)
        # Note: This is computationally expensive, so we do it only if needed or optimize it
        # Here we use a simplified approach by taking the last available weekly/monthly data
        # For a proper implementation, we should resample the whole series and reindex
        
        # Weekly
        df['date_idx'] = df['date']
        df.set_index('date_idx', inplace=True)
        
        weekly_df = df.resample('W').agg({'open': 'first', 'close': 'last'})
        monthly_df = df.resample('M').agg({'open': 'first', 'close': 'last'})
        
        # Reindex back to daily to fill values
        df['weekly_open'] = weekly_df['open'].reindex(df.index, method='ffill')
        df['weekly_close'] = weekly_df['close'].reindex(df.index, method='ffill')
        df['monthly_open'] = monthly_df['open'].reindex(df.index, method='ffill')
        df['monthly_close'] = monthly_df['close'].reindex(df.index, method='ffill')
        
        df.reset_index(drop=True, inplace=True)
        
        # [New] Mansfield RS (Simplified Relative Strength Score)
        # Since we don't have a reliable market index in this context efficiently, 
        # we use the RS score we already calculated (0-100) as a proxy for now.
        # Or we can implement a self-relative strength if needed.
        # For now, we map the existing RS to this field to ensure data availability.
        df['Mansfield_RS'] = df['RS'] 
        
        # æº–å‚™çµæœåˆ—è¡¨
        indicators_list = []
        start_index = 0 if not display_days else max(0, len(df) - display_days)
        
        for i in range(start_index, len(df)):
            row = df.iloc[i]
            prev_row = df.iloc[i-1] if i > 0 else row
            
            indicators = {
                'date': row['date'].strftime('%Y-%m-%d'),
                'open': row['open'],
                'high': row['high'],
                'low': row['low'],
                'close': row['close'],
                'volume': row['volume'],
                'close_prev': row['close_prev'] if pd.notnull(row['close_prev']) else None,
                'vol_prev': row['vol_prev'] if pd.notnull(row['vol_prev']) else None,
                'Vol_MA3': row['Vol_MA3'],
                'MA3': row['MA3'],
                'MA20': row['MA20'],
                'MA60': row['MA60'],
                'MA120': row['MA120'],
                'MA200': row['MA200'],
                'WMA3': row['WMA3'],
                'WMA20': row['WMA20'],
                'WMA60': row['WMA60'],
                'WMA120': row['WMA120'],
                'WMA200': row['WMA200'],
                'MA3_prev': prev_row['MA3'],
                'MA20_prev': prev_row['MA20'],
                'MA60_prev': prev_row['MA60'],
                'MA120_prev': prev_row['MA120'],
                'MA200_prev': prev_row['MA200'],
                'WMA3_prev': prev_row['WMA3'],
                'WMA20_prev': prev_row['WMA20'],
                'WMA60_prev': prev_row['WMA60'],
                'WMA120_prev': prev_row['WMA120'],
                'WMA200_prev': prev_row['WMA200'],
                'MFI': row['MFI'],
                'MFI_prev': prev_row['MFI'],
                'VWAP': row['VWAP'],
                'VWAP_prev': prev_row['VWAP'],
                'CHG14': row['CHG14'],
                'CHG14_prev': prev_row['CHG14'],
                'RSI': row['RSI'],
                'MACD': row['MACD'],
                'SIGNAL': row['SIGNAL'],
                'Month_K': row['Month_K'],
                'Month_D': row['Month_D'],
                'Daily_K': row['Daily_K'] if pd.notnull(row['Daily_K']) else None,
                'Daily_D': row['Daily_D'] if pd.notnull(row['Daily_D']) else None,
                'Week_K': row['Week_K'] if pd.notnull(row['Week_K']) else None,
                'Week_D': row['Week_D'] if pd.notnull(row['Week_D']) else None,
                'Month_K_prev': prev_row['Month_K'],
                'Month_D_prev': prev_row['Month_D'],
                'Daily_K_prev': prev_row['Daily_K'],
                'Daily_D_prev': prev_row['Daily_D'],
                'Week_K_prev': prev_row['Week_K'],
                'Week_D_prev': prev_row['Week_D'],
                'SMI': row['SMI'],
                'SVI': row['SVI'],
                'NVI': row['NVI'],
                'Smart_Score': int(row['Smart_Score']) if pd.notnull(row['Smart_Score']) else None,
                'SMI_Signal': int(row['SMI_Signal']) if pd.notnull(row['SMI_Signal']) else None,
                'NVI_Signal': int(row['NVI_Signal']) if pd.notnull(row['NVI_Signal']) else None,
                'VSA_Signal': int(row['VSA_Signal']) if pd.notnull(row['VSA_Signal']) else None,
                'SVI_Signal': int(row['SVI_Signal']) if pd.notnull(row['SVI_Signal']) else None,
                'SMI_Signal_prev': int(prev_row['SMI_Signal']) if pd.notnull(prev_row['SMI_Signal']) else None,
                'NVI_Signal_prev': int(prev_row['NVI_Signal']) if pd.notnull(prev_row['NVI_Signal']) else None,
                'SVI_Signal_prev': int(prev_row['SVI_Signal']) if pd.notnull(prev_row['SVI_Signal']) else None,
                'Smart_Score_prev': int(prev_row['Smart_Score']) if pd.notnull(prev_row['Smart_Score']) else None,
                'Smart_Score_prev': int(prev_row['Smart_Score']) if pd.notnull(prev_row['Smart_Score']) else None,
                'PVI': float(row['PVI']) if pd.notnull(row['PVI']) else None,
                'pvi_prev': float(prev_row['PVI']) if pd.notnull(prev_row['PVI']) else None, # [Fix] Add pvi_prev
                'clv': float(row['clv']) if pd.notnull(row.get('clv')) else None, # [Fix] åŠ å…¥ CLV
                'Vol_Div_Signal': int(row['Vol_Div_Signal']) if pd.notnull(row['Vol_Div_Signal']) else None,
                'Weekly_NVI_Signal': int(row['Weekly_NVI_Signal']) if pd.notnull(row['Weekly_NVI_Signal']) else None,
                'Div_3Day_Bull': int(row['div_3day_bull']) if pd.notnull(row.get('div_3day_bull')) else None,
                'Div_3Day_Bear': int(row['div_3day_bear']) if pd.notnull(row.get('div_3day_bear')) else None,
                'VWAP60': row['VWAP60'],
                'BBW': row['BBW'],
                'Fib_0618': row['Fib_0618'],
                'VWAP200': row['VWAP200'],
                'Weekly_Close': row['weekly_close'] if pd.notnull(row['weekly_close']) else None,
                'Weekly_Open': row['weekly_open'] if pd.notnull(row['weekly_open']) else None,
                'Monthly_Close': row['monthly_close'] if pd.notnull(row['monthly_close']) else None,
                'Monthly_Open': row['monthly_open'] if pd.notnull(row['monthly_open']) else None,
                'Mansfield_RS': row['Mansfield_RS'],
                'ADL': float(row['ADL']) if pd.notnull(row['ADL']) else None,
                'RS': float(row['RS']) if pd.notnull(row['RS']) else None,
            }
            
            current_window = df.iloc[max(0, i-19):i+1]
            vp = IndicatorCalculator.calculate_vp_scheme3(current_window, lookback=20)
            
            indicators['POC'] = vp['POC']
            indicators['VP_upper'] = vp['VP_upper']
            indicators['VP_lower'] = vp['VP_lower']
            
            # VSBC Bands
            indicators['VSBC_Upper'] = row['VSBC_Upper']
            indicators['VSBC_Lower'] = row['VSBC_Lower']
            
            # é›†ä¿äººæ•¸
            indicators['Total_Shareholders'] = total_shareholders
            
            indicators_list.append(indicators)
        
        return indicators_list[::-1]
        
    except Exception as e:
        # Log error for debugging purposes
        # logger.debug(f"Error in calculate_stock_history_indicators: {e}")
        return None

def process_single_stock_calculation(code, name, preloaded_df, conn):
    """
    è™•ç†å–®ä¸€è‚¡ç¥¨çš„æŒ‡æ¨™è¨ˆç®— (æå–è‡ª step7)
    :return: update_tuple (for executemany) or None
    """
    try:
        # è¨ˆç®—æŒ‡æ¨™ (ä½¿ç”¨é è¼‰å…¥çš„ DataFrame)
        # æ³¨æ„: é€™è£¡æˆ‘å€‘åªéœ€è¦æœ€æ–°çš„ä¸€ç­†ä¾†æ›´æ–° snapshotï¼Œæ‰€ä»¥ display_days=1 å³å¯
        # ä½†ç‚ºäº†è¨ˆç®— MA200 ç­‰é•·å¤©æœŸæŒ‡æ¨™ï¼Œlimit_days ä»éœ€è¶³å¤ é•· (ç”± Config æ§åˆ¶)
        indicators_list = calculate_stock_history_indicators(
            code, 
            display_days=1, 
            limit_days=Config.CALC_LOOKBACK_DAYS, 
            conn=conn, 
            preloaded_df=preloaded_df
        )
        
        if not indicators_list:
            return None
            
        # å–å¾—æœ€æ–°ä¸€ç­†è³‡æ–™ (calculate_stock_history_indicators å›å‚³çš„æ˜¯å€’åº listï¼Œæœ€æ–°åœ¨ index 0)
        latest = indicators_list[0]
        
        # å»ºæ§‹æ›´æ–° Tuple (å¿…é ˆèˆ‡ step7 çš„ SQL UPDATE é †åºå®Œå…¨ä¸€è‡´)
        return (
            latest.get('MA3'), latest.get('MA20'), latest.get('MA60'), latest.get('MA120'), latest.get('MA200'),
            latest.get('WMA3'), latest.get('WMA20'), latest.get('WMA60'), latest.get('WMA120'), latest.get('WMA200'),
            latest.get('MFI'), latest.get('VWAP'), latest.get('CHG14'), latest.get('RSI'), latest.get('MACD'), latest.get('SIGNAL'),
            latest.get('POC'), latest.get('VP_upper'), latest.get('VP_lower'),
            latest.get('Month_K'), latest.get('Month_D'),
            latest.get('Daily_K'), latest.get('Daily_D'),
            latest.get('Week_K'), latest.get('Week_D'),
            latest.get('MA3_prev'), latest.get('MA20_prev'), latest.get('MA60_prev'), latest.get('MA120_prev'), latest.get('MA200_prev'),
            latest.get('WMA3_prev'), latest.get('WMA20_prev'), latest.get('WMA60_prev'), latest.get('WMA120_prev'), latest.get('WMA200_prev'),
            latest.get('MFI_prev'), latest.get('VWAP_prev'), latest.get('CHG14_prev'),
            latest.get('Month_K_prev'), latest.get('Month_D_prev'),
            latest.get('Daily_K_prev'), latest.get('Daily_D_prev'),
            latest.get('Week_K_prev'), latest.get('Week_D_prev'),
            latest.get('close_prev'), latest.get('vol_prev'),
            latest.get('SMI'), latest.get('SVI'), latest.get('NVI'), latest.get('PVI'), latest.get('clv'),
            latest.get('Smart_Score'), latest.get('SMI_Signal'), latest.get('SVI_Signal'), latest.get('NVI_Signal'), latest.get('VSA_Signal'),
            latest.get('SMI_Signal_prev'), latest.get('SVI_Signal_prev'), latest.get('NVI_Signal_prev'), latest.get('Smart_Score_prev'),
            latest.get('Vol_Div_Signal'), latest.get('Weekly_NVI_Signal'),
            latest.get('Div_3Day_Bull'), latest.get('Div_3Day_Bear'),
            latest.get('Vol_MA3'), latest.get('pvi_prev'),
            latest.get('VWAP60'), latest.get('BBW'), latest.get('Fib_0618'),
            latest.get('Weekly_Close'), latest.get('Weekly_Open'),
            latest.get('Monthly_Close'), latest.get('Monthly_Open'),
            latest.get('VWAP200'), latest.get('Mansfield_RS'),
            latest.get('ADL'), latest.get('RS'),
            code # WHERE code=?
        )
    except Exception as e:
        # ç™¼ç”ŸéŒ¯èª¤æ™‚å›å‚³ Noneï¼Œé¿å…ä¸­æ–·æ‰¹æ¬¡è™•ç†
        return None


def step7_calc_indicators(data=None, force=False, batch_size=500):
    """[Step 7] è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ (å¤šé€²ç¨‹ä¸¦è¡Œç‰ˆ)"""
    from multiprocessing import Pool
    
    print_flush("\n[Step 7] è¨ˆç®—æŠ€è¡“æŒ‡æ¨™ (å¤šé€²ç¨‹åŠ é€Ÿ)...")
    
    if data is None:
        data = step4_load_data()
    
    if not data:
        print_flush("âŒ ç„¡è‚¡ç¥¨è³‡æ–™å¯è¨ˆç®—")
        return {}
    
    stocks = [(code, info['name']) for code, info in data.items()]
    total = len(stocks)
    
    if total == 0:
        print_flush("âŒ ç„¡è‚¡ç¥¨éœ€è¦è¨ˆç®—æŒ‡æ¨™")
        return {}
    
    # è®€å–é€²åº¦
    progress = load_progress()
    start_idx = 0
    if not force and progress.get('calc_last_idx', 0) > 0:
        start_idx = progress['calc_last_idx']
        print_flush(f"âš¡ åµæ¸¬åˆ°ä¸Šæ¬¡é€²åº¦ï¼Œå¾ç¬¬ {start_idx+1} ç­†ç¹¼çºŒè¨ˆç®—...")
    
    tracker = ProgressTracker(total_lines=3)
    start_time = time.time()
    
    # ä½¿ç”¨ CPU æ ¸å¿ƒæ•¸ (ä¿ç•™ 1 æ ¸çµ¦ UI)
    num_processes = max(1, os.cpu_count() - 1)
    print_flush(f"å•Ÿå‹• {num_processes} å€‹é€²ç¨‹ä¸¦è¡Œè¨ˆç®—...")
    
    with tracker:
        for batch_start in range(start_idx, total, batch_size):
            batch_end = min(batch_start + batch_size, total)
            batch_stocks = stocks[batch_start:batch_end]
            
            pending_updates = []
            
            with db_manager.get_connection() as conn:
                conn.execute("PRAGMA synchronous = OFF;")
                cur = conn.cursor()
                
                # 1. æ‰¹æ¬¡è¼‰å…¥æ­·å²è³‡æ–™ (å–®ç·šç¨‹ I/O)
                batch_codes = [s[0] for s in batch_stocks]
                history_map = batch_load_history(batch_codes, limit_days=Config.CALC_LOOKBACK_DAYS, conn=conn)
                
                # 2. æº–å‚™ä¸¦è¡Œä»»å‹™
                tasks = []
                for code, name in batch_stocks:
                    tasks.append((code, name, history_map.get(code)))
                
                # 3. å¤šé€²ç¨‹ä¸¦è¡Œè¨ˆç®— (CPU Bound)
                # ä½¿ç”¨ imap ä¿æŒé †åºä¸¦æ›´æ–°é€²åº¦
                with Pool(processes=num_processes) as pool:
                    for i, res in enumerate(pool.imap(_worker_calc_indicators, tasks, chunksize=20)):
                        current_idx = batch_start + i
                        
                        if res:
                            pending_updates.append(res)
                        
                        # æ›´æ–°é€²åº¦é¡¯ç¤º (æ¯ 10 ç­†æˆ–æœ€å¾Œä¸€ç­†)
                        if i % 10 == 0 or i == len(batch_stocks) - 1:
                            elapsed = time.time() - start_time
                            processed = current_idx - start_idx + 1
                            avg_speed = processed / elapsed if elapsed > 0 else 0
                            remaining = (total - current_idx - 1) / avg_speed if avg_speed > 0 else 0
                            
                            tracker.update_lines(
                                f'æ­£åœ¨è¨ˆç®—: {batch_stocks[i][0]} {batch_stocks[i][1]}',
                                f'é€²åº¦: {current_idx+1}/{total} (æ‰¹æ¬¡: {batch_start//batch_size + 1})',
                                f'é€Ÿåº¦: {avg_speed:.1f} æª”/ç§’ | é ä¼°å‰©é¤˜: {int(remaining/60)}åˆ†{int(remaining%60)}ç§’'
                            )
                
                # 4. æ‰¹æ¬¡å¯«å…¥ (å–®ç·šç¨‹ I/O)
                if pending_updates:
                    try:
                        cur.executemany("""
                            UPDATE stock_snapshot SET
                                ma3=?, ma20=?, ma60=?, ma120=?, ma200=?,
                                wma3=?, wma20=?, wma60=?, wma120=?, wma200=?,
                                mfi14=?, vwap20=?, chg14_pct=?, rsi=?, macd=?, signal=?,
                                vp_poc=?, vp_upper=?, vp_lower=?,
                                month_k=?, month_d=?,
                                daily_k=?, daily_d=?,
                                week_k=?, week_d=?,
                                ma3_prev=?, ma20_prev=?, ma60_prev=?, ma120_prev=?, ma200_prev=?,
                                wma3_prev=?, wma20_prev=?, wma60_prev=?, wma120_prev=?, wma200_prev=?,
                                mfi14_prev=?, vwap20_prev=?, chg14_pct_prev=?,
                                month_k_prev=?, month_d_prev=?,
                                daily_k_prev=?, daily_d_prev=?,
                                week_k_prev=?, week_d_prev=?,
                                close_prev=?, vol_prev=?,
                                smi=?, svi=?, nvi=?, pvi=?, clv=?,
                                smart_score=?, smi_signal=?, svi_signal=?, nvi_signal=?, vsa_signal=?,
                                smi_prev=?, svi_prev=?, nvi_prev=?, smart_score_prev=?,
                                vol_div_signal=?, weekly_nvi_signal=?,
                                div_3day_bull=?, div_3day_bear=?,
                                vol_ma3=?, pvi_prev=?,
                                vwap60=?, bbw=?, fib_0618=?,
                                weekly_close=?, weekly_open=?,
                                monthly_close=?, monthly_open=?,
                                vwap200=?, mansfield_rs=?,
                                adl=?, rs=?
                            WHERE code=?
                        """, pending_updates)
                        conn.commit()
                        save_progress(batch_end - 1)
                    except Exception as e:
                        tracker.update_lines(f"å¯«å…¥éŒ¯èª¤: {e}", "", "")
                        time.sleep(1)

    print_flush(f"\n[Step 7] è¨ˆç®—å®Œæˆ! ç¸½è€—æ™‚: {int(time.time() - start_time)} ç§’")
    clear_progress()
    return data


def scan_mfi_mode(indicators_data, order='asc', min_volume=0):
    """MFIæƒæ (ä¸¦è¡Œç‰ˆ)"""
    
    def filter_func(code, ind):
        mfi = safe_num(ind.get('mfi14') or ind.get('MFI'))
        mfi_prev = safe_num(ind.get('mfi14_prev') or ind.get('MFI_prev'))
        if mfi is None or mfi_prev is None:
            return False
        if order == 'asc':
            return mfi > mfi_prev and mfi < 30
        else:
            return mfi < mfi_prev and mfi > 70
    
    def transform_func(code, ind):
        mfi = safe_num(ind.get('mfi14') or ind.get('MFI'))
        return (code, mfi, ind)
    
    return scan_with_parallel(
        indicators_data,
        filter_func,
        transform_func,
        sort_key=lambda x: x[1],
        reverse=(order == 'desc'),
        min_volume=min_volume
    )


# ==============================
# Phase 4: çµ±ä¸€æƒæè¼¸å‡ºæ ¼å¼åŒ–
# ==============================

def format_volume_ratio(volume, vol_prev=None):
    """æ ¼å¼åŒ–æˆäº¤é‡èˆ‡é‡èƒ½æ¯”"""
    if volume is None:
        return "-"
    vol = safe_num(volume)
    if vol is None:
        return "-"
    if vol_prev and safe_num(vol_prev) and safe_num(vol_prev) > 0:
        ratio = vol / safe_num(vol_prev)
        return f"{vol:,.0f}({ratio:.2f}x)"
    return f"{vol:,.0f}"

def format_vsbc(ind):
    """æ ¼å¼åŒ– VSBC ä¸Š/ä¸‹"""
    upper = safe_num(ind.get('vsbc_upper') or ind.get('VSBC_Upper'))
    lower = safe_num(ind.get('vsbc_lower') or ind.get('VSBC_Lower'))
    if upper is None or lower is None:
        return "-/-"
    return f"{upper:.0f}/{lower:.0f}"

def format_vp(ind):
    """æ ¼å¼åŒ– VP ä¸Š/ä¸‹"""
    upper = safe_num(ind.get('vp_upper') or ind.get('VP_Upper'))
    lower = safe_num(ind.get('vp_lower') or ind.get('VP_Lower'))
    if upper is None or lower is None:
        return "-/-"
    return f"{upper:.0f}/{lower:.0f}"

def format_scan_row(code, ind, extra_cols=None):
    """
    æ ¼å¼åŒ–æƒæçµæœå–®è¡Œè¼¸å‡º
    
    çµ±ä¸€æ ¼å¼: ä»£è™Ÿ | åç¨± | æ”¶ç›¤ | æˆäº¤é‡(é‡èƒ½æ¯”) | VSBCä¸Š/ä¸‹ | VPä¸Š/ä¸‹ | [é¡å¤–æ¬„ä½]
    """
    name = ind.get('name', '')[:8]  # æœ€å¤š8å­—å…ƒ
    close = safe_num(ind.get('close'))
    vol = safe_num(ind.get('volume'))
    vol_prev = safe_num(ind.get('vol_prev') or ind.get('volume_prev'))
    
    close_str = f"{close:.2f}" if close else "-"
    vol_str = format_volume_ratio(vol, vol_prev)
    vsbc_str = format_vsbc(ind)
    vp_str = format_vp(ind)
    
    base = f"{code:<6} {name:<10} {close_str:>10} {vol_str:>18} {vsbc_str:>12} {vp_str:>12}"
    
    if extra_cols:
        extra = " ".join(str(c) for c in extra_cols)
        return f"{base} {extra}"
    return base

def print_scan_header(extra_headers=None):
    """å°å‡ºæƒæçµæœè¡¨é ­"""
    base = f"{'ä»£è™Ÿ':<6} {'åç¨±':<10} {'æ”¶ç›¤':>10} {'æˆäº¤é‡(é‡èƒ½æ¯”)':>18} {'VSBCä¸Š/ä¸‹':>12} {'VPä¸Š/ä¸‹':>12}"
    if extra_headers:
        extra = " ".join(extra_headers)
        print_flush(f"{base} {extra}")
    else:
        print_flush(base)
    print_flush("-" * 80)

def print_scan_results(results, title, limit=30, description="", extra_headers=None, extra_func=None):
    """
    çµ±ä¸€æƒæçµæœè¼¸å‡ºå‡½æ•¸
    
    :param results: list of (code, sort_val, ind) æˆ– (code, sort_val, ind, extra_data)
    :param title: æ¨™é¡Œ
    :param limit: é¡¯ç¤ºé™åˆ¶
    :param description: èªªæ˜æ–‡å­—
    :param extra_headers: é¡å¤–æ¬„ä½æ¨™é ­ list
    :param extra_func: é¡å¤–æ¬„ä½ç”¢ç”Ÿå‡½æ•¸ (code, sort_val, ind) -> list
    :return: list of codes
    """
    print_flush(f"\nã€{title}ã€‘ (å‰ {min(len(results), limit)} ç­†)")
    print_scan_header(extra_headers)
    
    codes = []
    for i, item in enumerate(results[:limit]):
        code = item[0]
        sort_val = item[1]
        ind = item[2]
        codes.append(code)
        
        extra_cols = None
        if extra_func:
            extra_cols = extra_func(code, sort_val, ind)
        elif len(item) > 3:
            extra_cols = [item[3]] if not isinstance(item[3], list) else item[3]
        
        print_flush(format_scan_row(code, ind, extra_cols))
    
    print_flush("-" * 80)
    if description:
        print_flush(description)
    print_flush(f"âœ“ æƒæå®Œæˆï¼Œå…±æ‰¾åˆ° {len(results)} æª”ç¬¦åˆæ¢ä»¶")
    
    return codes

# ==============================
# Phase 4: é«˜éšå‡½æ•¸ - é€šç”¨æƒææ¨¡æ¿
# ==============================
def scan_with_filter(indicators_data, filter_func, transform_func, sort_key, reverse=False, min_volume=0):
    """
    é€šç”¨æƒæå‡½æ•¸æ¨¡æ¿ (é«˜éšå‡½æ•¸æ¨¡å¼)
    
    :param indicators_data: æŒ‡æ¨™æ•¸æ“šå­—å…¸
    :param filter_func: éæ¿¾å‡½æ•¸ (code, ind) -> bool
    :param transform_func: è½‰æ›å‡½æ•¸ (code, ind) -> result_dict
    :param sort_key: æ’åºéµ
    :param reverse: æ˜¯å¦é™åº
    :param min_volume: æœ€å°æˆäº¤é‡
    """
    def volume_filter(item):
        code, ind = item
        if not ind:
            return False
        vol = safe_num(ind.get('volume', 0))
        return vol is not None and vol >= min_volume
    
    def combined_filter(item):
        return volume_filter(item) and filter_func(item[0], item[1])
    
    # ä½¿ç”¨é«˜éšå‡½æ•¸éˆå¼è™•ç†
    filtered = filter(combined_filter, indicators_data.items())
    transformed = map(lambda x: transform_func(x[0], x[1]), filtered)
    return sorted(transformed, key=sort_key, reverse=reverse)


def _scan_worker(args):
    """æƒæå·¥ä½œé€²ç¨‹ (ç”¨æ–¼å¤šé€²ç¨‹)"""
    code, ind, filter_func, transform_func, min_volume = args
    try:
        if not ind:
            return None
        vol = safe_num(ind.get('volume', 0))
        if vol is None or vol < min_volume:
            return None
        if not filter_func(code, ind):
            return None
        return transform_func(code, ind)
    except:
        return None


def scan_with_parallel(indicators_data, filter_func, transform_func, sort_key, 
                       reverse=False, min_volume=0, use_parallel=True, num_workers=None):
    """
    ä¸¦è¡Œæƒæå‡½æ•¸æ¨¡æ¿ (å¤šé€²ç¨‹ç‰ˆ)
    
    :param indicators_data: æŒ‡æ¨™æ•¸æ“šå­—å…¸
    :param filter_func: éæ¿¾å‡½æ•¸ (code, ind) -> bool
    :param transform_func: è½‰æ›å‡½æ•¸ (code, ind) -> result_dict
    :param sort_key: æ’åºéµ
    :param reverse: æ˜¯å¦é™åº
    :param min_volume: æœ€å°æˆäº¤é‡
    :param use_parallel: æ˜¯å¦ä½¿ç”¨ä¸¦è¡Œ (è³‡æ–™é‡ > 500 æ™‚è‡ªå‹•å•Ÿç”¨)
    :param num_workers: å·¥ä½œé€²ç¨‹æ•¸ (None = CPU æ ¸å¿ƒæ•¸ - 1)
    """
    items = list(indicators_data.items())
    
    # è³‡æ–™é‡å°æ™‚ä½¿ç”¨å–®ç·šç¨‹
    if len(items) < 500 or not use_parallel:
        return scan_with_filter(indicators_data, filter_func, transform_func, 
                                sort_key, reverse, min_volume)
    
    # æº–å‚™ä¸¦è¡Œä»»å‹™
    tasks = [(code, ind, filter_func, transform_func, min_volume) for code, ind in items]
    
    # ä½¿ç”¨å¤šé€²ç¨‹
    num_workers = num_workers or max(1, os.cpu_count() - 1)
    results = []
    
    with multiprocessing.Pool(processes=num_workers) as pool:
        for res in pool.imap_unordered(_scan_worker, tasks, chunksize=100):
            if res is not None:
                results.append(res)
    
    return sorted(results, key=sort_key, reverse=reverse)

def scan_ma_mode(indicators_data, ma_type='MA200', min_volume=0):
    """å‡ç·šæƒæ (é«˜éšå‡½æ•¸é‡æ§‹ç‰ˆ)"""
    ma_key = ma_type.lower()
    
    def filter_func(code, ind):
        close = safe_num(ind.get('close'))
        ma_val = safe_num(ind.get(ma_key) or ind.get(ma_type))
        if not (close and ma_val):
            return False
        diff_pct = (close - ma_val) / ma_val * 100
        return -10 <= diff_pct <= 0
    
    def transform_func(code, ind):
        close = safe_num(ind.get('close'))
        ma_val = safe_num(ind.get(ma_key) or ind.get(ma_type))
        diff_pct = (close - ma_val) / ma_val * 100
        return (code, diff_pct, ind)
    
    return scan_with_parallel(
        indicators_data,
        filter_func,
        transform_func,
        sort_key=lambda x: x[1],
        reverse=False,
        min_volume=min_volume
    )

def scan_smart_money_strategy():
    """è°æ˜éŒ¢æŒ‡æ¨™æƒæ (OpenSpec: Smart Score 0-5)"""
    # 1. è¨­å®šæƒæåƒæ•¸
    limit = get_display_limit(30)
    min_vol = get_volume_limit(100)  # é è¨­å¤§æ–¼100å¼µ
    
    # ä½¿ç”¨é è¨­åƒæ•¸ (ç„¡é˜»å¡ input)
    vol_mul = 1.1      # æˆäº¤é‡æ”¾å¤§å€æ•¸
    ma_key = 'MA200'   # å‡ç·šè¶¨å‹¢æª¢æŸ¥
    mfi_thr = 80.0     # MFI è¶…è²·é–¾å€¼

    print_flush(f"\næ­£åœ¨æƒæ è°æ˜éŒ¢æŒ‡æ¨™ (NVIä¸»åŠ›ç±Œç¢¼)...")
    print_flush(f"æ¢ä»¶: æˆäº¤é‡ > æ˜¨æ—¥x{vol_mul}, åƒ¹æ ¼ > {ma_key}, MFI < {mfi_thr}")
    
    results = []
    stats = {
        'total': 0,
        'vol_pass': 0,
        'has_score': 0,
        'smi_sig': 0,
        'svi_sig': 0,
        'nvi_sig': 0,
        'vsa_sig': 0,
        'vwap_sig': 0,
        'vol_div_sig': 0,
        'weekly_nvi_sig': 0,
        'score_3': 0,
        'score_4': 0,
        'score_5': 0,
        'score_6': 0
    }
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    
    if not data:
        print_flush("âŒ ç„¡æŒ‡æ¨™æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™æ›´æ–°")
        return

    stats['total'] = len(data)
    
    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol is None or vol < min_vol:
                continue
            
            # Volume Multiplier Filter
            vol_prev = safe_float_preserving_none(ind.get('vol_prev'))
            if vol_prev and vol < vol_prev * vol_mul:
                continue
                
            # MFI Filter
            mfi = safe_float_preserving_none(ind.get('mfi14') or ind.get('MFI'))
            if mfi and mfi > mfi_thr:
                continue
                
            # MA Trend Filter (Override SVI signal check if needed, or just add as extra filter)
            # The SVI signal in DB is based on Price > 200MA.
            # If user selects different MA, we check it here.
            close = safe_float_preserving_none(ind.get('close'))
            ma_val = safe_float_preserving_none(ind.get(ma_key) or ind.get(ma_key.lower()))
            if close and ma_val and close <= ma_val:
                continue
            
            stats['vol_pass'] += 1
            
            score = safe_int(ind.get('smart_score') or ind.get('Smart_Score'))
            
            if score is None:
                continue
                
            stats['has_score'] += 1
            
            if safe_int(ind.get('smi_signal') or ind.get('SMI_Signal')) == 1:
                stats['smi_sig'] += 1
            if safe_int(ind.get('svi_signal') or ind.get('SVI_Signal')) == 1:
                stats['svi_sig'] += 1
            if safe_int(ind.get('nvi_signal') or ind.get('NVI_Signal')) == 1:
                stats['nvi_sig'] += 1
            if safe_int(ind.get('vsa_signal') or ind.get('VSA_Signal')) > 0:
                stats['vsa_sig'] += 1
            if safe_int(ind.get('vol_div_signal') or ind.get('Vol_Div_Signal')) > 0:
                stats['vol_div_sig'] += 1
            if safe_int(ind.get('weekly_nvi_signal') or ind.get('Weekly_NVI_Signal')) > 0:
                stats['weekly_nvi_sig'] += 1
            
            vwap_val = safe_float_preserving_none(ind.get('vwap20') or ind.get('VWAP'))
            if ind.get('close') and vwap_val:
                if safe_float_preserving_none(ind.get('close')) > vwap_val:
                    stats['vwap_sig'] += 1
            
            # Score distribution (max score is 6)
            if score >= 4:
                stats['score_4'] += 1
            if score >= 5:
                stats['score_5'] += 1
            if score >= 6:
                stats['score_6'] += 1
            
            if score >= 4:
                results.append((code, score, ind))
                
        except:
            continue
        
    results.sort(key=lambda x: x[1], reverse=True)
    
    print_flush("\n" + "=" * 60)
    print_flush("[ç¯©é¸éç¨‹] è°æ˜éŒ¢æŒ‡æ¨™å¤šå±¤ç¯©é¸ (NVIç‰ˆ)")
    print_flush("=" * 60)
    print_flush(f"ç¸½è‚¡æ•¸: {stats['total']}")
    print_flush("â”€" * 60)
    print_flush(f"âœ“ æˆäº¤é‡ >= {min_vol//1000}å¼µ            â†’ {stats['vol_pass']} æª”")
    print_flush("â”€" * 60)
    print_flush("ã€å„é …è¨Šè™Ÿçµ±è¨ˆã€‘(é€šéæˆäº¤é‡é–€æª»è€…)")
    print_flush(f"  â€¢ NVI è¶¨å‹¢ (NVI>200MA)    â†’ {stats['smi_sig']} æª”")
    print_flush(f"  â€¢ NVI > PVI (å¤šé ­æ’åˆ—)    â†’ {stats['nvi_sig']} æª”")
    print_flush(f"  â€¢ ç„¡èƒŒé›¢ (åƒ¹é«˜NVIé«˜)      â†’ {stats['vsa_sig']} æª”")
    print_flush(f"  â€¢ åƒ¹æ ¼è¶¨å‹¢ (åƒ¹>200MA)     â†’ {stats['svi_sig']} æª”")
    print_flush(f"  â€¢ ç„¡é‡åƒ¹èƒŒé›¢ (æ–°)         â†’ {stats['vol_div_sig']} æª”")
    print_flush(f"  â€¢ é€±ç·šNVIè¶¨å‹¢ (æ–°)        â†’ {stats['weekly_nvi_sig']} æª”")
    print_flush("â”€" * 60)
    print_flush("ã€Smart Score åˆ†å¸ƒã€‘(æ»¿åˆ†6åˆ†)")
    print_flush(f"  â€¢ Score >= 4 (è²·å…¥è¨Šè™Ÿ)   â†’ {stats['score_4']} æª”")
    print_flush(f"  â€¢ Score >= 5 (å¼·çƒˆè²·å…¥)   â†’ {stats['score_5']} æª”")
    print_flush(f"  â€¢ Score >= 6 (æ¥µå¼·è¨Šè™Ÿ)   â†’ {stats['score_6']} æª”")
    print_flush("=" * 60)
    
    if stats['vol_div_sig'] == 0 and stats['weekly_nvi_sig'] == 0:
        print_flush("ğŸ’¡ æç¤º: è‹¥æ–°è¨Šè™Ÿ(ç„¡é‡åƒ¹èƒŒé›¢/é€±ç·šNVI)å‡ç‚º 0ï¼Œè«‹åŸ·è¡Œ [1] è³‡æ–™ç®¡ç† -> [4] é‡æ–°è¨ˆç®—æŒ‡æ¨™")
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º (v2)
    def smart_money_extra(code, ind):
        nvi = safe_num(ind.get('nvi') or ind.get('NVI'))
        nvi_ma = safe_num(ind.get('nvi_ma200') or ind.get('NVI_MA200'))
        score = safe_int(ind.get('smart_score') or ind.get('Smart_Score'))
        
        # ç°¡å–®é¢¨éšªå»ºè­°é‚è¼¯
        risk = "ä½" if score >= 5 else "ä¸­"
        suggestion = "å¼·åŠ›è²·é€²" if score >= 6 else ("è²·é€²" if score >= 4 else "è§€å¯Ÿ")
        
        nvi_str = f"{nvi:.1f}" if nvi else "-"
        nvi_ma_str = f"{nvi/nvi_ma:.2f}" if nvi and nvi_ma else "-"
        
        return [nvi_str, nvi_ma_str, str(score), risk, suggestion]

    codes = display_scan_results_v2(results, "è°æ˜éŒ¢æƒæçµæœ (NVIç‰ˆ)", limit=limit,
                            extra_headers=["NVIå€¼", "NVI/MA", "åˆ†æ•¸", "é¢¨éšª", "å»ºè­°"],
                            extra_func=smart_money_extra)
    
    prompt_stock_detail_report(codes)



def execute_kd_golden_scan():
    """æœˆKDäº¤å‰æƒæ"""
    limit, min_vol = get_user_scan_params()
    
    print_flush(f"\næ­£åœ¨æƒæ æœˆKDäº¤å‰ (Kâ†‘ç©¿è¶ŠDâ†‘ æˆ– Dâ†‘ç©¿è¶ŠKâ†‘)...")
    
    results = []
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    
    if not data:
        print_flush("âŒ ç„¡æŒ‡æ¨™æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™æ›´æ–°")
        return

    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol is None or vol < min_vol:
                continue

            k = safe_float_preserving_none(ind.get('month_k'))
            d = safe_float_preserving_none(ind.get('month_d'))
            k_prev = safe_float_preserving_none(ind.get('month_k_prev'))
            d_prev = safe_float_preserving_none(ind.get('month_d_prev'))
            
            if None in [k, d, k_prev, d_prev]:
                continue
            
            k_rising = k > k_prev
            d_rising = d > d_prev
            
            if (k > d and k_prev <= d_prev) and k_rising and d_rising:
                results.append((code, k, ind, "Kâ†‘ç©¿è¶ŠDâ†‘"))
            elif (d > k and d_prev <= k_prev) and d_rising and k_rising:
                results.append((code, k, ind, "Dâ†‘ç©¿è¶ŠKâ†‘"))
                
        except:
            continue
        
    results.sort(key=lambda x: x[1])
    
    print_flush(f"\næœˆKDäº¤å‰: æ‰¾åˆ° {len(results)} æª”ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
    print_flush(f"æ’åºæ–¹å¼: Kå€¼ç”±å°åˆ°å¤§ (0% -> 100%)")
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def kd_extra(code, ind):
        k = safe_num(ind.get('month_k')) or 0
        d = safe_num(ind.get('month_d')) or 0
        type_str = ind.get('_type_str', '')
        return [f"K:{k:.1f}", f"D:{d:.1f}", type_str]
    
    for item in results:
        item[2]['_type_str'] = item[3] if len(item) > 3 else ''
    
    codes = display_scan_results_v2(results, "æœˆKDäº¤å‰", limit=limit, 
                               description="KD: Kå€¼ç”±ä¸‹å¾€ä¸Šç©¿è¶ŠDå€¼=é»ƒé‡‘äº¤å‰ (è²·é€²è¨Šè™Ÿ)",
                               extra_headers=["æœˆK", "æœˆD", "è¨Šè™Ÿ"],
                               extra_func=kd_extra)
    prompt_stock_detail_report(codes)

def scan_nvi_pvi_crossover():
    """NVI/PVI äº¤å‰æƒæ"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ NVI/PVI äº¤å‰è¨Šè™Ÿ...")
    
    results = []
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    
    if not data:
        print_flush("âŒ ç„¡æŒ‡æ¨™æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™æ›´æ–°")
        return

    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol is None or vol < min_vol:
                continue

            # 1. NVI > PVI Golden Cross
            nvi = safe_float_preserving_none(ind.get('nvi'))
            pvi = safe_float_preserving_none(ind.get('pvi'))
            nvi_prev = safe_float_preserving_none(ind.get('nvi_prev'))
            pvi_prev = safe_float_preserving_none(ind.get('pvi_prev'))
            
            nvi_pvi_cross = False
            if None not in [nvi, pvi, nvi_prev, pvi_prev]:
                if nvi > pvi and nvi_prev <= pvi_prev:
                    nvi_pvi_cross = True

            # 2. NVI > MA200 Golden Cross (using smi_signal)
            # smi_signal = 1 means NVI > MA200 & MA200 Rising
            smi_sig = ind.get('smi_signal')
            smi_sig_prev = ind.get('smi_signal_prev')
            
            nvi_ma_cross = False
            if smi_sig == 1 and (smi_sig_prev is None or smi_sig_prev == 0):
                nvi_ma_cross = True
                
            if nvi_pvi_cross or nvi_ma_cross:
                signals = []
                if nvi_pvi_cross: signals.append("NVIç©¿è¶ŠPVI")
                if nvi_ma_cross: signals.append("NVIå¤šé ­ç¢ºèª")
                
                results.append((code, nvi, ind, ",".join(signals)))
                
        except:
            continue
            
    results.sort(key=lambda x: x[1], reverse=True) # NVI å¤§çš„æ’å‰é¢
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def nvi_extra(code, ind):
        val = safe_num(ind.get('nvi') or ind.get('NVI')) or 0
        pvi = safe_num(ind.get('pvi') or ind.get('PVI')) or 0
        signals = ind.get('_scan_note', '')
        return [f"NVI:{val:.1f}", f"PVI:{pvi:.1f}", signals]
    
    for r in results:
        r[2]['_scan_note'] = r[3] if len(r) > 3 else ''
    
    codes = display_scan_results_v2(results, "NVI/PVI äº¤å‰æƒæ", limit=limit, 
                               description="NVI: è² é‡æŒ‡æ¨™(è°æ˜éŒ¢), PVI: æ­£é‡æŒ‡æ¨™(æ•£æˆ¶). NVIç©¿è¶ŠPVI=ä¸»åŠ›æ§ç›¤",
                               extra_headers=["NVI", "PVI", "è¨Šè™Ÿ"],
                               extra_func=nvi_extra)
    prompt_stock_detail_report(codes)

def crossover_scan_submenu():
    """äº¤å‰æƒæå­é¸å–® (æœˆKD, NVI/PVI)"""
    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€äº¤å‰è¨Šè™Ÿæƒæã€‘")
        print_flush("="*60)
        print_flush("[1] æœˆKDäº¤å‰ (Kâ†‘ç©¿è¶ŠDâ†‘)")
        print_flush("[2] NVI/PVI äº¤å‰ (ä¸»åŠ›ç±Œç¢¼è¨Šè™Ÿ)")
        print_flush("[0] è¿”å›")
        
        ch = read_single_key()
        
        if ch == '0':
            break
        elif ch == '1':
            execute_kd_golden_scan()
        elif ch == '2':
            scan_nvi_pvi_crossover()

def scan_ma_alignment_rising(check_price_above=True):
    """å‡ç·šå¤šé ­æƒæ (å››ç·š: 20,60,120,200)"""
    limit, min_vol = get_user_scan_params()

    title = "å‡ç·šç¯©é¸ (å››ç·šä¸Šæš+è‚¡åƒ¹åœ¨ä¸Š+0-10%)" if check_price_above else "å‡ç·šç¯©é¸ (å››ç·šä¸Šæš)"
    print_flush(f"\næ­£åœ¨æƒæ {title}...")
    
    results = []
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    
    if not data:
        print_flush("âŒ ç„¡æŒ‡æ¨™æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™æ›´æ–°")
        return

    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol is None or vol < min_vol:
                continue

            close = safe_float_preserving_none(ind.get('close'))
            ma20 = safe_float_preserving_none(ind.get('ma20'))
            ma60 = safe_float_preserving_none(ind.get('ma60'))
            ma120 = safe_float_preserving_none(ind.get('ma120'))
            ma200 = safe_float_preserving_none(ind.get('ma200'))
            
            ma20_prev = safe_float_preserving_none(ind.get('ma20_prev'))
            ma60_prev = safe_float_preserving_none(ind.get('ma60_prev'))
            ma120_prev = safe_float_preserving_none(ind.get('ma120_prev'))
            ma200_prev = safe_float_preserving_none(ind.get('ma200_prev'))
            
            if None in [close, ma20, ma60, ma120, ma200]:
                continue
            
            if not (ma20_prev and ma60_prev and ma120_prev and ma200_prev):
                continue
                
            # æª¢æŸ¥æ–œç‡ > 0 (Rising) - å››ç·šä¸Šæš
            is_all_rising = (ma20 > ma20_prev and
                            ma60 > ma60_prev and
                            ma120 > ma120_prev and
                            ma200 > ma200_prev)
            
            if not is_all_rising:
                continue
            
            if check_price_above:
                # æª¢æŸ¥è‚¡åƒ¹ > æ‰€æœ‰å‡ç·š (å››ç·š)
                is_above = (close > ma20 and close > ma60 and 
                           close > ma120 and close > ma200)
                
                if not is_above:
                    continue
            
            highest_ma = max(ma20, ma60, ma120, ma200)
            lowest_ma = min(ma20, ma60, ma120, ma200)
            
            if highest_ma <= 0 or lowest_ma <= 0:
                continue
            
            # æª¢æŸ¥å››ç·šå·®è· (æœ€é«˜èˆ‡æœ€ä½å‡ç·šå·®è· <= 10%)
            ma_spread_pct = (highest_ma - lowest_ma) / lowest_ma * 100
            if ma_spread_pct > 10:
                continue
                
            # æª¢æŸ¥è·é›¢ (0-10%)
            distance_pct = (close - highest_ma) / highest_ma * 100
            
            if not (0 <= distance_pct <= 10):
                continue
            
            ind['distance_pct'] = distance_pct
            results.append((code, distance_pct, ind))
                
        except:
            continue
    
    results = sorted(results, key=lambda x: x[1])
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def ma_extra(code, ind):
        dist_pct = ind.get('distance_pct', 0)
        ma20 = safe_num(ind.get('ma20') or ind.get('MA20'))
        ma200 = safe_num(ind.get('ma200') or ind.get('MA200'))
        return [f"è·MA:{dist_pct:.1f}%", f"MA200:{ma200:.1f}" if ma200 else "-"]
    
    codes = display_scan_results_v2(results, title, limit=limit, 
                               description="æ¢ä»¶: 20,60,120,200 å…¨æ•¸å‘ä¸Šï¼Œè‚¡åƒ¹ > æ‰€æœ‰å‡ç·šï¼Œè·æœ€é«˜å‡ç·š 0-10%",
                               extra_headers=["è·MA", "MA200"],
                               extra_func=ma_extra)
    prompt_stock_detail_report(codes)

def triple_filter_scan():
    """ä¸‰é‡ç¯©é¸å…¥å£"""
    limit, min_vol = get_user_scan_params()

    title = "ä¸‰é‡ç¯©é¸ (é€²éšç‰ˆ)"
    print_flush(f"â—‡ æ­£åœ¨åŸ·è¡Œ{title}... (æœ€å°æˆäº¤é‡: {min_vol}å¼µ)")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    
    if not data:
        print_flush("âŒ ç„¡æŒ‡æ¨™æ•¸æ“šï¼Œè«‹å…ˆåŸ·è¡Œè³‡æ–™æ›´æ–°")
        return
    
    print_flush("æ­¤åŠŸèƒ½å·²æ•´åˆè‡³ [7] è°æ˜éŒ¢æƒæ")

def analyze_smart_money(code):
    """åˆ†æå–®ä¸€å€‹è‚¡çš„è°æ˜éŒ¢æŒ‡æ¨™ç‹€æ…‹"""
    print_flush(f"\næ­£åœ¨åˆ†æ {code} çš„è°æ˜éŒ¢æŒ‡æ¨™ç‹€æ…‹...")
    
    try:
        with db_manager.get_connection() as conn:
            # Fetch last 400 days to ensure enough data for 200MA + lookback
            df = pd.read_sql_query("SELECT date_int, close, volume, high, low FROM stock_history WHERE code=? ORDER BY date_int ASC", conn, params=(code,))
            
        if df.empty or len(df) < 250:
            print_flush("âŒ æ­·å²æ•¸æ“šä¸è¶³ (éœ€è‡³å°‘250å¤©)ï¼Œç„¡æ³•é€²è¡Œå®Œæ•´åˆ†æ")
            return None

        # Calculate NVI/PVI
        df = IndicatorCalculator.calculate_nvi_pvi_df(df)
        
        # Calculate MFI
        df['MFI'] = IndicatorCalculator.calculate_mfi(df, 14)
        
        latest = df.iloc[-1]
        prev_20 = df.iloc[-20] if len(df) >= 20 else df.iloc[0]
        
        # Cond 1: NVI Trend
        cond1 = (latest['NVI'] > latest['NVI_200MA']) and (latest['NVI_200MA'] > prev_20['NVI_200MA'])
        
        # Cond 2: NVI > PVI and Crossover
        prev_5 = df.iloc[-5] if len(df) >= 5 else df.iloc[0]
        cond2 = (latest['NVI'] > latest['PVI']) and (prev_5['NVI'] <= prev_5['PVI'])
        
        # Cond 3: No Divergence
        lookback = 60
        recent_df = df.iloc[-lookback:]
        recent_high = recent_df['close'].max()
        recent_nvi_high = recent_df['NVI'].max()
        
        divergence = (
            (abs(latest['close'] - recent_high) / recent_high < 0.02) and
            (abs(latest['NVI'] - recent_nvi_high) / recent_nvi_high > 0.05)
        )
        cond3 = not divergence
        
        # Cond 4: Price Trend
        cond4 = latest['close'] > latest['Price_200MA']
        
        overall_pass = cond1 and cond2 and cond3 and cond4
        
        print_flush("\n" + "="*60)
        print_flush(f"ã€è°æ˜éŒ¢æŒ‡æ¨™æ·±åº¦åˆ†æã€‘ {code}")
        print_flush("="*60)
        print_flush(f"ç•¶å‰åƒ¹æ ¼: {latest['close']:.2f}")
        print_flush(f"åƒ¹æ ¼200æ—¥å‡ç·š: {latest['Price_200MA']:.2f}")
        print_flush(f"NVIå€¼: {latest['NVI']:.2f} (200MA: {latest['NVI_200MA']:.2f})")
        print_flush(f"PVIå€¼: {latest['PVI']:.2f} (200MA: {latest['PVI_200MA']:.2f})")
        print_flush(f"MFI(14æ—¥): {latest['MFI']:.1f}")
        print_flush("-" * 60)
        print_flush("ç¯©é¸æ¢ä»¶æª¢æŸ¥:")
        
        # 5. Volume Divergence
        vol_div = IndicatorCalculator.detect_volume_divergence(df)
        no_vol_div_pass = ~vol_div.iloc[-1]
        
        # 6. Weekly NVI
        weekly_nvi_signal = IndicatorCalculator.calculate_weekly_nvi_signal(df)
        weekly_nvi_pass = weekly_nvi_signal.iloc[-1] == 1
        
        # è¨ˆç®—ç¸½åˆ†
        score = (int(cond1) + int(cond2) + int(cond3) + 
                 int(cond4) + int(no_vol_div_pass) + int(weekly_nvi_pass))
    
        print_flush(f"  1. NVI è¶¨å‹¢ (NVI > 200MA & Rising): {'âœ… é€šé' if cond1 else 'âŒ æœªé€šé'}")
        print_flush(f"  2. NVI/PVI é—œä¿‚ (NVI > PVI):        {'âœ… é€šé' if cond2 else 'âŒ æœªé€šé'}")
        print_flush(f"  3. ç„¡èƒŒé›¢ (åƒ¹é«˜NVIé«˜):              {'âœ… é€šé' if cond3 else 'âŒ æœªé€šé'}")
        print_flush(f"  4. åƒ¹æ ¼è¶¨å‹¢ (åƒ¹ > 200MA):           {'âœ… é€šé' if cond4 else 'âŒ æœªé€šé'}")
        print_flush(f"  5. ç„¡é‡åƒ¹èƒŒé›¢ (åƒ¹æ¼²é‡å¢):           {'âœ… é€šé' if no_vol_div_pass else 'âŒ æœªé€šé'}")
        print_flush(f"  6. é€±ç·šNVIè¶¨å‹¢ (é€±NVI > 40MA):      {'âœ… é€šé' if weekly_nvi_pass else 'âŒ æœªé€šé'}")
        print_flush("-" * 40)
        print_flush(f"ç¶œåˆè©•åˆ†: {score}/6")
        print_flush("="*60)
        
        return {
            'nvi': latest['NVI'],
            'pvi': latest['PVI'],
            'mfi': latest['MFI'],
            'pass_all': overall_pass
        }
        
    except Exception as e:
        print_flush(f"âŒ åˆ†æå¤±æ•—: {e}")
        return None

# ==========================================
# Advanced Price-Volume Divergence Analysis
# ==========================================
def calculate_mfi_series_advanced(df, period=14):
    """è¨ˆç®—MFIï¼ˆè³‡é‡‘æµé‡æŒ‡æ¨™ï¼‰åºåˆ— (Advanced)"""
    df = df.copy()
    # Ensure columns exist (case insensitive)
    if 'High' not in df.columns: df['High'] = df['high']
    if 'Low' not in df.columns: df['Low'] = df['low']
    if 'Close' not in df.columns: df['Close'] = df['close']
    if 'Volume' not in df.columns: df['Volume'] = df['volume']
    
    df['Typical_Price'] = (df['High'] + df['Low'] + df['Close']) / 3
    df['Money_Flow'] = df['Typical_Price'] * df['Volume']
    df['Price_Change'] = df['Typical_Price'].diff()
    df['Positive_MF'] = np.where(df['Price_Change'] > 0, df['Money_Flow'], 0)
    df['Negative_MF'] = np.where(df['Price_Change'] < 0, df['Money_Flow'], 0)
    df['Positive_MF_14'] = df['Positive_MF'].rolling(window=period).sum()
    df['Negative_MF_14'] = df['Negative_MF'].rolling(window=period).sum()
    df['MF_Ratio'] = df['Positive_MF_14'] / df['Negative_MF_14']
    df['MFI'] = 100 - (100 / (1 + df['MF_Ratio']))
    return df['MFI']

def calculate_mfi_for_volume(volume_series, period=14):
    """ç‚ºæˆäº¤é‡è¨ˆç®—MFI"""
    fake_df = pd.DataFrame({
        'High': volume_series,
        'Low': volume_series,
        'Close': volume_series,
        'Volume': np.ones_like(volume_series)
    })
    return calculate_mfi_series_advanced(fake_df, period)

def method1_direct_comparison(df, consecutive_days=3):
    """æ–¹æ³•1ï¼šç›´æ¥æ¯”è¼ƒæ³•"""
    results = {'åƒ¹æ¼²é‡ç¸®': False, 'åƒ¹è·Œé‡ç¸®': False, 'é€£çºŒå¤©æ•¸': 0, 'è©³ç´°æ•¸æ“š': []}
    if len(df) < consecutive_days + 1: return results
    
    price_changes = df['Close'].diff()
    volume_changes = df['Volume'].diff()
    consecutive_up_down = 0
    consecutive_down_up = 0
    price_up_volume_down_days = []
    price_down_volume_up_days = []
    
    for i in range(1, min(consecutive_days + 5, len(df))):
        price_change = price_changes.iloc[-i]
        volume_change = volume_changes.iloc[-i]
        
        if price_change > 0 and volume_change < 0:
            consecutive_up_down += 1
            consecutive_down_up = 0
            price_up_volume_down_days.append(i)
        elif price_change < 0 and volume_change < 0:
            consecutive_down_up += 1
            consecutive_up_down = 0
            price_down_volume_up_days.append(i)
        else:
            consecutive_up_down = 0
            consecutive_down_up = 0
        
        if consecutive_up_down >= consecutive_days:
            results['åƒ¹æ¼²é‡ç¸®'] = True
            results['é€£çºŒå¤©æ•¸'] = consecutive_up_down
        if consecutive_down_up >= consecutive_days:
            results['åƒ¹è·Œé‡ç¸®'] = True
            if results['é€£çºŒå¤©æ•¸'] < consecutive_down_up:
                results['é€£çºŒå¤©æ•¸'] = consecutive_down_up
    
    results['è©³ç´°æ•¸æ“š'] = {
        'åƒ¹æ¼²é‡ç¸®å¤©æ•¸': price_up_volume_down_days[:consecutive_days],
        'åƒ¹è·Œé‡ç¸®å¤©æ•¸': price_down_volume_up_days[:consecutive_days]
    }
    return results

def calculate_slope_r2(x, y):
    """ä½¿ç”¨ numpy è¨ˆç®—æ–œç‡èˆ‡ Rå¹³æ–¹"""
    if len(x) < 2: return 0.0, 0.0
    try:
        slope, intercept = np.polyfit(x, y, 1)
        y_pred = slope * x + intercept
        residuals = y - y_pred
        ss_res = np.sum(residuals**2)
        ss_tot = np.sum((y - np.mean(y))**2)
        r2 = 1 - (ss_res / ss_tot) if ss_tot != 0 else 0
        return slope, r2
    except:
        return 0.0, 0.0

def method2_trend_regression(df, lookback_period=3):
    """æ–¹æ³•2ï¼šè¶¨å‹¢å›æ­¸æ³•"""
    results = {'åƒ¹æ¼²é‡ç¸®': False, 'åƒ¹è·Œé‡ç¸®': False, 'åƒ¹æ ¼æ–œç‡': 0, 'æˆäº¤é‡æ–œç‡': 0, 'åƒ¹æ ¼Ræ–¹': 0, 'æˆäº¤é‡Ræ–¹': 0}
    if len(df) < lookback_period: return results
    
    recent_df = df.iloc[-lookback_period:].copy()
    x = np.arange(len(recent_df))
    price_values = recent_df['Close'].values
    volume_values = recent_df['Volume'].values
    
    try:
        price_slope, price_r2 = calculate_slope_r2(x, price_values)
        volume_log = np.log1p(volume_values)
        volume_slope, volume_r2 = calculate_slope_r2(x, volume_log)
        
        if price_slope > 0 and volume_slope < 0: results['åƒ¹æ¼²é‡ç¸®'] = True
        elif price_slope < 0 and volume_slope < 0: results['åƒ¹è·Œé‡ç¸®'] = True
        
        results['åƒ¹æ ¼æ–œç‡'] = price_slope
        results['æˆäº¤é‡æ–œç‡'] = volume_slope
        results['åƒ¹æ ¼Ræ–¹'] = price_r2
        results['æˆäº¤é‡Ræ–¹'] = volume_r2
    except:
        pass
    return results

def method3_ma_slope_divergence(df, short_ma_period=5, slope_lookback=5):
    """æ–¹æ³•3ï¼šçŸ­å‡ç·šæ–œç‡èƒŒé›¢æ³•"""
    results = {'åƒ¹æ¼²é‡ç¸®': False, 'åƒ¹è·Œé‡ç¸®': False, 'åƒ¹æ ¼_sMA5_æ–œç‡': 0.0, 'æˆäº¤é‡_sMA5_æ–œç‡': 0.0, 
               'åƒ¹æ ¼_sMA5_è¶¨å‹¢å¼·åº¦': 'ç„¡è¶¨å‹¢', 'æˆäº¤é‡_sMA5_è¶¨å‹¢å¼·åº¦': 'ç„¡è¶¨å‹¢'}
    if len(df) < short_ma_period + slope_lookback: return results
    
    price_sma5 = df['Close'].rolling(window=short_ma_period).mean()
    volume_sma5 = df['Volume'].rolling(window=short_ma_period).mean()
    
    recent_price = price_sma5.iloc[-slope_lookback:].values
    recent_volume = volume_sma5.iloc[-slope_lookback:].values
    x = np.arange(len(recent_price))
    
    try:
        if len(recent_price) > 1:
            p_slope, p_r2 = calculate_slope_r2(x, recent_price)
        else: p_slope, p_r2 = 0, 0
        
        if len(recent_volume) > 1:
            v_log = np.log1p(recent_volume)
            v_slope, v_r2 = calculate_slope_r2(x, v_log)
        else: v_slope, v_r2 = 0, 0
        
        p_str = 'ç„¡è¶¨å‹¢'
        if p_r2 > 0.6: p_str = 'å¼·ä¸Šå‡' if p_slope > 0 else 'å¼·ä¸‹é™'
        elif p_r2 > 0.3: p_str = 'å¼±ä¸Šå‡' if p_slope > 0 else 'å¼±ä¸‹é™'
            
        v_str = 'ç„¡è¶¨å‹¢'
        if v_r2 > 0.6: v_str = 'å¼·ä¸Šå‡' if v_slope > 0 else 'å¼·ä¸‹é™'
        elif v_r2 > 0.3: v_str = 'å¼±ä¸Šå‡' if v_slope > 0 else 'å¼±ä¸‹é™'
        
        if abs(p_slope) > 0.001 and abs(v_slope) > 0.001:
            if p_slope > 0 and v_slope < 0: results['åƒ¹æ¼²é‡ç¸®'] = True
            elif p_slope < 0 and v_slope < 0: results['åƒ¹è·Œé‡ç¸®'] = True
            
        results['åƒ¹æ ¼_sMA5_æ–œç‡'] = p_slope
        results['æˆäº¤é‡_sMA5_æ–œç‡'] = v_slope
        results['åƒ¹æ ¼_sMA5_è¶¨å‹¢å¼·åº¦'] = p_str
        results['æˆäº¤é‡_sMA5_è¶¨å‹¢å¼·åº¦'] = v_str
    except:
        pass
    return results

def method4_mfi_divergence(df, period=14):
    """æ–¹æ³•4ï¼šMFIèƒŒé›¢æ³•"""
    results = {'åƒ¹æ¼²é‡ç¸®': False, 'åƒ¹è·Œé‡ç¸®': False, 'åƒ¹æ ¼MFI': 0, 'æˆäº¤é‡MFI': 0, 'åƒ¹æ ¼ç‹€æ…‹': 'æ­£å¸¸', 'æˆäº¤é‡ç‹€æ…‹': 'æ­£å¸¸'}
    if len(df) < period + 1: return results
    
    p_mfi = calculate_mfi_series_advanced(df, period)
    cur_p_mfi = p_mfi.iloc[-1]
    
    v_mfi = calculate_mfi_for_volume(df['Volume'], period)
    cur_v_mfi = v_mfi.iloc[-1]
    
    if cur_p_mfi > 70 and cur_v_mfi < 30:
        results['åƒ¹æ¼²é‡ç¸®'] = True
        results['åƒ¹æ ¼ç‹€æ…‹'] = 'è¶…è²·'
        results['æˆäº¤é‡ç‹€æ…‹'] = 'è¶…è³£'
    elif cur_p_mfi < 30 and cur_v_mfi < 30:
        results['åƒ¹è·Œé‡ç¸®'] = True
        results['åƒ¹æ ¼ç‹€æ…‹'] = 'è¶…è³£'
        results['æˆäº¤é‡ç‹€æ…‹'] = 'è¶…è³£'
    else:
        if cur_p_mfi > 70: results['åƒ¹æ ¼ç‹€æ…‹'] = 'è¶…è²·'
        elif cur_p_mfi < 30: results['åƒ¹æ ¼ç‹€æ…‹'] = 'è¶…è³£'
        if cur_v_mfi > 70: results['æˆäº¤é‡ç‹€æ…‹'] = 'è¶…è²·'
        elif cur_v_mfi < 30: results['æˆäº¤é‡ç‹€æ…‹'] = 'è¶…è³£'
            
    results['åƒ¹æ ¼MFI'] = cur_p_mfi
    results['æˆäº¤é‡MFI'] = cur_v_mfi
    return results

def detect_all_divergence_methods(df, params=None):
    """ç¶œåˆæ‡‰ç”¨å››ç¨®æ–¹æ³•æª¢æ¸¬é‡åƒ¹èƒŒé›¢"""
    if params is None: params = {}
    default_params = {
        'consecutive_days': 3, 'trend_lookback': 20, 
        'short_ma_period': 5, 'ma_slope_lookback': 5, 'mfi_period': 14
    }
    config = {**default_params, **params}
    results = {}
    
    results['æ–¹æ³•1_ç›´æ¥æ¯”è¼ƒ'] = method1_direct_comparison(df, config['consecutive_days'])
    results['æ–¹æ³•2_è¶¨å‹¢å›æ­¸'] = method2_trend_regression(df, config['trend_lookback'])
    results['æ–¹æ³•3_çŸ­å‡ç·šæ–œç‡'] = method3_ma_slope_divergence(df, config['short_ma_period'], config['ma_slope_lookback'])
    results['æ–¹æ³•4_MFIèƒŒé›¢'] = method4_mfi_divergence(df, config['mfi_period'])
    
    bullish_count = 0
    bearish_count = 0
    for m in ['æ–¹æ³•1_ç›´æ¥æ¯”è¼ƒ', 'æ–¹æ³•2_è¶¨å‹¢å›æ­¸', 'æ–¹æ³•3_çŸ­å‡ç·šæ–œç‡', 'æ–¹æ³•4_MFIèƒŒé›¢']:
        res = results.get(m, {})
        if res.get('åƒ¹è·Œé‡ç¸®', False): bullish_count += 1
        if res.get('åƒ¹æ¼²é‡ç¸®', False): bearish_count += 1
        
    if bullish_count > bearish_count:
        primary = 'çœ‹æ¼²èƒŒé›¢'
        strength = bullish_count / 4.0
    elif bearish_count > bullish_count:
        primary = 'çœ‹è·ŒèƒŒé›¢'
        strength = bearish_count / 4.0
    else:
        primary = 'ç„¡æ˜ç¢ºèƒŒé›¢'
        strength = 0.0
        
    if bullish_count >= 3: sugg = "å¼·çƒˆçœ‹æ¼²ä¿¡è™Ÿï¼Œè€ƒæ…®åˆ†æ‰¹è²·å…¥"
    elif bullish_count == 2: sugg = "æº«å’Œçœ‹æ¼²ä¿¡è™Ÿï¼Œå¯å°‘é‡å¸ƒå±€"
    elif bearish_count >= 3: sugg = "å¼·çƒˆçœ‹è·Œä¿¡è™Ÿï¼Œè€ƒæ…®æ¸›å€‰æˆ–è§€æœ›"
    elif bearish_count == 2: sugg = "æº«å’Œçœ‹è·Œä¿¡è™Ÿï¼Œæ³¨æ„é¢¨éšª"
    elif bullish_count == 1 and bearish_count == 1: sugg = "ä¿¡è™ŸçŸ›ç›¾ï¼Œå»ºè­°è§€æœ›"
    elif bullish_count == 1: sugg = "è¼•å¾®çœ‹æ¼²ä¿¡è™Ÿï¼Œç­‰å¾…ç¢ºèª"
    elif bearish_count == 1: sugg = "è¼•å¾®çœ‹è·Œä¿¡è™Ÿï¼Œè¬¹æ…æ“ä½œ"
    else: sugg = "ç„¡æ˜é¡¯èƒŒé›¢ï¼Œè¶¨å‹¢å¯èƒ½å»¶çºŒ"
    
    results['ç¶œåˆè©•åˆ†'] = {
        'çœ‹æ¼²èƒŒé›¢æ¬¡æ•¸': bullish_count,
        'çœ‹è·ŒèƒŒé›¢æ¬¡æ•¸': bearish_count,
        'ä¸»è¦ä¿¡è™Ÿ': primary,
        'ä¿¡è™Ÿå¼·åº¦': strength,
        'äº¤æ˜“å»ºè­°': sugg,
        'åƒæ•¸é…ç½®': config
    }
    return results

def generate_detailed_report(df, divergence_results):
    """ç”Ÿæˆè©³ç´°åˆ†æå ±å‘Š"""
    report = []
    report.append("=" * 70)
    report.append("ğŸ“Š é‡åƒ¹èƒŒé›¢å››æ–¹æ³•æ•´åˆåˆ†æå ±å‘Š")
    report.append("=" * 70)
    
    latest_price = df['Close'].iloc[-1]
    latest_volume = df['Volume'].iloc[-1]
    price_change = df['Close'].iloc[-1] - df['Close'].iloc[-2]
    volume_change = df['Volume'].iloc[-1] - df['Volume'].iloc[-2]
    
    report.append(f"\nğŸ“ˆ åŸºæœ¬è³‡æ–™:")
    report.append(f"  ç•¶å‰åƒ¹æ ¼: {latest_price:.2f}")
    report.append(f"  ç•¶å‰æˆäº¤é‡: {latest_volume:,.0f}")
    report.append(f"  åƒ¹æ ¼è®ŠåŒ–: {price_change:+.2f}")
    report.append(f"  æˆäº¤é‡è®ŠåŒ–: {volume_change:+,.0f}")
    
    m1 = divergence_results['æ–¹æ³•1_ç›´æ¥æ¯”è¼ƒ']
    report.append(f"\nğŸ” æ–¹æ³•1: ç›´æ¥æ¯”è¼ƒæ³•")
    if m1['åƒ¹æ¼²é‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹æ¼²é‡ç¸® (é€£çºŒ{m1['é€£çºŒå¤©æ•¸']}å¤©)")
    if m1['åƒ¹ç¸®é‡æ¼²']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹ç¸®é‡æ¼² (é€£çºŒ{m1['é€£çºŒå¤©æ•¸']}å¤©)")
    if not m1['åƒ¹æ¼²é‡ç¸®'] and not m1['åƒ¹ç¸®é‡æ¼²']: report.append(f"  â—‹ æœªæª¢æ¸¬åˆ°æ˜é¡¯èƒŒé›¢")
    
    m2 = divergence_results['æ–¹æ³•2_è¶¨å‹¢å›æ­¸']
    report.append(f"\nğŸ“Š æ–¹æ³•2: è¶¨å‹¢å›æ­¸æ³•")
    report.append(f"  åƒ¹æ ¼æ–œç‡: {m2['åƒ¹æ ¼æ–œç‡']:.6f}")
    report.append(f"  æˆäº¤é‡æ–œç‡: {m2['æˆäº¤é‡æ–œç‡']:.6f}")
    if m2['åƒ¹æ¼²é‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹æ¼²é‡ç¸®")
    if m2['åƒ¹è·Œé‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹è·Œé‡ç¸®")
    
    m3 = divergence_results['æ–¹æ³•3_çŸ­å‡ç·šæ–œç‡']
    report.append(f"\nğŸ“ˆ æ–¹æ³•3: çŸ­å‡ç·šæ–œç‡æ³•")
    report.append(f"  åƒ¹æ ¼sMA5æ–œç‡: {m3['åƒ¹æ ¼_sMA5_æ–œç‡']:.6f} ({m3['åƒ¹æ ¼_sMA5_è¶¨å‹¢å¼·åº¦']})")
    report.append(f"  æˆäº¤é‡sMA5æ–œç‡: {m3['æˆäº¤é‡_sMA5_æ–œç‡']:.6f} ({m3['æˆäº¤é‡_sMA5_è¶¨å‹¢å¼·åº¦']})")
    if m3['åƒ¹æ¼²é‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹æ¼²é‡ç¸®")
    if m3['åƒ¹è·Œé‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹è·Œé‡ç¸®")
    
    m4 = divergence_results['æ–¹æ³•4_MFIèƒŒé›¢']
    report.append(f"\nâš¡ æ–¹æ³•4: MFIèƒŒé›¢æ³•")
    report.append(f"  åƒ¹æ ¼MFI: {m4['åƒ¹æ ¼MFI']:.1f} ({m4['åƒ¹æ ¼ç‹€æ…‹']})")
    report.append(f"  æˆäº¤é‡MFI: {m4['æˆäº¤é‡MFI']:.1f} ({m4['æˆäº¤é‡ç‹€æ…‹']})")
    if m4['åƒ¹æ¼²é‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹æ¼²é‡ç¸®")
    if m4['åƒ¹è·Œé‡ç¸®']: report.append(f"  âœ“ æª¢æ¸¬åˆ°åƒ¹è·Œé‡ç¸®")
    
    summary = divergence_results['ç¶œåˆè©•åˆ†']
    report.append(f"\n{'='*70}")
    report.append(f"ğŸ¯ ç¶œåˆè©•åˆ†")
    report.append(f"  ä¸»è¦ä¿¡è™Ÿ: {summary['ä¸»è¦ä¿¡è™Ÿ']}")
    report.append(f"  ä¿¡è™Ÿå¼·åº¦: {summary['ä¿¡è™Ÿå¼·åº¦']:.2f}")
    report.append(f"  äº¤æ˜“å»ºè­°: {summary['äº¤æ˜“å»ºè­°']}")
    report.append(f"{'='*70}")
    return "\n".join(report)

# ==========================================
# 3-Day Divergence System
# ==========================================
def get_three_day_divergence_params():
    """3æ—¥èƒŒé›¢æª¢æ¸¬å°ˆç”¨åƒæ•¸é…ç½®"""
    return {
        'æ–¹æ³•1_ç›´æ¥æ¯”è¼ƒ': {'consecutive_days': 3, 'description': 'æª¢æŸ¥æ˜¯å¦é€£çºŒ3å¤©å‡ºç¾åƒ¹æ¼²é‡ç¸®æˆ–åƒ¹è·Œé‡ç¸®'},
        'æ–¹æ³•2_è¶¨å‹¢å›æ­¸': {'lookback_period': 3, 'description': 'åˆ†ææœ€è¿‘3å¤©åŸå§‹åƒ¹æ ¼/æˆäº¤é‡çš„ç·šæ€§è¶¨å‹¢'},
        'æ–¹æ³•3_çŸ­å‡ç·šæ–œç‡': {'short_ma_period': 3, 'slope_lookback': 3, 'description': 'åˆ†æsMA3å‡ç·šåœ¨æœ€è¿‘3å¤©çš„è¶¨å‹¢æ–¹å‘'},
        'æ–¹æ³•4_MFIèƒŒé›¢': {'mfi_period': 3, 'description': 'è¨ˆç®—3æ—¥MFIåˆ¤æ–·è¶…è²·è¶…è³£ç‹€æ…‹'}
    }

def check_three_day_data_sufficiency(df):
    """æª¢æŸ¥æ•¸æ“šæ˜¯å¦è¶³å¤ é€²è¡Œ3æ—¥åˆ†æ"""
    warnings = []
    if len(df) < 10:
        warnings.append(f"æ•¸æ“šé‡ä¸è¶³ ({len(df)}å¤©)ï¼Œå»ºè­°è‡³å°‘10å¤©")
    return warnings




def scan_pv_divergence_analysis():
    """é‡åƒ¹èƒŒé›¢å½¢æ…‹è©³è§£ (ä½¿ç”¨å¿«å–ä¿¡è™Ÿ)"""
    limit = get_display_limit(30)
    min_vol = get_volume_limit(100)
    
    print_flush("\næ­£åœ¨æƒæ é‡åƒ¹èƒŒé›¢å½¢æ…‹...")
    
    results = []
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else None
    if not data:
        data = step4_load_data()
        if GLOBAL_INDICATOR_CACHE:
            GLOBAL_INDICATOR_CACHE.set_data(data)
            
    if not data:
        print_flush("âŒ ç„¡æ³•è¼‰å…¥è³‡æ–™")
        return

    # Use cached signals from snapshot
    for code, info in data.items():
        try:
            vol = safe_float_preserving_none(info.get('volume', 0))
            if vol is None or vol < min_vol:
                continue
            
            # Read cached divergence signals
            div_bull = safe_int(info.get('div_3day_bull'))
            div_bear = safe_int(info.get('div_3day_bear'))
            nvi_sig = safe_int(info.get('nvi_signal') or info.get('NVI_Signal'))
            
            if div_bull == 0 and div_bear == 0:
                continue
            
            # Determine divergence type
            if div_bull > 0:
                div_type = "åƒ¹è·Œé‡æ¼²"
                score = div_bull / 3.0  # Normalize to 0-1
                color = get_color_code(1)
                suggestion = "å¼·çƒˆçœ‹æ¼²ä¿¡è™Ÿï¼Œè€ƒæ…®åˆ†æ‰¹è²·å…¥"
            elif div_bear > 0:
                div_type = "åƒ¹æ¼²é‡è·Œ"
                score = div_bear / 3.0
                color = get_color_code(-1)
                suggestion = "å¼·çƒˆçœ‹è·Œä¿¡è™Ÿï¼Œè€ƒæ…®æ¸›å€‰è§€æœ›"
            else:
                continue
            
            # Calculate MA200 trend
            ma200 = safe_float_preserving_none(info.get('ma200') or info.get('MA200'))
            ma200_prev = safe_float_preserving_none(info.get('ma200_prev') or info.get('MA200_prev'))
            
            if ma200 and ma200_prev:
                ma200_trend = "ä¸Šæš" if ma200 > ma200_prev else "ä¸‹è·Œ"
            else:
                ma200_trend = "N/A"
            
            # Calculate TP/SL
            close = safe_float_preserving_none(info.get('close'))
            vp_upper = safe_float_preserving_none(info.get('vp_upper') or info.get('VP_upper'))
            vp_lower = safe_float_preserving_none(info.get('vp_lower') or info.get('VP_lower'))
            
            tp = vp_upper if vp_upper else (close * 1.1 if close else 0)
            sl = vp_lower if vp_lower else (close * 0.95 if close else 0)
            
            results.append({
                'code': code,
                'name': info.get('name', code),
                'close': close,
                'volume': vol,
                'type': div_type,
                'score': score,
                'color': color,
                'nvi_sig': nvi_sig,
                'suggestion': suggestion,
                'ma200_trend': ma200_trend,
                'tp': tp,
                'sl': sl,
                'info': info
            })
            
        except Exception:
            continue
    
    # Sort by score desc
    results.sort(key=lambda x: x['score'], reverse=True)
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def pv_extra(code, item):
        div_type = item.get('type', '')
        score = item.get('score', 0)
        ma200_trend = item.get('ma200_trend', '')
        suggestion = item.get('suggestion', '').split("ï¼Œ")[0]
        
        stars = "â˜…" * int(score * 3)
        if not stars: stars = "â˜…"
        
        risk = "ä½" if ma200_trend == "ä¸Šæš" else "é«˜"
        
        return [div_type, stars, risk, suggestion]

    codes = display_scan_results_v2(results, "é‡åƒ¹èƒŒé›¢å½¢æ…‹", limit=limit,
                            extra_headers=["å‹æ…‹", "å¼·åº¦", "é¢¨éšª", "å»ºè­°"],
                            extra_func=pv_extra)
    prompt_stock_detail_report(codes)


# ==============================
# è¼”åŠ©åˆ¤æ–·å‡½æ•¸
# ==============================

def get_user_scan_params():
    """ç²å–ä½¿ç”¨è€…è¼¸å…¥çš„æƒæåƒæ•¸"""
    try:
        print("é¸æ“‡æª”æ•¸(é è¨­30æª”): ", end='', flush=True)
        l = sys.stdin.readline().strip()
        limit = int(l) if l else 30
    except:
        limit = 30
    
    try:
        print("å¤§æ–¼æˆäº¤é‡(é è¨­å¤§æ–¼100å¼µ): ", end='', flush=True)
        v = sys.stdin.readline().strip()
        min_vol_lots = int(v) if v else 100
        min_vol = min_vol_lots
    except:
        min_vol = 100
    
    return limit, min_vol

# ==============================
# é›²ç«¯åŒæ­¥ç®¡ç†å™¨
# ==============================
class CloudSync:
    """Supabase é›²ç«¯åŒæ­¥ç®¡ç†å™¨"""
    
    @staticmethod
    def get_headers():
        return {
            "apikey": SUPABASE_KEY,
            "Authorization": f"Bearer {SUPABASE_KEY}",
            "Content-Type": "application/json",
            "Prefer": "resolution=merge-duplicates"
        }

    @staticmethod
    def upload_stock_list():
        """ä¸Šå‚³è‚¡ç¥¨æ¸…å–®åˆ°é›²ç«¯"""
        if not ENABLE_CLOUD_SYNC:
            print_flush("âš  æœªè¨­å®š Supabaseï¼Œç„¡æ³•åŒæ­¥")
            return False
            
        print_flush("â˜ æ­£åœ¨ä¸Šå‚³è‚¡ç¥¨æ¸…å–®åˆ°é›²ç«¯...")
        
        try:
            df = pd.read_csv(STOCK_LIST_PATH, dtype=str)
            records = []
            
            for _, row in df.iterrows():
                records.append({
                    "code": row['code'],
                    "name": row['name'],
                    "market_type": row.get('market', 'æœªçŸ¥')
                })
            
            batch_size = 100
            total = len(records)
            
            for i in range(0, total, batch_size):
                batch = records[i:i+batch_size]
                url = f"{SUPABASE_URL}/rest/v1/stock_list"
                response = requests.post(url, headers=CloudSync.get_headers(), json=batch, verify=False)
                
                if response.status_code not in [200, 201]:
                    print_flush(f"âš  ä¸Šå‚³å¤±æ•— (æ‰¹æ¬¡ {i}): {response.text}")
                
                print_flush(f"\ré€²åº¦: {min(i+batch_size, total)}/{total}", end="")
            
            print_flush("\nâœ“ è‚¡ç¥¨æ¸…å–®ä¸Šå‚³å®Œæˆ")
            return True
            
        except Exception as e:
            print_flush(f"\nâŒ ä¸Šå‚³éŒ¯èª¤: {e}")
            return False

    @staticmethod
    def upload_calculated_data(days=None):
        """ä¸Šå‚³è¨ˆç®—çµæœåˆ°é›²ç«¯"""
        if not ENABLE_CLOUD_SYNC:
            print_flush("âš  æœªè¨­å®š Supabaseï¼Œç„¡æ³•åŒæ­¥")
            return False
            
        range_str = f"æœ€è¿‘ {days} å¤©" if days else "æ‰€æœ‰"
        print_flush(f"â˜ æ­£åœ¨ä¸Šå‚³ {range_str} æ•¸æ“šåˆ°é›²ç«¯...")
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                if days:
                    # ä½¿ç”¨ stock_snapshot (æ–°ä¸‰è¡¨æ¶æ§‹) - ä¾æ—¥æœŸæ’åº
                    sql = f"SELECT DISTINCT date FROM stock_snapshot ORDER BY date DESC LIMIT {days}"
                else:
                    sql = "SELECT DISTINCT date FROM stock_snapshot ORDER BY date DESC"
                
                cur.execute(sql)
                dates = [row[0] for row in cur.fetchall()]
                
                if not dates:
                    print_flush("âš  æœ¬åœ°ç„¡æ•¸æ“šå¯ä¸Šå‚³")
                    return False
                
                total_dates = len(dates)
                
                for idx, date in enumerate(dates):
                    print_flush(f"æ­£åœ¨è™•ç†æ—¥æœŸ: {date} ({idx+1}/{total_dates})")
                    
                    # å¾ stock_snapshot å–å¾—æœ€æ–°æŒ‡æ¨™è³‡æ–™
                    df = pd.read_sql_query("SELECT * FROM stock_snapshot WHERE date=?", conn, params=(date,))
                    
                    def clean_value(x):
                        if isinstance(x, bytes):
                            try:
                                return int.from_bytes(x, byteorder='little')
                            except:
                                return str(x)
                        return x

                    for col in df.columns:
                        if df[col].dtype == 'object':
                            df[col] = df[col].apply(clean_value)
                            
                    vol_cols = ['volume', 'vol_prev', 'volume_prev']
                    
                    for col in vol_cols:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                            df[col] = df[col].astype('Int64')
                            df[col] = df[col].apply(lambda x: int(x) if pd.notnull(x) else None)

                    records = df.to_dict(orient='records')
                    
                    batch_size = 500
                    total_recs = len(records)
                    
                    for i in range(0, total_recs, batch_size):
                        batch = records[i:i+batch_size]
                        url = f"{SUPABASE_URL}/rest/v1/stock_data"
                        response = requests.post(url, headers=CloudSync.get_headers(), json=batch, verify=False)
                        
                        if response.status_code not in [200, 201]:
                            print_flush(f"âš  ä¸Šå‚³å¤±æ•— ({date} æ‰¹æ¬¡ {i}): {response.text}")
            
            print_flush("\nâœ“ æ•¸æ“šä¸Šå‚³å®Œæˆ")
            return True
            
        except Exception as e:
            print_flush(f"\nâŒ ä¸Šå‚³éŒ¯èª¤: {e}")
            return False

    @staticmethod
    def upload_daily_history(date_int):
        """ä¸Šå‚³æŒ‡å®šæ—¥æœŸçš„ K ç·šæ•¸æ“šåˆ°é›²ç«¯ (å¢é‡æ›´æ–°)"""
        if not ENABLE_CLOUD_SYNC:
            return
            
        d_str = str(date_int)
        date_fmt = f"{d_str[:4]}-{d_str[4:6]}-{d_str[6:]}"
        print_flush(f"â˜ æ­£åœ¨åŒæ­¥ {date_fmt} çš„æ•¸æ“šåˆ° Supabase...")
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                cur.execute(f"""
                    SELECT code, date_int, open, high, low, close, volume, amount 
                    FROM stock_history 
                    WHERE date_int = ?
                """, (date_int,))
                rows = cur.fetchall()
                
                if not rows:
                    print_flush("âš  ç„¡æ•¸æ“šå¯ä¸Šå‚³")
                    return

                # è½‰æ›è³‡æ–™æ ¼å¼
                upload_data = []
                for r in rows:
                    upload_data.append({
                        "code": r[0],
                        "date": date_fmt,
                        "open": r[2],
                        "high": r[3],
                        "low": r[4],
                        "close": r[5],
                        "volume": r[6],
                        "amount": r[7]
                    })
                
                # åˆ†æ‰¹ä¸Šå‚³
                batch_size = 500
                total = len(upload_data)
                
                for i in range(0, total, batch_size):
                    batch = upload_data[i:i+batch_size]
                    # [Fix] åŠ å…¥ on_conflict=code,date
                    url = f"{SUPABASE_URL}/rest/v1/stock_data?on_conflict=code,date"
                    
                    try:
                        headers = CloudSync.get_headers()
                        headers["Prefer"] = "resolution=merge-duplicates"
                        
                        res = requests.post(url, headers=headers, json=batch, verify=False, timeout=30)
                        if res.status_code not in [200, 201]:
                            # å˜—è©¦ PATCH
                            print_flush("x", end="")
                        else:
                            print_flush(".", end="")
                    except Exception:
                        print_flush("t", end="")
                        
            print_flush(" âœ“")
            
        except Exception as e:
            print_flush(f" âœ— ä¸Šå‚³éŒ¯èª¤: {e}")

    @staticmethod
    def upload_all_history():
        """ä¸Šå‚³æ‰€æœ‰æ­·å² K ç·šæ•¸æ“šåˆ°é›²ç«¯"""
        if not ENABLE_CLOUD_SYNC:
            print_flush("âš  æœªè¨­å®š Supabaseï¼Œç„¡æ³•åŒæ­¥")
            return
            
        # ä¸‰è¡Œé€²åº¦åˆå§‹åŒ–
        print_flush("\n" * 3) # é ç•™ç©ºé–“
        
        try:
            with db_manager.get_connection() as conn:
                cur = conn.cursor()
                
                # ç²å–ç¸½ç­†æ•¸
                cur.execute("SELECT COUNT(*) FROM stock_history")
                total_count = cur.fetchone()[0]
                
                # åˆ†é è®€å–èˆ‡ä¸Šå‚³
                batch_size = 2000 # æœ¬åœ°è®€å–æ‰¹æ¬¡
                upload_batch_size = 500 # ä¸Šå‚³æ‰¹æ¬¡
                offset = 0
                success_count = 0
                fail_count = 0
                last_error = ""
                
                start_time = time.time()
                
                while offset < total_count:
                    cur.execute(f"""
                        SELECT code, date_int, open, high, low, close, volume, amount 
                        FROM stock_history 
                        LIMIT {batch_size} OFFSET {offset}
                    """)
                    rows = cur.fetchall()
                    if not rows:
                        break
                        
                    # è½‰æ›è³‡æ–™æ ¼å¼
                    upload_data = []
                    for r in rows:
                        d_str = str(r[1])
                        date_fmt = f"{d_str[:4]}-{d_str[4:6]}-{d_str[6:]}"
                        upload_data.append({
                            "code": r[0],
                            "date": date_fmt,
                            "open": r[2],
                            "high": r[3],
                            "low": r[4],
                            "close": r[5],
                            "volume": r[6],
                            "amount": r[7]
                        })
                    
                    # åˆ†æ‰¹ä¸Šå‚³åˆ° Supabase
                    for i in range(0, len(upload_data), upload_batch_size):
                        batch = upload_data[i:i+upload_batch_size]
                        
                        # [Fix] åŠ å…¥ on_conflict=code,date ä»¥è§£æ±º 409 éŒ¯èª¤
                        # ç•¶ PK ä¸æ˜¯ (code, date) ä½†æœ‰å”¯ä¸€ç´„æŸæ™‚ï¼Œå¿…é ˆæ˜ç¢ºæŒ‡å®š
                        url = f"{SUPABASE_URL}/rest/v1/stock_data?on_conflict=code,date"
                        
                        try:
                            headers = CloudSync.get_headers()
                            headers["Prefer"] = "resolution=merge-duplicates"
                            
                            res = requests.post(url, headers=headers, json=batch, verify=False, timeout=30)
                            if res.status_code not in [200, 201]:
                                fail_count += len(batch)
                                last_error = f"[{res.status_code}] {res.text[:50]}..."
                            else:
                                success_count += len(batch)
                                last_error = ""
                        except Exception as e:
                            fail_count += len(batch)
                            last_error = str(e)[:50]
                            
                    offset += batch_size
                    
                    # ç°¡åŒ–é€²åº¦é¡¯ç¤º (ç§»é™¤é€²åº¦æ¢)
                    percent = int(offset / total_count * 100)
                    
                    # ANSI Escape Codes
                    UP = "\033[2A" # ä¸Šç§»2è¡Œ
                    CLR = "\033[K" # æ¸…é™¤è¡Œ
                    
                    status_line = f"ç‹€æ…‹: æˆåŠŸ {success_count} | å¤±æ•— {fail_count}"
                    if last_error:
                        status_line += f" | âš  {last_error}"
                        
                    print(f"{UP}{CLR}ã€å…¨é‡ä¸Šå‚³ã€‘é€²åº¦: {percent}% ({offset}/{total_count})")
                    print(f"{CLR}{status_line}")
                    
            print_flush("\nâœ“ æ­·å²è³‡æ–™ä¸Šå‚³å®Œæˆ")
            
        except Exception as e:
            print_flush(f"\nâŒ ä¸Šå‚³éŒ¯èª¤: {e}")

# ==============================
# ç³»çµ±ç¶­è­·å‡½æ•¸
# ==============================
def backup_menu():
    """è³‡æ–™åº«å‚™ä»½èˆ‡é‚„åŸé¸å–®"""
    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€è³‡æ–™åº«å‚™ä»½èˆ‡é‚„åŸã€‘")
        print_flush("="*60)
        print_flush("[1] å‚™ä»½è³‡æ–™åº«")
        print_flush("[2] é‚„åŸè³‡æ–™åº«")
        print_flush("[3] åˆ—å‡ºç¾æœ‰å‚™ä»½")
        print_flush("[0] è¿”å›")
        
        choice = read_single_key("è«‹é¸æ“‡: ")
        
        if choice == '1':
            try:
                import shutil
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = BACKUP_DIR / f"taiwan_stock_backup_{timestamp}.db"
                shutil.copy2(DB_FILE, backup_file)
                print_flush(f"âœ“ å‚™ä»½æˆåŠŸ: {backup_file}")
            except Exception as e:
                print_flush(f"âŒ å‚™ä»½å¤±æ•—: {e}")
        
        elif choice == '2':
            backups = sorted(BACKUP_DIR.glob("*.db"), reverse=True)
            
            if not backups:
                print_flush("âŒ æ²’æœ‰å¯ç”¨çš„å‚™ä»½æª”æ¡ˆ")
                continue
            
            print_flush("\nå¯ç”¨å‚™ä»½:")
            for i, b in enumerate(backups[:10], 1):
                size_mb = b.stat().st_size / (1024*1024)
                print_flush(f"  [{i}] {b.name} ({size_mb:.2f} MB)")
            
            try:
                idx = int(input("è«‹é¸æ“‡è¦é‚„åŸçš„å‚™ä»½ (è¼¸å…¥æ•¸å­—): ").strip()) - 1
                
                if 0 <= idx < len(backups):
                    import shutil
                    shutil.copy2(backups[idx], DB_FILE)
                    print_flush(f"âœ“ é‚„åŸæˆåŠŸ: {backups[idx].name}")
                else:
                    print_flush("âŒ ç„¡æ•ˆçš„é¸æ“‡")
            except Exception as e:
                print_flush(f"âŒ é‚„åŸå¤±æ•—: {e}")
        
        elif choice == '3':
            backups = sorted(BACKUP_DIR.glob("*.db"), reverse=True)
            
            if not backups:
                print_flush("âŒ æ²’æœ‰å‚™ä»½æª”æ¡ˆ")
            else:
                print_flush(f"\næ‰¾åˆ° {len(backups)} å€‹å‚™ä»½:")
                for b in backups[:20]:
                    size_mb = b.stat().st_size / (1024*1024)
                    print_flush(f"  â€¢ {b.name} ({size_mb:.2f} MB)")
        
        elif choice == '0':
            break

def check_db_nulls():
    """æª¢æŸ¥è³‡æ–™åº«ç©ºå€¼ç‡ (æ’é™¤æ–°ä¸Šå¸‚è‚¡ç¥¨å½±éŸ¿)"""
    print_flush("\n[æª¢æŸ¥] è³‡æ–™åº«ç©ºå€¼ç‡åˆ†æ (å¿«ç…§è¡¨)...")
    
    try:
        with db_manager.get_connection() as conn:
            cursor = conn.cursor()
            
            # 0. é å…ˆè¼‰å…¥ä¸Šå¸‚æ—¥æœŸ
            list_date_map = {}
            try:
                cursor.execute("SELECT code, list_date FROM stock_meta")
                for r in cursor.fetchall():
                    if r[1]: list_date_map[r[0]] = r[1]
            except:
                pass

            cursor.execute("PRAGMA table_info(stock_snapshot)")
            columns = [row[1] for row in cursor.fetchall()]
            
            cursor.execute("SELECT COUNT(*) FROM stock_snapshot")
            total_rows = cursor.fetchone()[0]
            
            if total_rows == 0:
                print_flush("âŒ å¿«ç…§è¡¨ç„¡æ•¸æ“š")
                return

            print_flush(f"åˆ†æç¯„åœ: æœ€æ–°å¿«ç…§ ({total_rows} ç­†)")
            print_flush("-" * 60)
            print_flush(f"{'æ¬„ä½åç¨±':<20} | {'ç©ºå€¼ç‡%':<12} | {'ç‹€æ…‹':<10}")
            print_flush("-" * 60)
            
            # å®šç¾©é•·å¤©æœŸæŒ‡æ¨™æ‰€éœ€çš„æœ€å°å¤©æ•¸
            required_days_map = {
                'ma200': 200, 'wma200': 200, 'vwap200': 200, 'ma200_prev': 200, 'wma200_prev': 200,
                'ma120': 120, 'wma120': 120, 'ma120_prev': 120, 'wma120_prev': 120,
                'ma60': 60, 'wma60': 60, 'vwap60': 60, 'vol_ma60': 60, 'ma60_prev': 60, 'wma60_prev': 60,
                'ma25': 25, 'ma25_slope': 25,
                'ma20': 20, 'wma20': 20, 'vwap20': 20, 'ma20_prev': 20, 'wma20_prev': 20, 'vwap20_prev': 20,
                'ma3': 3, 'wma3': 3, 'ma3_prev': 3, 'wma3_prev': 3,
                'rsi': 14, 'rsi12': 12, 'mfi14': 14, 'mfi14_prev': 14,
                'macd': 26, 'macd_signal': 26, 'macd_diff': 26,
                'kdj_k': 9, 'kdj_d': 9, 'kdj_j': 9,
                'week_k': 35, 'week_d': 35, # é€±ç·šéœ€è¦æ›´å¤šæ—¥è³‡æ–™
                'month_k': 150, 'month_d': 150 # æœˆç·šéœ€è¦æ›´å¤šæ—¥è³‡æ–™
            }

            for col in columns:
                if col in ['code', 'name', 'date']:
                    continue
                
                # ç™½åå–®é©—è­‰
                if col not in columns:
                    continue
                
                # æŸ¥è©¢ç©ºå€¼çš„è‚¡ç¥¨ä»£ç¢¼
                cursor.execute(f"SELECT code FROM stock_snapshot WHERE {col} IS NULL")
                null_codes = [r[0] for r in cursor.fetchall()]
                raw_null_count = len(null_codes)
                
                if raw_null_count == 0:
                    print_flush(f"{col:<20} | 0.00%       | OK")
                    continue

                # åˆ†æç©ºå€¼åŸå›  (æ˜¯å¦ç‚ºæ–°è‚¡)
                real_missing_count = 0
                new_stock_count = 0
                req_days = required_days_map.get(col, 0)
                
                for code in null_codes:
                    is_new_stock = False
                    if req_days > 0:
                        l_date_str = list_date_map.get(code)
                        if l_date_str:
                            try:
                                l_date = datetime.strptime(l_date_str, '%Y-%m-%d')
                                days_since = (datetime.now() - l_date).days
                                # å¯¬é™æœŸ: éœ€æ±‚å¤©æ•¸ * 1.5 (è€ƒæ…®å‡æ—¥)
                                if days_since < req_days * 1.5:
                                    is_new_stock = True
                            except:
                                pass
                    
                    if is_new_stock:
                        new_stock_count += 1
                    else:
                        real_missing_count += 1
                
                # è¨ˆç®—èª¿æ•´å¾Œçš„ç©ºå€¼ç‡ (åªè¨ˆç®—çœŸæ­£ç¼ºå¤±çš„)
                real_null_pct = (real_missing_count / total_rows) * 100
                
                status = "OK"
                if real_null_pct > 20:
                    if col in ['pe', 'yield']:
                        status = "ç„¡ (è™§æ/ç„¡è‚¡åˆ©)"
                    else:
                        status = "ç¼ºè³‡æ–™ (!)"
                elif real_null_pct > 0:
                    if col in ['pe', 'yield']:
                         status = "éƒ¨åˆ†ç„¡ (æ­£å¸¸)"
                    else:
                        status = "éƒ¨åˆ†ç¼º"
                elif new_stock_count > 0:
                    status = "OK (å«æ–°è‚¡)"
                
                # é¡¯ç¤ºé‚è¼¯: å¦‚æœæœ‰æ–°è‚¡è¢«æ’é™¤ï¼Œé¡¯ç¤ºè¨»è¨˜
                display_pct = f"{real_null_pct:.2f}%"
                if new_stock_count > 0 and real_missing_count == 0:
                     display_pct = "0.00%*"
                
                print_flush(f"{col:<20} | {display_pct:<10} | {status}")
            
            # é¡å¤–æª¢æŸ¥: æˆäº¤é‡‘é¡ (æœ€æ–°äº¤æ˜“æ—¥ï¼Œæ’é™¤æˆäº¤é‡ç‚º0çš„è‚¡ç¥¨)
            try:
                cursor.execute("""
                    SELECT COUNT(*) FROM stock_history 
                    WHERE date_int = (SELECT MAX(date_int) FROM stock_history)
                    AND volume > 0 
                    AND (amount IS NULL OR amount = 0)
                """)
                amount_null = cursor.fetchone()[0]
                amount_pct = (amount_null / total_rows) * 100
                st = "OK" if amount_pct == 0 else "ç¼ºè³‡æ–™ (!)"
                print_flush(f"{'amount (æœ€æ–°)':<20} | {amount_pct:<10.2f}% | {st}")
            except:
                print_flush(f"{'amount (æœ€æ–°)':<20} | {'N/A':<10} | æª¢æŸ¥å¤±æ•—")

            # é¡å¤–æª¢æŸ¥: æ³•äººè³‡æ–™ (æœ€æ–°äº¤æ˜“æ—¥)
            try:
                cursor.execute("SELECT MAX(date_int) FROM institutional_investors")
                max_inst_date = cursor.fetchone()[0]
                if max_inst_date:
                    cursor.execute(f"""
                        SELECT COUNT(*) FROM stock_snapshot
                        WHERE code NOT IN (
                            SELECT code FROM institutional_investors WHERE date_int = {max_inst_date}
                        )
                    """)
                    inst_null = cursor.fetchone()[0]
                    inst_pct = (inst_null / total_rows) * 100
                    st = "ç„¡äº¤æ˜“ (æ­£å¸¸)" if inst_pct > 0 else "OK"
                    print_flush(f"{'æ³•äººè³‡æ–™ (æœ€æ–°)':<20} | {inst_pct:<10.2f}% | {st}")
                else:
                    print_flush(f"{'æ³•äººè³‡æ–™ (æœ€æ–°)':<20} | {'100.00%':<10} | ç„¡è³‡æ–™")
            except:
                print_flush(f"{'æ³•äººè³‡æ–™ (æœ€æ–°)':<20} | {'N/A':<10} | æª¢æŸ¥å¤±æ•—")
            
            # é¡å¤–æª¢æŸ¥: èè³‡èåˆ¸è³‡æ–™
            try:
                cursor.execute("SELECT COUNT(DISTINCT date_int) FROM margin_data")
                margin_days = cursor.fetchone()[0]
                target_days = 450
                margin_pct = ((target_days - margin_days) / target_days) * 100 if margin_days < target_days else 0
                st = "OK" if margin_days >= target_days else f"å·® {target_days - margin_days} å¤©"
                print_flush(f"{'èè³‡èåˆ¸ (å¤©æ•¸)':<20} | {margin_days:<10} | {st}")
            except:
                print_flush(f"{'èè³‡èåˆ¸ (å¤©æ•¸)':<20} | {'N/A':<10} | æª¢æŸ¥å¤±æ•—")
            
            # é¡å¤–æª¢æŸ¥: å¤§ç›¤æŒ‡æ•¸è³‡æ–™
            try:
                cursor.execute("SELECT COUNT(DISTINCT date_int) FROM market_index")
                index_days = cursor.fetchone()[0]
                target_days = 450
                st = "OK" if index_days >= target_days else f"å·® {target_days - index_days} å¤©"
                print_flush(f"{'å¤§ç›¤æŒ‡æ•¸ (å¤©æ•¸)':<20} | {index_days:<10} | {st}")
            except:
                print_flush(f"{'å¤§ç›¤æŒ‡æ•¸ (å¤©æ•¸)':<20} | {'N/A':<10} | æª¢æŸ¥å¤±æ•—")
                
            print_flush("-" * 60)
            print_flush("èªªæ˜:")
            print_flush("1. [0.00%*] ä»£è¡¨ç©ºå€¼çš†ä¾†è‡ªã€Œæ–°ä¸Šå¸‚è‚¡ç¥¨ã€(ä¸Šå¸‚å¤©æ•¸ä¸è¶³ä»¥è¨ˆç®—è©²æŒ‡æ¨™)ï¼Œå±¬æ­£å¸¸ç¾è±¡ã€‚")
            print_flush("2. [PE/Yield] ç©ºå€¼ä»£è¡¨å…¬å¸è™§ææˆ–ä¸ç™¼è‚¡åˆ©ï¼Œå±¬æ­£å¸¸ç¾è±¡ã€‚")
            print_flush("3. [æ³•äººè³‡æ–™] ç©ºå€¼ä»£è¡¨ç•¶æ—¥ä¸‰å¤§æ³•äººç„¡è²·è³£ç´€éŒ„ï¼Œå±¬æ­£å¸¸ç¾è±¡ã€‚")
            print_flush("4. [Amount] å·²æ’é™¤æˆäº¤é‡ç‚º 0 ä¹‹è‚¡ç¥¨ã€‚")
            
            print_flush("\n" + "="*50)
            ans = input("æ˜¯å¦ç«‹å³åŸ·è¡Œ [1]~[7] å®Œæ•´æ›´æ–°ä»¥ä¿®å¾©ç¼ºå¤±æ•¸æ“šï¼Ÿ (y/N, é è¨­n): ").strip().lower()
            
            if ans == 'y':
                step1_fetch_stock_list()
                updated_codes = set()
                
                s2 = step2_download_tpex_daily()
                if isinstance(s2, set):
                    updated_codes.update(s2)
                
                s3 = step3_download_twse_daily()
                if isinstance(s3, set):
                    updated_codes.update(s3)
                
                step5_clean_delisted()
                step4_check_data_gaps()
                data = step4_load_data()
                step6_verify_and_backfill(data, resume=True)
                step7_calc_indicators(data, force=True)
                
                global GLOBAL_INDICATOR_CACHE
                if GLOBAL_INDICATOR_CACHE:
                    GLOBAL_INDICATOR_CACHE.clear()
                print_flush("[OK] ç³»çµ±å¿«å–å·²æ¸…é™¤ï¼Œæ›´æ–°å®Œæˆ")
            else:
                print_flush("[INFO] å·²è·³éæ›´æ–°")
            
    except Exception as e:
        print_flush(f"âŒ æª¢æŸ¥å¤±æ•—: {e}")

def delete_data_by_date():
    """åˆªé™¤æŒ‡å®šæ—¥æœŸçš„è³‡æ–™"""
    print_flush("\nã€åˆªé™¤æŒ‡å®šæ—¥æœŸè³‡æ–™ã€‘")
    print_flush("-" * 40)
    
    try:
        date_str = input("è«‹è¼¸å…¥è¦åˆªé™¤çš„æ—¥æœŸ (æ ¼å¼: YYYY-MM-DD): ").strip()
        
        try:
            datetime.strptime(date_str, "%Y-%m-%d")
        except ValueError:
            print_flush("âŒ æ—¥æœŸæ ¼å¼éŒ¯èª¤ï¼Œè«‹ä½¿ç”¨ YYYY-MM-DD æ ¼å¼")
            return
        
        date_int = int(date_str.replace('-', ''))
        
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            
            # çµ±ä¸€ä½¿ç”¨æ–°ä¸‰è¡¨æ¶æ§‹
            cur.execute("SELECT COUNT(*) FROM stock_history WHERE date_int=?", (date_int,))
            count_history = cur.fetchone()[0]
        
        if count_history == 0:
            print_flush(f"âš  æ—¥æœŸ {date_str} æ²’æœ‰ä»»ä½•è³‡æ–™")
            return
        
        print_flush(f"[INFO] åˆªé™¤ {date_str} çš„è³‡æ–™ ({count_history} ç­†)")
        
        with db_manager.get_connection() as conn:
            cur = conn.cursor()
            cur.execute("DELETE FROM stock_history WHERE date_int=?", (date_int,))
            conn.commit()
        
        print_flush(f"[OK] å·²åˆªé™¤ {date_str} çš„æ‰€æœ‰è³‡æ–™")
    
    except Exception as e:
        print_flush(f"âŒ åˆªé™¤å¤±æ•—: {e}")

# ==============================
# é¸å–®ç³»çµ±




# ==============================
# VSBC ç­–ç•¥ (Inserted)
# ==============================





# ==============================
# 1ï¸âƒ£ VSBC è¨ˆç®—ï¼ˆæ ¸å¿ƒï¼‰
# ==============================
def calc_vsbc(df, win=10):
    """
    è¨ˆç®— VSBC ä¸­ç·šï¼ˆvsbc_midï¼‰åŠç®±é«”åŸºç¤ç¯„åœï¼ˆbase_rangeï¼‰
    win: æ»¾å‹•è¦–çª—å¤§å°
    """
    # è¨ˆç®—æˆäº¤é‡æƒ…ç·’
    signed_vol = np.where(df['close'] >= df['open'],
                          df['volume'],
                          -df['volume'])
    signed_vol = pd.Series(signed_vol, index=df.index)

    # æƒ…ç·’æ¨åŠ›å¹³å‡ & å¹³å‡æˆäº¤é‡
    vs_force = signed_vol.rolling(win, min_periods=1).mean()
    vol_mean = df['volume'].rolling(win, min_periods=1).mean()

    # ç®±é«”åŸºç¤
    base_mid = (df['high'] + df['low']) / 2
    base_range = (df['high'] - df['low']).rolling(win, min_periods=1).mean().replace(0, 1e-9)

    # ä¸­ç·šä½ç§»ï¼ˆé˜²çˆ†ç¯„åœ -0.5 ~ 0.5ï¼‰
    shift = (vs_force / vol_mean).fillna(0).clip(-0.5, 0.5)

    vsbc_mid = base_mid + shift * base_range
    return vsbc_mid, base_range


# ==============================
# 2ï¸âƒ£ è¨ˆç®— VSBC åˆ†æ•¸ï¼ˆæ•¸å€¼åŒ–ï¼‰
# ==============================
def compute_vsbc_score(df, win=10, n_recent=3, scale=100):
    """
    è¨ˆç®— VSBC åˆ†æ•¸ï¼ˆå¯æ’åºï¼‰
    è¿”å›ï¼š
        score: -scale~scaleï¼Œæ­£æ•¸ç‚ºå¤šæ–¹ï¼Œè² æ•¸ç‚ºç©ºæ–¹
    """
    vsbc_mid, base_range = calc_vsbc(df, win)
    diffs = vsbc_mid.diff().iloc[-n_recent:]

    up_count = (diffs > 0).sum()
    down_count = (diffs < 0).sum()

    if up_count > down_count:
        direction = 1
    elif down_count > up_count:
        direction = -1
    else:
        direction = 0

    magnitude = abs(diffs.mean()) / (base_range.iloc[-1] + 1e-9)
    consistency = max(up_count, down_count) / n_recent

    score = direction * magnitude * consistency * scale
    return score





# ==============================
# 1ï¸âƒ£ åŸºç¤æ¨¡çµ„ (å‡ç·š & VP/POC)
# ==============================
def add_ma(df):
    df = df.copy()
    df['MA20'] = df['close'].rolling(20).mean()
    df['MA60'] = df['close'].rolling(60).mean()
    return df

def calc_vp_poc(df, window=60, bins=30):
    """è¨ˆç®— Volume Profile POC (Point of Control)"""
    sub = df.tail(window)
    if len(sub) < 2:
        return df['close'].iloc[-1]
        
    hist, edges = np.histogram(
        sub['close'],
        bins=bins,
        weights=sub['volume']
    )
    i = hist.argmax()
    return (edges[i] + edges[i+1]) / 2

# ==============================
# 2ï¸âƒ£ VSBC åºåˆ—è¨ˆç®— (ä¾›å¾ŒçºŒä½¿ç”¨)
# ==============================
def calc_vsbc_series(df, win=10, n_recent=3, scale=100):
    """
    è¨ˆç®— VSBC åºåˆ— (vsbc) èˆ‡ ç™¾åˆ†ä½ (vsbc_pct)
    """
    # 1. è¨ˆç®— VSBC ä¸­ç·šèˆ‡ç¯„åœ
    vsbc_mid, base_range = calc_vsbc(df, win)
    
    # 2. è¨ˆç®— diffs (åºåˆ—)
    diffs = vsbc_mid.diff()
    
    # 3. è¨ˆç®—æ¯å€‹æ™‚é–“é»çš„ score (éœ€å‘é‡åŒ–æˆ– rolling)
    # ç”±æ–¼ compute_vsbc_score æ˜¯é‡å°æœ€å¾Œ n_recent é»ï¼Œé€™è£¡æˆ‘å€‘éœ€è¦ä¸€å€‹ rolling version
    # ç°¡åŒ–ç‰ˆ: ä½¿ç”¨ rolling apply æˆ–å‘é‡åŒ–è¿‘ä¼¼
    # Score = direction * magnitude * consistency * scale
    
    # Direction: rolling count of ups vs downs
    diff_sign = np.sign(diffs)
    up_counts = (diff_sign > 0).rolling(n_recent).sum()
    down_counts = (diff_sign < 0).rolling(n_recent).sum()
    
    direction = np.where(up_counts > down_counts, 1, 
                         np.where(down_counts > up_counts, -1, 0))
    
    # Magnitude: abs(mean diff) / base_range
    mag_num = diffs.abs().rolling(n_recent).mean()
    mag_denom = base_range + 1e-9
    magnitude = mag_num / mag_denom
    
    # Consistency: max(up, down) / n_recent
    consistency = np.maximum(up_counts, down_counts) / n_recent
    
    # Final Score Series
    vsbc_series = direction * magnitude * consistency * scale
    
    return pd.Series(vsbc_series, index=df.index).fillna(0)

def add_vsbc_columns(df):
    """åŠ å…¥ vsbc èˆ‡ vsbc_pct æ¬„ä½"""
    df = df.copy()
    
    # è¨ˆç®— VSBC åˆ†æ•¸åºåˆ—
    df['vsbc'] = calc_vsbc_series(df)
    
    # è¨ˆç®— VSBC ç™¾åˆ†ä½ (Rolling 100 days rank)
    # Rank pct=True returns 0.0 to 1.0, multiply by 100
    df['vsbc_pct'] = df['vsbc'].rolling(100, min_periods=20).rank(pct=True) * 100
    df['vsbc_pct'] = df['vsbc_pct'].fillna(50) # Default mid
    
    return df

# ==============================
# 3ï¸âƒ£ è¡Œç‚ºé‡åŒ–ï¼ˆå¤šæ–¹ï¼‰
# ==============================
def vsbc_behavior_score(df):
    t = df.iloc[-1]
    y = df.iloc[-2]

    return (
        (t['vsbc_pct'] - 50) * 2 +
        (t['vsbc'] - y['vsbc']) / max(abs(y['vsbc']), 1) * 100
    )

def cost_shift_score(close, poc):
    return (close - poc) / poc * 100

# ==============================
# 4ï¸âƒ£ å¤šæ–¹è¡Œç‚ºåˆ¤æ–·å™¨ï¼ˆæ ¸å¿ƒï¼‰
# ==============================
def long_behavior(df):
    # ç¢ºä¿å¿…è¦æ¬„ä½å­˜åœ¨
    if 'MA20' not in df.columns:
        df = add_ma(df)
    if 'vsbc' not in df.columns or 'vsbc_pct' not in df.columns:
        df = add_vsbc_columns(df)

    if len(df) < 61:
        return False, None, None

    t = df.iloc[-1]
    y = df.iloc[-2]

    poc = calc_vp_poc(df)

    # æ¢ä»¶åˆ¤æ–·
    cond = (
        t['vsbc_pct'] >= 99 and
        t['vsbc'] > y['vsbc'] and
        t['close'] >= poc and
        t['MA20'] > t['MA60'] and
        t['close'] > t['MA20']
    )

    if not cond:
        return False, None, None

    score = (
        vsbc_behavior_score(df) * 0.6 +
        cost_shift_score(t['close'], poc) * 0.4
    )

    return True, round(score, 2), round(poc, 2)

def scan_vsbc_strategy():
    """VSBC å¤šæ–¹è¡Œç‚ºæƒæç­–ç•¥ (ä¸»åŠ›æ¨å‡)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨åŸ·è¡Œ VSBC å¤šæ–¹è¡Œç‚ºæƒæ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    print_flush("ç¯©é¸æ¢ä»¶: VSBC PR>=99, VSBCä¸Šå‡, ç«™ä¸ŠPOC, å¤šé ­æ’åˆ—, ç«™ä¸Šæœˆç·š")
    
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT code, name FROM stock_snapshot")
        stocks = cur.fetchall()
        
    codes = [s[0] for s in stocks]
    history_map = batch_load_history(codes, limit_days=150)
    
    results = []
    
    # Counters
    count_total = len(stocks)
    count_data = 0
    count_vol = 0
    count_pr99 = 0
    count_vsbc_up = 0
    count_poc = 0
    count_ma_bull = 0
    count_price_ma20 = 0
    
    for code, name in stocks:
        df = history_map.get(code)
        if df is None or len(df) < 100:
            continue
        count_data += 1
            
        try:
            # 1. Volume Filter
            if df['volume'].iloc[-1] < min_vol * 1000:
                continue
            count_vol += 1
            
            # 2. Prepare Data
            df = add_ma(df)
            df = add_vsbc_columns(df)
            
            t = df.iloc[-1]
            y = df.iloc[-2]
            poc = calc_vp_poc(df)
            
            # 3. Sequential Filtering
            if t['vsbc_pct'] < 99: continue
            count_pr99 += 1
            
            if t['vsbc'] <= y['vsbc']: continue
            count_vsbc_up += 1
            
            if t['close'] < poc: continue
            count_poc += 1
            
            if t['MA20'] <= t['MA60']: continue
            count_ma_bull += 1
            
            if t['close'] <= t['MA20']: continue
            count_price_ma20 += 1
            
            # Passed All
            score = (
                vsbc_behavior_score(df) * 0.6 +
                cost_shift_score(t['close'], poc) * 0.4
            )
            
            vol_ma60 = df['volume'].tail(60).mean()
            vol_ratio = t['volume'] / vol_ma60 if vol_ma60 > 0 else 0
            ma20_bias = ((t['close'] - t['MA20']) / t['MA20']) * 100 if t['MA20'] > 0 else 0
            
            results.append({
                'code': code, 'name': name,
                'close': t['close'], 'close_prev': df.iloc[-2]['close'],
                'vsbc': t['vsbc'],
                'vsbc_pct': t['vsbc_pct'],
                'poc': poc,
                'behavior_score': score,
                'vol_ratio': vol_ratio,
                'volume': t['volume'], # Keep as raw volume
                'ma20_bias': ma20_bias
            })

        except Exception as e:
            continue
            
    # Sort
    results.sort(key=lambda x: x['behavior_score'], reverse=True)
    
    # Summary
    print_flush("\n" + "="*60)
    print_flush("[ç¯©é¸éç¨‹] VSBC å¤šæ–¹è¡Œç‚ºæƒæ")
    print_flush("="*60)
    print_flush(f"ç¸½è‚¡æ•¸: {count_total}")
    print_flush("â”€"*60)
    print_flush(f"âœ“ è³‡æ–™å……è¶³ (>100æ—¥)       â†’ {count_data} æª”")
    print_flush(f"âœ“ æˆäº¤é‡ >= {min_vol}å¼µ        â†’ {count_vol} æª”")
    print_flush(f"âœ“ VSBC PR >= 99           â†’ {count_pr99} æª”")
    print_flush(f"âœ“ VSBC æ•¸å€¼ä¸Šå‡           â†’ {count_vsbc_up} æª”")
    print_flush(f"âœ“ è‚¡åƒ¹ç«™ä¸Š POC            â†’ {count_poc} æª”")
    print_flush(f"âœ“ å‡ç·šå¤šé ­ (MA20>MA60)    â†’ {count_ma_bull} æª”")
    print_flush(f"âœ“ è‚¡åƒ¹ç«™ä¸Š MA20           â†’ {count_price_ma20} æª” (æœ€çµ‚é¸å‡º)")
    print_flush("â”€"*60)
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def vsbc_extra(code, item):
        ma20_bias = item.get('ma20_bias', 0)
        return [f"{ma20_bias:+.2f}%"]

    codes = display_scan_results_v2(results, "VSBC å¤šæ–¹è¡Œç‚ºæƒæ", limit=limit,
                            extra_headers=["MA20ä¹–é›¢"],
                            extra_func=vsbc_extra)
    
    prompt_stock_detail_report(codes)




def calculate_2560_strategy(df):
    """
    ä¿®æ­£å¾Œçš„ 2560 æˆ°æ³•ä¿¡è™Ÿç”Ÿæˆå‡½æ•¸
    """
    # ç¢ºä¿æ¬„ä½åç¨±ä¸€è‡´ (è½‰æ›ç‚ºå°å¯«ä»¥ç¬¦åˆç³»çµ±æ…£ä¾‹)
    # ç³»çµ±æ…£ä¾‹: close, open, high, low, volume
    df = df.copy()
    
    # Map system columns to strategy expected columns if needed, or just use system columns
    # Strategy uses: Close, Open, Volume
    # System uses: close, open, volume
    
    # 1. è¨ˆç®—åŸºç¤æŒ‡æ¨™
    df['ma25'] = df['close'].rolling(window=25).mean()
    df['vol_ma5'] = df['volume'].rolling(window=5).mean()
    df['vol_ma60'] = df['volume'].rolling(window=60).mean()
    
    # è¨ˆç®— 25MA çš„æ–œç‡ (ä»Šæ—¥ - æ˜¨æ—¥)
    df['ma25_slope'] = df['ma25'].diff()
    
    # 2. å®šç¾©é‚è¼¯æ¢ä»¶
    
    # (A) è¶¨å‹¢æ¢ä»¶ï¼šè‚¡åƒ¹åœ¨ç·šä¸Šï¼Œä¸”ç·šå‘ä¸Š
    # åš´æ ¼æ¨¡å¼ï¼šè¦æ±‚æ–œç‡å¤§æ–¼æŸå€‹å¾®å°é–¾å€¼ï¼Œé¿å…èµ°å¹³
    cond_trend = (df['close'] > df['ma25']) & (df['ma25_slope'] > 0)
    
    # (B) è§¸ç™¼æ¢ä»¶ï¼šå‡é‡ç·šé‡‘å‰ (Crossover)
    # ä½¿ç”¨ shift(1) ä¾†æ¯”è¼ƒæ˜¨æ—¥ç‹€æ…‹ï¼Œç¢ºèªæ˜¯"äº¤å‰"å‹•ä½œç™¼ç”Ÿåœ¨ä»Šæ—¥
    cond_vol_cross = (df['vol_ma5'] > df['vol_ma60']) & (df['vol_ma5'].shift(1) <= df['vol_ma60'].shift(1))
    
    # (C) é©—è­‰æ¢ä»¶ (ä¿®æ­£å‡æ‹é»)ï¼šå¿…é ˆæ˜¯é™½ç·šä¸”ä¸Šæ¼²
    # é€™æ˜¯ Video _q-eVVBLbE4 å¼·èª¿çš„é—œéµä¿®æ­£
    cond_validation = (df['close'] > df['open']) & (df['close'] > df['close'].shift(1))
    
    # (D) ä¹–é›¢ç‡éæ¿¾ (é˜²æ­¢è¿½é«˜ - è·‘æ­¥å°äººé‚è¼¯çš„é€†æ‡‰ç”¨)
    # å‡è¨­æˆ‘å€‘ä¸å¸Œæœ›åœ¨é›¢ 25MA è¶…é 10% çš„åœ°æ–¹é€²å ´
    cond_proximity = (df['close'] < df['ma25'] * 1.10)
    
    # 3. ç¶œåˆä¿¡è™Ÿç”Ÿæˆ
    # åªæœ‰åŒæ™‚æ»¿è¶³æ‰€æœ‰æ¢ä»¶æ™‚ï¼Œæ‰æ¨™è¨˜ç‚º Buy Signal (1)
    df['signal_2560'] = 0
    df.loc[cond_trend & cond_vol_cross & cond_validation & cond_proximity, 'signal_2560'] = 1
    
    return df

def scan_2560_strategy():
    """2560 æˆ°æ³•æƒæ (å‡é‡ç·šé‡‘å‰ + 25MAå¤šé ­)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨åŸ·è¡Œ 2560 æˆ°æ³•æƒæ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    print_flush("ç¯©é¸æ¢ä»¶: è‚¡åƒ¹>25MA, 25MAå‘ä¸Š, å‡é‡ç·šé‡‘å‰(5>60), é™½ç·šæ”¶æ¼², ä¹–é›¢<10%")
    
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT code, name FROM stock_snapshot")
        stocks = cur.fetchall()
        
    codes = [s[0] for s in stocks]
    history_map = batch_load_history(codes, limit_days=100)
    
    results = []
    
    # Counters
    count_total = len(stocks)
    count_data = 0
    count_vol = 0
    count_trend = 0
    count_cross = 0
    count_valid = 0
    count_prox = 0
    
    for code, name in stocks:
        df = history_map.get(code)
        if df is None or len(df) < 65:
            continue
        count_data += 1
            
        try:
            # 1. Volume Filter
            if df['volume'].iloc[-1] < min_vol * 1000:
                continue
            count_vol += 1
            
            # 2. Calculate Strategy
            df = calculate_2560_strategy(df)
            t = df.iloc[-1]
            
            # Check conditions
            is_trend = (t['close'] > t['ma25']) and (t['ma25_slope'] > 0)
            is_cross = (t['vol_ma5'] > t['vol_ma60']) and (df['vol_ma5'].iloc[-2] <= df['vol_ma60'].iloc[-2])
            is_valid = (t['close'] > t['open']) and (t['close'] > df['close'].iloc[-2])
            is_prox = (t['close'] < t['ma25'] * 1.10)
            
            if is_trend: count_trend += 1
            if is_trend and is_cross: count_cross += 1
            if is_trend and is_cross and is_valid: count_valid += 1
            if is_trend and is_cross and is_valid and is_prox: count_prox += 1
            
            if t['signal_2560'] == 1:
                vol_ratio = t['volume'] / df['volume'].tail(60).mean()
                
                results.append({
                    'code': code, 'name': name,
                    'close': t['close'], 'close_prev': df.iloc[-2]['close'],
                    'ma25': t['ma25'],
                    'vol_ma5': t['vol_ma5'] / 1000,
                    'vol_ma60': t['vol_ma60'] / 1000,
                    'vol_ratio': vol_ratio,
                    'volume': t['volume'] / 1000
                })
        except Exception as e:
            continue
            
    # Sort
    results.sort(key=lambda x: x['vol_ratio'], reverse=True)
    
    # Summary
    print_flush("\n" + "="*60)
    print_flush("[ç¯©é¸éç¨‹] 2560 æˆ°æ³• (åš´æ ¼ç‰ˆ)")
    print_flush("="*60)
    print_flush(f"ç¸½è‚¡æ•¸: {count_total}")
    print_flush("â”€"*60)
    print_flush(f"âœ“ è³‡æ–™å……è¶³ (>65æ—¥)        â†’ {count_data} æª”")
    print_flush(f"âœ“ æˆäº¤é‡ >= {min_vol}å¼µ        â†’ {count_vol} æª”")
    print_flush(f"âœ“ è¶¨å‹¢æ¢ä»¶ (è‚¡åƒ¹>25MAå‘ä¸Š) â†’ {count_trend} æª”")
    print_flush(f"âœ“ è§¸ç™¼æ¢ä»¶ (å‡é‡ç·šé‡‘å‰)   â†’ {count_cross} æª”")
    print_flush(f"âœ“ é©—è­‰æ¢ä»¶ (é™½ç·šæ”¶æ¼²)     â†’ {count_valid} æª”")
    print_flush(f"âœ“ ä¹–é›¢éæ¿¾ (ä¹–é›¢<10%)     â†’ {count_prox} æª” (æœ€çµ‚é¸å‡º)")
    print_flush("â”€"*60)
    
    if not results:
        print_flush("\næ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨ã€‚")
        return

    print_flush(f"\nã€2560 æˆ°æ³• TOPã€‘ (å‰ {limit} ç­†)")
    # Header: ä»£è™Ÿ åç¨± æ”¶ç›¤ æˆäº¤é‡(é‡æ¯”) MA25 é‡MA5 é‡MA60 è¨Šè™Ÿ
    header = f"{'ä»£è™Ÿ':<6} {'åç¨±':<8} {'æ”¶ç›¤':<10} {'æˆäº¤é‡(é‡æ¯”)':<16} {'MA25':<10} {'é‡MA5':<10} {'é‡MA60':<10} {'è¨Šè™Ÿ':<6}"
    print_flush(header)
    print_flush("-" * 90)
    
    reset = reset_color()
    for res in results[:limit]:
        c_price = get_trend_color(res['close'], res['close_prev'])
        price_str = f"{c_price}{res['close']:.2f}{reset}"
        
        # Format columns
        vol_str = f"{int(res['volume'])}å¼µ({res['vol_ratio']:.1f})"
        ma25_str = f"{res['ma25']:.2f}"
        vma5_str = f"{int(res['vol_ma5'])}"
        vma60_str = f"{int(res['vol_ma60'])}"
        signal_str = f"{Colors.RED}è²·å…¥{reset}"
        
        print_flush(f"{res['code']:<6} {res['name']:<8} {price_str:<19} {vol_str:<21} {ma25_str:<12} {vma5_str:<12} {vma60_str:<12} {signal_str:<15}")


def scan_candlestick_patterns():
    """K ç·šå‹æ…‹æƒæ (æ™¨æ˜Ÿ/å¤œæ˜Ÿ) - è©³ç´°æ¼æ–—ç‰ˆ"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ K ç·šå‹æ…‹ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    print_flush("ç¯©é¸æ¢ä»¶: æ™¨æ˜Ÿ(T-2é•·é»‘, T-1æ˜Ÿç·š, Té•·ç´…, çˆ†é‡), å¤œæ˜Ÿ(åä¹‹)")
    
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT code, name FROM stock_snapshot")
        stocks = cur.fetchall()
        
    codes = [s[0] for s in stocks]
    history_map = batch_load_history(codes, limit_days=10) 
    
    morning_stars = []
    evening_stars = []
    
    # Counters (Morning Star Funnel)
    count_total = len(stocks)
    count_vol = 0
    count_m_step1 = 0 # T-2 Long Black
    count_m_step2 = 0 # T-1 Star
    count_m_step3 = 0 # T Long Red
    count_m_step4 = 0 # Vol Surge
    count_m_final = 0
    
    # Counters (Evening Star - Simplified tracking)
    count_e_final = 0
    
    for code, name in stocks:
        df = history_map.get(code)
        if df is None or len(df) < 5:
            continue
            
        try:
            # 1. Volume Filter
            vol = df['volume'].iloc[-1]
            if vol < min_vol * 1000:
                continue
            count_vol += 1
            
            # Prepare Data for Manual Checking (Latest 3 days)
            # T (Today), T-1 (Yesterday), T-2 (Day before)
            c0, c1, c2 = df['close'].iloc[-1], df['close'].iloc[-2], df['close'].iloc[-3]
            o0, o1, o2 = df['open'].iloc[-1], df['open'].iloc[-2], df['open'].iloc[-3]
            h0, h1, h2 = df['high'].iloc[-1], df['high'].iloc[-2], df['high'].iloc[-3]
            l0, l1, l2 = df['low'].iloc[-1], df['low'].iloc[-2], df['low'].iloc[-3]
            v0, v1 = df['volume'].iloc[-1], df['volume'].iloc[-2]
            
            # Ranges
            range0 = h0 - l0
            range1 = h1 - l1
            range2 = h2 - l2
            body0 = abs(c0 - o0)
            body1 = abs(c1 - o1)
            body2 = abs(c2 - o2)
            
            # --- Morning Star Logic (Sequential) ---
            passed_m = False
            
            # Step 1: T-2 Long Black (Body > 0.6 * Range)
            is_long_black_2 = (c2 < o2) and (body2 > range2 * 0.6)
            if is_long_black_2:
                count_m_step1 += 1
                
                # Step 2: T-1 Star (Body < 0.3 * T-2 Body, Close < T-2 Close)
                # Note: User text says "Close < T-2 Close", standard is "Gap" or "Low body".
                # We stick to user prompt: "å¯¦é«”å¾ˆå°ï¼Œä¸”æ”¶ç›¤åƒ¹ä½æ–¼ T-2æ—¥æ”¶ç›¤"
                is_star_1 = (body1 < body2 * 0.3) and (c1 < c2)
                if is_star_1:
                    count_m_step2 += 1
                    
                    # Step 3: T Long Red (Close > T-2 Mid)
                    mid_point_2 = (o2 + c2) / 2
                    is_long_red_0 = (c0 > o0) and (c0 > mid_point_2)
                    # Also check if it's a "Long" candle (Body > 0.6 Range) as per previous logic?
                    # User text says "é•·ç´… K", so yes.
                    is_long_red_0 = is_long_red_0 and (body0 > range0 * 0.6)
                    
                    if is_long_red_0:
                        count_m_step3 += 1
                        
                        # Step 4: Volume Surge (Third candle volume > ?)
                        # "çˆ†é‡" -> Let's say > 1.3x Prev or > MA5
                        # Let's use > 1.3x Prev for strictness or > MA5
                        # User text: "ç¬¬ä¸‰æ ¹é™½ç·šè‹¥çˆ†é‡"
                        vol_surge = v0 > v1 * 1.3
                        if vol_surge:
                            count_m_step4 += 1
                            count_m_final += 1
                            passed_m = True
                            
                            # Calculate VSBC & Fib
                            df = add_vsbc_columns(df)
                            t = df.iloc[-1]
                            vsbc_val = t['vsbc'] if 'vsbc' in t else 0
                            
                            # Calculate POC (Simple approximation using mode of close price in recent period or just use VSBC value itself if that's what user wants)
                            # User said "VSBCä¸Š/ä¸‹ (å¦‚ï¼šå£“åŠ›å€)<--é€™å€‹æ‡‰è©²æ˜¯æ•¸å­—å§ï¼0000/0000"
                            # Let's assume they want VSBC Value / Price or VSBC / POC.
                            # In scan_vsbc_strategy, we use calc_vp_poc(df). Let's use that if available, or implement simple one.
                            # Since calc_vp_poc is defined elsewhere, let's check if we can use it.
                            # It seems calc_vp_poc is a global function.
                            try:
                                poc = calc_vp_poc(df)
                            except:
                                poc = df['close'].mean() # Fallback
                            
                            # Fib 60 days
                            recent_60 = df.iloc[-60:]
                            h60 = recent_60['high'].max()
                            l60 = recent_60['low'].min()
                            diff = h60 - l60
                            fib_0618 = h60 - (diff * 0.618)
                            
                            # Calculate current retracement ratio
                            # Ratio = (High - Close) / (High - Low) for pullback from High
                            if diff > 0:
                                current_ratio = (h60 - c0) / diff
                            else:
                                current_ratio = 0
                            
                            morning_stars.append({
                                'code': code, 'name': name,
                                'close': c0, 'close_prev': c1,
                                'pattern': 'æ—©æ™¨ä¹‹æ˜Ÿ',
                                'volume': v0, # Raw volume
                                'vol_ratio': v0/v1 if v1>0 else 1,
                                'vsbc_lower': vsbc_val, # Map to VSBC Lower
                                'vsbc_upper': poc,      # Map to VSBC Upper (POC)
                                'fib_val': fib_0618,
                                'fib_ratio': current_ratio
                            })

            # --- Evening Star Logic (Simplified for now, or parallel) ---
            # T-2 Long Red
            is_long_red_2 = (c2 > o2) and (body2 > range2 * 0.6)
            if is_long_red_2:
                # T-1 Star (High)
                is_star_1 = (body1 < body2 * 0.3) and (c1 > c2)
                if is_star_1:
                    # T Long Black (Close < T-2 Mid)
                    mid_point_2 = (o2 + c2) / 2
                    is_long_black_0 = (c0 < o0) and (c0 < mid_point_2) and (body0 > range0 * 0.6)
                    if is_long_black_0:
                        # Vol Surge (Optional for Evening? Usually volume shrinks on top, but breakdown needs volume)
                        # Let's apply same surge logic for symmetry or just pass
                        # User only specified Morning Star funnel details.
                        # We'll just add it.
                        count_e_final += 1
                        # Calculate VSBC & Fib (Same as above)
                        df = add_vsbc_columns(df)
                        t = df.iloc[-1]
                        vsbc_val = t['vsbc'] if 'vsbc' in t else 0
                        
                        try:
                            poc = calc_vp_poc(df)
                        except:
                            poc = df['close'].mean()

                        recent_60 = df.iloc[-60:]
                        h60 = recent_60['high'].max()
                        l60 = recent_60['low'].min()
                        diff = h60 - l60
                        fib_0618 = h60 - (diff * 0.618)
                        
                        if diff > 0:
                            current_ratio = (h60 - c0) / diff
                        else:
                            current_ratio = 0

                        evening_stars.append({
                            'code': code, 'name': name,
                            'close': c0, 'close_prev': c1,
                            'pattern': 'é»ƒæ˜ä¹‹æ˜Ÿ',
                            'volume': v0,
                            'vol_ratio': v0/v1 if v1>0 else 1,
                            'vsbc_lower': vsbc_val,
                            'vsbc_upper': poc,
                            'fib_val': fib_0618,
                            'fib_ratio': current_ratio
                        })

        except Exception as e:
            continue
            
    # Summary
    print_flush("\n" + "="*60)
    print_flush("[ç¯©é¸éç¨‹] Kç·šå‹æ…‹ (æ™¨æ˜Ÿ/å¤œæ˜Ÿ)")
    print_flush("="*60)
    print_flush(f"ç¸½è‚¡æ•¸: {count_total}")
    print_flush("â”€"*60)
    print_flush(f"âœ“ æˆäº¤é‡ >= {min_vol}å¼µ        â†’ {count_vol} æª”")
    print_flush(f"âœ“ [ç¬¬1éš] T-2: é•·é»‘ K (å¯¦é«” > 0.6 * ç¸½é•·)   â†’ {count_m_step1} æª”")
    print_flush(f"âœ“ [ç¬¬2éš] T-1: æ˜Ÿç·š (å¯¦é«” < 0.3 * T-2å¯¦é«”)  â†’ {count_m_step2} æª”")
    print_flush(f"âœ“ [ç¬¬3éš] T: é•·ç´… K (æ”¶ç›¤ > T-2å¯¦é«”ä¸­é»)    â†’ {count_m_step3} æª”")
    print_flush(f"âœ“ [ç¬¬4éš] ç¬¬ä¸‰æ ¹é™½ç·šè‹¥çˆ†é‡ (>1.3å€)         â†’ {count_m_step4} æª”")
    print_flush(f"âœ“ ç¶œåˆè©•åˆ† >= ä»¥ä¸Šéƒ½ç¬¦åˆ (æ™¨æ˜Ÿ)             â†’ {count_m_final} æª”")
    if count_e_final > 0:
        print_flush(f"âœ“ é»ƒæ˜ä¹‹æ˜Ÿ (é¡å¤–ç¯©é¸)                       â†’ {count_e_final} æª”")
    print_flush("â”€"*60)
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def candle_extra(code, item):
        ratio = item.get('fib_ratio', 0)
        close = item.get('close', 0)
        levels = [0.236, 0.382, 0.5, 0.618, 0.786]
        nearest = min(levels, key=lambda x: abs(x - ratio))
        if abs(ratio - nearest) < 0.05:
            fib_str = f"{nearest}({close:.0f})"
        else:
            fib_str = f"{ratio:.2f}({close:.0f})"
        return [fib_str]

    if morning_stars:
        display_scan_results_v2(morning_stars, "æ—©æ™¨ä¹‹æ˜Ÿ (åº•éƒ¨åè½‰)", limit=limit,
                                extra_headers=["è²»æ³¢é‚£å¥‘"],
                                extra_func=candle_extra)
    
    if evening_stars:
        display_scan_results_v2(evening_stars, "é»ƒæ˜ä¹‹æ˜Ÿ (é ‚éƒ¨åè½‰)", limit=limit,
                                extra_headers=["è²»æ³¢é‚£å¥‘"],
                                extra_func=candle_extra)
    
    if not morning_stars and not evening_stars:
        print_flush("\næ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨ã€‚")


def scan_vp(indicators_data, mode='lower', min_volume=100):
    """VPæƒæ (ä¸¦è¡Œç‰ˆ)"""
    
    def filter_func(code, ind):
        close = safe_float_preserving_none(ind.get('close'))
        if not close:
            return False
        
        if mode == 'lower':
            vp_lower = safe_float_preserving_none(ind.get('vp_lower') or ind.get('VP_lower'))
            if not vp_lower:
                return False
            return abs(close - vp_lower) / close < 0.02
        else:
            vp_upper = safe_float_preserving_none(ind.get('vp_upper') or ind.get('VP_upper'))
            if not vp_upper:
                return False
            return abs(close - vp_upper) / close < 0.02
    
    def transform_func(code, ind):
        return (code, 0, ind)
    
    return scan_with_parallel(
        indicators_data,
        filter_func,
        transform_func,
        sort_key=lambda x: x[1],
        reverse=False,
        min_volume=min_volume * 1000  # Convert to shares
    )


def scan_ma_cross():
    """å‡ç·šäº¤å‰æƒæ (å¤šé€²ç¨‹)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ å‡ç·šäº¤å‰ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if not data:
        print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
        return

    golden = []
    death = []
    
    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol < min_vol * 1000: continue
            
            close = safe_float_preserving_none(ind.get('close'))
            ma5 = safe_float_preserving_none(ind.get('ma5') or ind.get('MA5'))
            ma20 = safe_float_preserving_none(ind.get('ma20') or ind.get('MA20'))
            ma5_prev = safe_float_preserving_none(ind.get('ma5_prev') or ind.get('MA5_prev'))
            ma20_prev = safe_float_preserving_none(ind.get('ma20_prev') or ind.get('MA20_prev'))
            
            if not (ma5 and ma20 and ma5_prev and ma20_prev): continue
            
            # Golden Cross: MA5 crosses above MA20
            if ma5_prev <= ma20_prev and ma5 > ma20:
                golden.append({'code': code, 'name': get_stock_name(code), 'close': close, 'ma5': ma5, 'ma20': ma20})
                
            # Death Cross: MA5 crosses below MA20
            if ma5_prev >= ma20_prev and ma5 < ma20:
                death.append({'code': code, 'name': get_stock_name(code), 'close': close, 'ma5': ma5, 'ma20': ma20})
        except:
            continue
    
    print_flush(f"\nã€é»ƒé‡‘äº¤å‰ (MA5ä¸Šç©¿MA20)ã€‘ (å‰ {limit} ç­†)")
    for res in golden[:limit]:
        print_flush(f"{res['code']} {res['name']} : ç¾åƒ¹={res['close']:.2f} MA5={res['ma5']:.2f} MA20={res['ma20']:.2f}")
        
    print_flush(f"\nã€æ­»äº¡äº¤å‰ (MA5ä¸‹ç©¿MA20)ã€‘ (å‰ {limit} ç­†)")
    for res in death[:limit]:
        print_flush(f"{res['code']} {res['name']} : ç¾åƒ¹={res['close']:.2f} MA5={res['ma5']:.2f} MA20={res['ma20']:.2f}")


def scan_kd_nvi_cross():
    """æœˆKD / NVI+PVI äº¤å‰è¨Šè™Ÿ (å¤šé€²ç¨‹)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ KD/NVI äº¤å‰è¨Šè™Ÿ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if not data:
        print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
        return

    results = []
    
    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol < min_vol * 1000: continue
            
            k = safe_float_preserving_none(ind.get('month_k') or ind.get('Month_K'))
            d = safe_float_preserving_none(ind.get('month_d') or ind.get('Month_D'))
            k_prev = safe_float_preserving_none(ind.get('month_k_prev') or ind.get('Month_K_prev'))
            d_prev = safe_float_preserving_none(ind.get('month_d_prev') or ind.get('Month_D_prev'))
            
            nvi = safe_float_preserving_none(ind.get('nvi') or ind.get('NVI'))
            pvi = safe_float_preserving_none(ind.get('pvi') or ind.get('PVI'))
            ma60 = safe_float_preserving_none(ind.get('ma60') or ind.get('MA60'))
            
            # KD Golden Cross
            kd_golden = False
            if k and d and k_prev and d_prev:
                if k_prev <= d_prev and k > d and k < 80:
                    kd_golden = True
            
            # NVI > PVI (Bullish)
            nvi_bull = False
            if nvi and pvi and nvi > pvi:
                nvi_bull = True
                
            if kd_golden and nvi_bull:
                results.append({'code': code, 'name': get_stock_name(code), 'close': ind.get('close'), 'k': k, 'd': d})
        except:
            continue
    
    print_flush(f"\nã€KDé‡‘å‰+NVIå¼·å‹¢ã€‘ (å‰ {limit} ç­†)")
    print_flush(f"{'ä»£è™Ÿ':<6} {'åç¨±':<8} {'ç¾åƒ¹':<8} {'Kå€¼':<6} {'Då€¼':<6}")
    print_flush("-" * 50)
    for res in results[:limit]:
        print_flush(f"{res['code']:<6} {res['name']:<8} {res['close']:<8.2f} {res['k']:<6.1f} {res['d']:<6.1f}")


def scan_ma_bullish():
    """å‡ç·šå¤šé ­æ’åˆ— (å¤šé€²ç¨‹)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ å‡ç·šå¤šé ­æ’åˆ— (æˆäº¤é‡ > {min_vol} å¼µ)...")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if not data:
        print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
        return

    results = []
    
    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol < min_vol * 1000: continue
            
            close = safe_float_preserving_none(ind.get('close'))
            ma5 = safe_float_preserving_none(ind.get('ma5') or ind.get('MA5'))
            ma20 = safe_float_preserving_none(ind.get('ma20') or ind.get('MA20'))
            ma60 = safe_float_preserving_none(ind.get('ma60') or ind.get('MA60'))
            ma120 = safe_float_preserving_none(ind.get('ma120') or ind.get('MA120'))
            
            if not (close and ma5 and ma20 and ma60 and ma120): continue
            
            # Bullish Alignment: Price > MA5 > MA20 > MA60 > MA120
            if close > ma5 > ma20 > ma60 > ma120:
                # Bias check (0-10%)
                bias = (close - ma20) / ma20 * 100
                if 0 < bias < 10:
                    results.append({'code': code, 'name': get_stock_name(code), 'close': close, 'bias': bias, 'volume': vol})
        except:
            continue
    
    results.sort(key=lambda x: x['bias'])
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def ma_bullish_extra(code, item):
        bias = item.get('bias', 0)
        return [f"{bias:.2f}%"]

    codes = display_scan_results_v2(results, "å‡ç·šå¤šé ­æ’åˆ— (ä¹–é›¢0-10%)", limit=limit,
                            extra_headers=["ä¹–é›¢%"],
                            extra_func=ma_bullish_extra)


def scan_five_filter():
    """äº”éšç¯©é¸å™¨ (åƒå•ç‰ˆ - æ•´åˆ)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨åŸ·è¡Œäº”éšç¯©é¸ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    print_flush("ç¯©é¸æ¢ä»¶: 1.ç›¸å°å¼·åº¦ 2.å¼·å‹¢è‚¡ 3.ä¸»åŠ›é©—è­‰ 4.åƒ¹å€¼å€é–“ 5.å‹•èƒ½è§¸ç™¼")
    
    screener = TaiwanStockScreenerAdvanced(None)
    
    with db_manager.get_connection() as conn:
        cur = conn.cursor()
        cur.execute("SELECT code, name FROM stock_snapshot")
        stocks = cur.fetchall()
    
    results = []
    
    # Counters
    count_total = len(stocks)
    count_data = 0
    count_vol = 0
    
    # Level Counters
    count_rs = 0
    count_s1 = 0
    count_s2 = 0
    count_s3 = 0
    count_s4 = 0
    count_final = 0
    
    codes = [s[0] for s in stocks]
    history_map = batch_load_history(codes, limit_days=150)
    
    # Load Market Data
    market_df = history_map.get('0050')
    if market_df is None:
        if history_map: market_df = list(history_map.values())[0]
        else: return

    # Ensure Market Index
    market_df = market_df.copy()
    if 'date' in market_df.columns:
        market_df['date'] = pd.to_datetime(market_df['date'])
        market_df.set_index('date', inplace=True)
    market_df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}, inplace=True)
    
    # Market Filter
    try:
        m_data = screener.market_filter(market_df)
        adj = m_data['adjustment_factor']
    except:
        adj = 1.0

    for code, name in stocks:
        try:
            df = history_map.get(code)
            if df is None or len(df) < 100: continue
            count_data += 1
            
            # Ensure Stock Index
            df = df.copy()
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df.set_index('date', inplace=True)
            df.rename(columns={'open': 'Open', 'high': 'High', 'low': 'Low', 'close': 'Close', 'volume': 'Volume'}, inplace=True)
            
            # Volume Filter
            if df['Volume'].iloc[-1] < min_vol * 1000: continue
            count_vol += 1
            
            # Indicators
            df = screener.calculate_technical_indicators(df)
            
            # Level 1: Relative Strength
            rs = screener.calculate_relative_strength(df, market_df)
            if rs >= screener.current_params['min_relative_strength']:
                count_rs += 1
            else:
                continue # RS is a hard filter
            
            # Calculate Scores for Levels 2-5
            ok1, s1 = screener.stock_strength_filter(df, adj)
            ok2, s2 = screener.smart_money_validation(df, adj)
            ok3, s3 = screener.value_zone_filter(df)
            ok4, s4 = screener.entry_trigger(df)
            
            if ok1: count_s1 += 1
            if ok2: count_s2 += 1
            if ok3: count_s3 += 1
            if ok4: count_s4 += 1
            
            # Final Score (Weighted)
            # Weights: Strength 30%, Smart 30%, Value 20%, Trigger 20%
            final_score = (s1 * 0.3) + (s2 * 0.3) + (s3 * 0.2) + (s4 * 0.2)
            
            if final_score >= 60: # Threshold
                count_final += 1
                results.append({
                    'code': code, 'name': name,
                    'close': df.iloc[-1]['Close'],
                    'close_prev': df.iloc[-2]['Close'],
                    'volume': df.iloc[-1]['Volume'], # Raw shares for display_scan_results_v2
                    'score': final_score,
                    'rs': rs,
                    'k': df.iloc[-1]['k'] if 'k' in df.columns else 0
                })
                
        except Exception as e:
            continue
            
    # Sort
    results.sort(key=lambda x: x['score'], reverse=True)
    
    # Summary
    print_flush("\n" + "="*60)
    print_flush("[ç¯©é¸éç¨‹] äº”éšç¯©é¸å™¨ (åƒå•ç‰ˆ)")
    print_flush(f"ç¸½è‚¡æ•¸: {count_total}")
    print_flush("â”€"*60)
    print_flush(f"âœ“ è³‡æ–™å……è¶³ (>100æ—¥)       â†’ {count_data} æª”")
    print_flush(f"âœ“ æˆäº¤é‡ >= {min_vol}å¼µ        â†’ {count_vol} æª”")
    print_flush(f"âœ“ [ç¬¬1éš] ç›¸å°å¼·åº¦ (RS)   â†’ {count_rs} æª”")
    print_flush(f"âœ“ [ç¬¬2éš] å¼·å‹¢è‚¡æ¢ä»¶      â†’ {count_s1} æª” (åˆæ ¼ç‡)")
    print_flush(f"âœ“ [ç¬¬3éš] ä¸»åŠ›ç±Œç¢¼é©—è­‰    â†’ {count_s2} æª” (åˆæ ¼ç‡)")
    print_flush(f"âœ“ [ç¬¬4éš] åƒ¹å€¼å€é–“å®šä½    â†’ {count_s3} æª” (åˆæ ¼ç‡)")
    print_flush(f"âœ“ [ç¬¬5éš] å‹•èƒ½è§¸ç™¼è¨Šè™Ÿ    â†’ {count_s4} æª” (åˆæ ¼ç‡)")
    print_flush(f"âœ“ ç¶œåˆè©•åˆ† >= 60åˆ†        â†’ {count_final} æª” (æœ€çµ‚é¸å‡º)")
    print_flush("â”€"*60)
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def five_stage_extra(code, item):
        score = item.get('score', 0)
        rs = item.get('rs', 0)
        k = item.get('k', 0)
        return [f"{score:.1f}", f"{rs:.2f}", f"{k:.1f}"]

    codes = display_scan_results_v2(results, "äº”éšç¯©é¸ TOP", limit=limit,
                            extra_headers=["åˆ†æ•¸", "RSå€¼", "Kå€¼"],
                            extra_func=five_stage_extra)





def scan_six_dim_resonance():
    """å…­ç¶­å…±æŒ¯äº¤æ˜“ç³»çµ± (MACD/KDJ/RSI/LWR/BBI/MTM)"""
    limit, min_vol = get_user_scan_params()
    print_flush(f"\næ­£åœ¨æƒæ å…­ç¶­å…±æŒ¯è¨Šè™Ÿ (æˆäº¤é‡ > {min_vol} å¼µ)...")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if not data:
        print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
        return

    results = []
    
    for code, ind in data.items():
        try:
            vol = safe_float_preserving_none(ind.get('volume', 0))
            if vol < min_vol * 1000: continue
            
            close = safe_float_preserving_none(ind.get('close'))
            
            # 1. MACD: DIF > DEM (Bullish) or MACD > 0
            macd = safe_float_preserving_none(ind.get('MACD'))
            signal = safe_float_preserving_none(ind.get('SIGNAL'))
            cond_macd = (macd is not None and signal is not None and macd > signal)
            
            # 2. KDJ: K > D (Bullish)
            k = safe_float_preserving_none(ind.get('Month_K') or ind.get('month_k')) # Using Monthly or Daily? Plan said KDJ. Let's use Monthly for trend or Daily for trigger. Using Monthly as per context of "Resonance" usually implies stronger trend. Let's use Daily for sensitivity or Monthly for stability. Let's stick to Daily for "Trading System".
            # Actually, let's check if Daily K/D exists.
            k_d = safe_float_preserving_none(ind.get('Daily_K') or ind.get('daily_k'))
            d_d = safe_float_preserving_none(ind.get('Daily_D') or ind.get('daily_d'))
            cond_kdj = (k_d is not None and d_d is not None and k_d > d_d)
            
            # 3. RSI: RSI > 50
            rsi = safe_float_preserving_none(ind.get('RSI'))
            cond_rsi = (rsi is not None and rsi > 50)
            
            # 4. LWR: LWR > -50 (Bullish/Strong) - Note: LWR is usually -100 to 0.
            lwr = safe_float_preserving_none(ind.get('LWR'))
            cond_lwr = (lwr is not None and lwr > -50)
            
            # 5. BBI: Price > BBI
            bbi = safe_float_preserving_none(ind.get('BBI'))
            cond_bbi = (close is not None and bbi is not None and close > bbi)
            
            # 6. MTM: MTM > 0
            mtm = safe_float_preserving_none(ind.get('MTM'))
            cond_mtm = (mtm is not None and mtm > 0)
            
            # Calculate Resonance Score (How many conditions met)
            score = sum([cond_macd, cond_kdj, cond_rsi, cond_lwr, cond_bbi, cond_mtm])
            
            if score >= 5: # At least 5 out of 6
                results.append({
                    'code': code, 
                    'name': get_stock_name(code), 
                    'close': close, 
                    'score': score,
                    'volume': vol, # Rename to volume for display_scan_results_v2
                    'details': [cond_macd, cond_kdj, cond_rsi, cond_lwr, cond_bbi, cond_mtm]
                })
        except:
            continue
    
    results.sort(key=lambda x: (x['score'], x['volume']), reverse=True)
    
    # ä½¿ç”¨çµ±ä¸€æ ¼å¼è¼¸å‡º
    def six_dim_extra(code, item):
        score = item.get('score', 0)
        d = item.get('details', [])
        # d is [cond_macd, cond_kdj, cond_rsi, cond_lwr, cond_bbi, cond_mtm]
        
        signal_str = "è²·å…¥"
        dim_str = f"{score}/6"
        checks = ["âœ“" if x else " " for x in d]
        
        return [signal_str, dim_str] + checks

    codes = display_scan_results_v2(results, "å…­ç¶­å…±æŒ¯ (è‡³å°‘5é …ç¬¦åˆ)", limit=limit,
                            description="æŒ‡æ¨™èªªæ˜: 1.MACD>Sig 2.K>D 3.RSI>50 4.LWR>-50 5.Px>BBI 6.MTM>0",
                            extra_headers=["è¨Šè™Ÿ", "ç¶­åº¦", "MACD", "KDJ", "RSI", "LWR", "BBI", "MTM"],
                            extra_func=six_dim_extra)


def market_scan_menu():
    """å¸‚å ´æƒæé¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    # æª¢æŸ¥å¿«å–
    data = GLOBAL_INDICATOR_CACHE.get_data()
    if not data:
        print_flush("\næ­£åœ¨è¼‰å…¥æŒ‡æ¨™ (Snapshot)...")
        data = step4_load_data()
        GLOBAL_INDICATOR_CACHE.set_data(data)

    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€å¸‚å ´æƒæã€‘")
        print_flush("="*60)
        print_flush("[1] VPæƒæ (ç®±å‹å£“åŠ›/æ”¯æ’)")
        print_flush("[2] MFIæƒæ (è³‡é‡‘æµå‘)")
        print_flush("[3] å‡ç·šæƒæ (å«å¤šé ­æƒæ)")
        print_flush("[4] äº¤å‰è¨Šè™Ÿæƒæ (æœˆKD)")
        print_flush("-" * 60)
        print_flush("[5] VSBC ç±Œç¢¼ç­–ç•¥ (é‡åƒ¹/ç®±å‹/ç±Œç¢¼)")
        print_flush("[6] è°æ˜éŒ¢æƒæ (NVIä¸»åŠ›ç±Œç¢¼)")
        print_flush("[7] 2560 æˆ°æ³• (å‡ç·š/é‡èƒ½)")
        print_flush("[8] äº”éšç¯©é¸å™¨ (åƒå•ç‰ˆ)")
        print_flush("[9] æ©Ÿæ§‹åƒ¹å€¼å›æ­¸ç­–ç•¥ (Gemini)")
        print_flush("[a] å…­ç¶­å…±æŒ¯äº¤æ˜“ç³»çµ± (MACD/KDJ/RSI/LWR/BBI/MTM)")
        print_flush("-" * 60)
        print_flush("[b] Kç·šå‹æ…‹ (æ™¨æ˜Ÿ/å¤œæ˜Ÿ)")
        print_flush("[c] é‡åƒ¹èƒŒé›¢å½¢æ…‹è©³è§£ (é€²éšåµæ¸¬)")
        print_flush("[0] è¿”å›ä¸»é¸å–®")
        print_flush("-" * 60)
        print_flush("ğŸ’¡ è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ (å¦‚ 2330) å¯ç›´æ¥æŸ¥çœ‹å€‹è‚¡")
        
        ch = input("è«‹é¸æ“‡: ").strip().lower()
        
        if ch == '0': break
        elif ch == '1': vp_scan_submenu()
        elif ch == '2': mfi_scan_submenu()
        elif ch == '3': ma_scan_submenu()
        elif ch == '4': scan_kd_nvi_cross()
        elif ch == '5': scan_vsbc_strategy()
        elif ch == '6': scan_smart_money_strategy()
        elif ch == '7': scan_2560_strategy()
        elif ch == '8': scan_five_filter()
        elif ch == '9': run_institutional_value_strategy()
        elif ch == 'a': scan_six_dim_resonance()
        elif ch == 'b': scan_candlestick_patterns()
        elif ch == 'c': scan_pv_divergence_analysis()
        
        # è‚¡ç¥¨ä»£è™ŸæŸ¥è©¢
        elif ch.isdigit() and len(ch) == 4:
            _handle_stock_query(ch)
        else:
            if ch: print_flush("âŒ ç„¡æ•ˆè¼¸å…¥")


def vp_scan_submenu():
    """VPæƒæå­é¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    print_flush("\nã€VPæƒæã€‘")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if data:
        print_flush(f"[å·²è¼‰å…¥æŒ‡æ¨™: {len(data)} æª”]")
    else:
        print_flush("[æœªè¼‰å…¥æŒ‡æ¨™]")
    
    print_flush("[1] VP æ¥è¿‘ä¸‹ç·£ (æ”¯æ’)")
    print_flush("[2] VP æ¥è¿‘ä¸Šç·£ (å£“åŠ›)")
    print_flush("[0] è¿”å›")
    
    ch = read_single_key()
    
    if ch == '0':
        return
    
    mode = 'lower' if ch == '1' else 'upper'
    title = "VP æ¥è¿‘ä¸‹ç·£ (æ”¯æ’)" if mode == 'lower' else "VP æ¥è¿‘ä¸Šç·£ (å£“åŠ›)"
    
    if ch in ['1', '2']:
        limit, min_vol = get_user_scan_params()

        print_flush(f"\næ­£åœ¨æƒæ {title}...")
        
        print_flush(f"\næ­£åœ¨æƒæ {title}...")
        
        data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
        if not data:
            print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
            return
        
        res = scan_vp(data, mode, min_volume=min_vol)
        codes = display_scan_results_v2(res, title, limit=limit, description="VP: Volume Profile (ç±Œç¢¼åˆ†å¸ƒ), æ¥è¿‘ä¸‹ç·£=æ”¯æ’, æ¥è¿‘ä¸Šç·£=å£“åŠ›")
        prompt_stock_detail_report(codes)

def mfi_scan_submenu():
    """MFIæƒæå­é¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    print_flush("\nã€MFIæƒæã€‘")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if data:
        print_flush(f"[å·²è¼‰å…¥æŒ‡æ¨™: {len(data)} æª”]")
    else:
        print_flush("[æœªè¼‰å…¥æŒ‡æ¨™]")
    
    print_flush("[1] MFIç”±å°â†’å¤§ (è³‡é‡‘æµå…¥é–‹å§‹)")
    print_flush("[2] MFIç”±å¤§â†’å° (è³‡é‡‘æµå‡ºçµæŸ)")
    print_flush("[0] è¿”å›")
    
    ch = read_single_key()
    
    if ch == '0':
        return
    
    if ch in ['1', '2']:
        limit, min_vol = get_user_scan_params()

        order = 'asc' if ch == '1' else 'desc'
        title = "MFIç”±å°â†’å¤§ (è³‡é‡‘æµå…¥é–‹å§‹)" if order == 'asc' else "MFIç”±å¤§â†’å° (è³‡é‡‘æµå‡ºçµæŸ)"
        
        print_flush(f"\næ­£åœ¨æƒæ {title}...")
        
        print_flush(f"\næ­£åœ¨æƒæ {title}...")
        
        data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
        if not data:
            print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
            return
        
        results = scan_mfi_mode(data, order=order, min_volume=min_vol)
        
        def mfi_extra(code, ind):
            mfi = safe_num(ind.get('mfi14') or ind.get('MFI'))
            return [f"{mfi:.1f}" if mfi else "-"]
            
        codes = display_scan_results_v2(results, title, limit=limit, 
                                   description="MFI: Money Flow Index (è³‡é‡‘æµé‡), >80 è¶…è²·, <20 è¶…è³£, 50 åˆ†ç•Œ",
                                   extra_headers=["MFI"],
                                   extra_func=mfi_extra)
        prompt_stock_detail_report(codes)

def ma_scan_submenu():
    """å‡ç·šæƒæå­é¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    print_flush("\nã€å‡ç·šæƒæã€‘")
    
    data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
    if data:
        print_flush(f"[å·²è¼‰å…¥æŒ‡æ¨™: {len(data)} æª”]")
    else:
        print_flush("[æœªè¼‰å…¥æŒ‡æ¨™]")
    
    print_flush("[1] ä½æ–¼MA200 -0%~-10%")
    print_flush("[2] ä½æ–¼MA20 -0%~-10%")
    print_flush("[3] å‡ç·šå¤šé ­ (å››ç·šä¸Šæš+è‚¡åƒ¹åœ¨ä¸Š+0-10%)")
    print_flush("[0] è¿”å›")
    
    ch = read_single_key()
    
    if ch == '0':
        return
    
    if ch in ['1', '2']:
        limit, min_vol = get_user_scan_params()

        ma_type = 'MA200' if ch == '1' else 'MA20'
        title = f"ä½æ–¼{ma_type} -0%~-10%"
        
        print_flush(f"\næ­£åœ¨æƒæ {title}...")
        
        data = GLOBAL_INDICATOR_CACHE.get_data() if GLOBAL_INDICATOR_CACHE else {}
        if not data:
            print_flush("âŒ è«‹å…ˆè¼‰å…¥æŒ‡æ¨™æ•¸æ“š")
            return
        
        results = scan_ma_mode(data, ma_type=ma_type, min_volume=min_vol)
        
        def ma_extra(code, ind):
            ma_val = safe_num(ind.get(ma_type.lower()) or ind.get(ma_type))
            return [f"{ma_type}:{ma_val:.1f}" if ma_val else "-"]
            
        codes = display_scan_results_v2(results, title, limit=limit, 
                                   description="MA: Moving Average (ç§»å‹•å¹³å‡ç·š), è‚¡åƒ¹ä½æ–¼å‡ç·š=å›æ¸¬æˆ–è·Œç ´",
                                   extra_headers=[ma_type],
                                   extra_func=ma_extra)
        prompt_stock_detail_report(codes)
    
    elif ch == '3':
        scan_ma_bullish()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                       APP/CLI                                 â•‘
# â•‘  ä¸»é¸å–®ã€æµç¨‹æ§åˆ¶ã€CLI å…¥å£é»                                  â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def data_management_menu():
    """è³‡æ–™ç®¡ç†å­é¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    # è¡¨é©…å‹•æ³•ï¼šæ­¥é©ŸåŠŸèƒ½æ˜ å°„
    DATA_MENU_ACTIONS = {
        '1': _run_full_daily_update,
        '2': step1_fetch_stock_list,
        '3': step2_download_tpex_daily,
        '4': step3_download_twse_daily,
        '5': step3_5_download_institutional,
        '6': step3_6_download_major_holders,
        '7': step3_7_fetch_margin_data,
        '8': step3_8_fetch_market_index,
        '9': step4_check_data_gaps,
        'a': step5_clean_delisted,
        'b': _handle_step6_with_resume,
        'c': _handle_step7_with_cache_clear
    }
    
    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€è³‡æ–™ç®¡ç†èˆ‡æ›´æ–°ã€‘")
        print_flush("="*60)
        print_flush("[1] ä¸€éµåŸ·è¡Œæ¯æ—¥æ›´æ–° (Steps 1-7)")
        print_flush("-" * 60)
        print_flush("[2] æ­¥é©Ÿ1: æ›´æ–°ä¸Šå¸‚æ«ƒæ¸…å–®")
        print_flush("[3] æ­¥é©Ÿ2: ä¸‹è¼‰ TPEx (ä¸Šæ«ƒ)")
        print_flush("[4] æ­¥é©Ÿ3: ä¸‹è¼‰ TWSE (ä¸Šå¸‚)")
        print_flush("[5] æ­¥é©Ÿ3.5: ä¸‹è¼‰ä¸‰å¤§æ³•äººè²·è³£è¶…")
        print_flush("[6] æ­¥é©Ÿ3.6: ä¸‹è¼‰é›†ä¿å¤§æˆ¶è³‡æ–™")
        print_flush("[7] æ­¥é©Ÿ3.7: ä¸‹è¼‰èè³‡èåˆ¸è³‡æ–™")
        print_flush("[8] æ­¥é©Ÿ3.8: ä¸‹è¼‰å¤§ç›¤æŒ‡æ•¸è³‡æ–™")
        print_flush("[9] æ­¥é©Ÿ4: æª¢æŸ¥æ•¸æ“šç¼ºå¤±")
        print_flush("[a] æ­¥é©Ÿ5: æ¸…ç†ä¸‹å¸‚è‚¡ç¥¨")
        print_flush("[b] æ­¥é©Ÿ6: é©—è­‰ä¸€è‡´æ€§ä¸¦è£œæ¼ (æ–·é»çºŒæŠ“)")
        print_flush("[c] æ­¥é©Ÿ7: è¨ˆç®—æŠ€è¡“æŒ‡æ¨™")
        print_flush("[0] è¿”å›ä¸»é¸å–®")

        ch = read_single_key().lower()

        # è¡›èªå¥ï¼šè¿”å›
        if ch == '0':
            break
        
        # è¡¨é©…å‹•æ³•ï¼šæŸ¥æ‰¾ä¸¦åŸ·è¡Œ
        action = DATA_MENU_ACTIONS.get(ch)
        if action:
            action()


def _force_redownload_all_history():
    """è£œé½Šæ‰€æœ‰è‚¡ç¥¨çš„æˆäº¤é‡‘é¡ï¼ˆå·²æ£„ç”¨ï¼Œä¿ç•™å‡½æ•¸ä½†æ¸…ç©ºå…§å®¹ï¼‰"""
    print_flush("æ­¤åŠŸèƒ½å·²å®Œæˆä»»å‹™ä¸¦åœç”¨ã€‚")
    return

def _handle_step6_with_resume():
    """æ­¥é©Ÿ6ï¼šé©—è­‰ä¸€è‡´æ€§ä¸¦è£œæ¼ï¼ˆå«æ–·é»çºŒæŠ“æç¤ºï¼‰"""
    resume = True
    if PROGRESS_FILE.exists():
        print_flush("\nç™¼ç¾é€²åº¦ç´€éŒ„:")
        print_flush("[1] ç¹¼çºŒä¸Šæ¬¡é€²åº¦ (é è¨­)")
        print_flush("[2] é‡é ­é–‹å§‹")
        
        sub_ch = read_single_key()
        if sub_ch == '2':
            resume = False
            print_flush("å·²é¸æ“‡é‡é ­é–‹å§‹")
        else:
            print_flush("å·²é¸æ“‡ç¹¼çºŒä¸Šæ¬¡é€²åº¦")
    
    step6_verify_and_backfill(resume=resume)


def _handle_step7_with_cache_clear():
    """æ­¥é©Ÿ7ï¼šè¨ˆç®—æŠ€è¡“æŒ‡æ¨™ï¼ˆå«å¿«å–æ¸…é™¤ï¼‰"""
    step7_calc_indicators()
    
    if GLOBAL_INDICATOR_CACHE:
        GLOBAL_INDICATOR_CACHE.clear()
    print_flush("âœ“ ç³»çµ±å¿«å–å·²æ¸…é™¤")


def _run_full_daily_update():
    """ä¸€éµåŸ·è¡Œæ¯æ—¥æ›´æ–° (Steps 1->..->8)"""
    global GLOBAL_INDICATOR_CACHE
    updated_codes = set()
    out = StepOutput  # ç°¡åŒ–èª¿ç”¨
    
    # é–‹å§‹æ›´æ–°
    out.box_start("ä¸€éµæ¯æ—¥æ›´æ–°")
    
    # Step 1: æ›´æ–°æ¸…å–® (å¿…é ˆå…ˆåŸ·è¡Œï¼Œå› ç‚ºå¾ŒçºŒæ­¥é©Ÿä¾è³´æ¸…å–®)
    out.header("æ›´æ–°ä¸Šå¸‚æ«ƒæ¸…å–®", "1")
    step1_fetch_stock_list(silent_header=True)
    
    # Step 2-3.10: å…¨é¢ä¸¦è¡Œä¸‹è¼‰
    out.header("ä¸¦è¡Œä¸‹è¼‰æ‰€æœ‰å¸‚å ´è³‡æ–™", "2-3.10")
    print_flush("  å•Ÿå‹•é«˜ä½µç™¼ä¸‹è¼‰æ¨¡å¼ (ä¸Šå¸‚/ä¸Šæ«ƒ/æ³•äºº/èè³‡/ä¼°å€¼)...")
    
    parallel_tasks = [
        # å¸‚å ´è¡Œæƒ…
        (step2_download_tpex_daily, (), {'silent_header': True}, "TPEx (ä¸Šæ«ƒ)", "2"),
        (step3_download_twse_daily, (), {'silent_header': True}, "TWSE (ä¸Šå¸‚)", "3"),
        # ç±Œç¢¼èˆ‡ä¼°å€¼
        (step3_5_download_institutional, (60,), {'silent_header': True}, "æ³•äººè²·è³£è¶…", "3.5"),
        (step3_6_download_major_holders, (), {'silent_header': True}, "é›†ä¿å¤§æˆ¶", "3.6"),
        (step3_7_fetch_margin_data, (60,), {'silent_header': True}, "èè³‡èåˆ¸", "3.7"),
        (step3_8_fetch_market_index, (), {'silent_header': True}, "å¤§ç›¤æŒ‡æ•¸", "3.8"),
        (PePbDataAPI.fetch_all_pepb, (), {}, "PE/PBä¼°å€¼", "3.9"),
        (ShareholderDataAPI.fetch_all_shareholder, (), {}, "é›†ä¿æˆ¶æ•¸", "3.10"),
    ]
    
    # åŸ·è¡Œä¸¦è¡Œä»»å‹™ (å¢åŠ  max_workers ä»¥å®¹ç´æ›´å¤š I/O å¯†é›†ä»»å‹™)
    results = run_parallel_tasks(parallel_tasks, max_workers=8, show_progress=True)
    
    # æ”¶é›†æ›´æ–°çš„è‚¡ç¥¨ä»£ç¢¼
    if results.get("TPEx (ä¸Šæ«ƒ)") and isinstance(results["TPEx (ä¸Šæ«ƒ)"], set):
        updated_codes.update(results["TPEx (ä¸Šæ«ƒ)"])
    if results.get("TWSE (ä¸Šå¸‚)") and isinstance(results["TWSE (ä¸Šå¸‚)"], set):
        updated_codes.update(results["TWSE (ä¸Šå¸‚)"])
    
    # Step 4: æª¢æŸ¥æ•¸æ“šç¼ºå¤±
    out.header("æª¢æŸ¥æ•¸æ“šç¼ºå¤±", "4")
    step4_check_data_gaps()
    
    # Step 5: æ¸…ç†ä¸‹å¸‚è‚¡ç¥¨
    out.header("æ¸…ç†ä¸‹å¸‚è‚¡ç¥¨", "5")
    step5_clean_delisted()
    
    # Step 6: è£œæ¼
    out.header("é©—è­‰ä¸€è‡´æ€§ä¸¦è£œæ¼", "6")
    data = step4_load_data()
    s6 = step6_verify_and_backfill(data, resume=True, skip_downloads=True)
    if isinstance(s6, set):
        updated_codes.update(s6)
    
    # Step 7: è¨ˆç®—æŒ‡æ¨™
    out.header("è¨ˆç®—æŠ€è¡“æŒ‡æ¨™", "7")
    step7_calc_indicators(data)
    
    # Step 8: åŒæ­¥ Supabase
    out.header("åŒæ­¥é›²ç«¯", "8")
    step8_sync_supabase()
    
    # æ›´æ–°å¿«å–
    if GLOBAL_INDICATOR_CACHE is None:
        GLOBAL_INDICATOR_CACHE = IndicatorCacheManager()
    GLOBAL_INDICATOR_CACHE.set_data(data)
    
    # å®Œæˆ
    out.box_end("æ¯æ—¥æ›´æ–°å®Œæˆï¼å¿«å–å·²æ›´æ–°ï¼Œå¯ç›´æ¥é€²è¡Œæƒæ")


def _run_quick_update():
    """å¿«é€Ÿæ›´æ–° (åƒ… 2->3->7ï¼Œè·³éè£œæ¼)"""
    step2_download_tpex_daily()
    step3_download_twse_daily()
    step7_calc_indicators()
    
    if GLOBAL_INDICATOR_CACHE:
        GLOBAL_INDICATOR_CACHE.clear()
    print_flush("âœ“ ç³»çµ±å¿«å–å·²æ¸…é™¤")


def backup_menu():
    """è³‡æ–™åº«å‚™ä»½èˆ‡é‚„åŸé¸å–®"""
    import shutil
    
    while True:
        print_flush("\nã€è³‡æ–™åº«å‚™ä»½èˆ‡é‚„åŸã€‘")
        print_flush("[1] å»ºç«‹å‚™ä»½")
        print_flush("[2] é‚„åŸå‚™ä»½")
        print_flush("[3] åˆ—å‡ºæ‰€æœ‰å‚™ä»½")
        print_flush("[0] è¿”å›")
        
        ch = read_single_key()
        
        if ch == '0':
            return
            
        if ch == '1':
            print_flush("æ­£åœ¨å‚™ä»½è³‡æ–™åº«...")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = WORK_DIR / f"backup_{timestamp}.db"
            try:
                shutil.copy2(DB_FILE, backup_file)
                size_mb = backup_file.stat().st_size / (1024*1024)
                print_flush(f"âœ“ å‚™ä»½å®Œæˆ: {backup_file.name}")
                print_flush(f"   æª”æ¡ˆå¤§å°: {size_mb:.1f} MB")
            except Exception as e:
                print_flush(f"âŒ å‚™ä»½å¤±æ•—: {e}")
                
        elif ch == '2':
            print_flush("è«‹æ‰‹å‹•å°‡å‚™ä»½æª”è¦†è“‹ taiwan_stock.db (éœ€é‡å•Ÿç¨‹å¼)")
            
        elif ch == '3':
            backups = list(WORK_DIR.glob("backup_*.db"))
            if not backups:
                print_flush("ç„¡å‚™ä»½æª”æ¡ˆ")
            else:
                for b in backups:
                    print_flush(f"- {b.name} ({b.stat().st_size/(1024*1024):.1f} MB)")
        
        print_flush("\næŒ‰ Enter ç¹¼çºŒ...")
        sys.stdin.readline()

def check_db_nulls():
    """æª¢æŸ¥è³‡æ–™åº«ç©ºå€¼ç‡"""
    print_flush("\næ­£åœ¨æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§...")
    with db_manager.get_connection() as conn:
        try:
            cur = conn.execute("SELECT COUNT(*) FROM stock_history")
            total = cur.fetchone()[0]
            
            if total == 0:
                print_flush("è³‡æ–™åº«ç‚ºç©º")
                return

            nulls = {}
            for col in ['open', 'high', 'low', 'close', 'volume']:
                cur = conn.execute(f"SELECT COUNT(*) FROM stock_history WHERE {col} IS NULL")
                nulls[col] = cur.fetchone()[0]
            
            print_flush(f"ç¸½ç­†æ•¸: {total}")
            for col, count in nulls.items():
                pct = (count / total) * 100
                print_flush(f"- {col} ç©ºå€¼: {count} ({pct:.2f}%)")
                
        except Exception as e:
            print_flush(f"æª¢æŸ¥å¤±æ•—: {e}")

def delete_data_by_date():
    """åˆªé™¤æŒ‡å®šæ—¥æœŸçš„è³‡æ–™"""
    date_str = input("è«‹è¼¸å…¥è¦åˆªé™¤çš„æ—¥æœŸ (YYYYMMDD): ").strip()
    if not date_str.isdigit() or len(date_str) != 8:
        print_flush("æ—¥æœŸæ ¼å¼éŒ¯èª¤")
        return
        
    date_int = int(date_str)
    print_flush(f"ç¢ºå®šè¦åˆªé™¤ {date_int} çš„æ‰€æœ‰è³‡æ–™å—? (y/n)")
    if input().lower() != 'y':
        return
        
    with db_manager.get_connection() as conn:
        try:
            conn.execute("DELETE FROM stock_history WHERE date_int = ?", (date_int,))
            conn.execute("DELETE FROM institutional_investors WHERE date_int = ?", (date_int,))
            conn.commit()
            print_flush(f"âœ“ å·²åˆªé™¤ {date_int} çš„è³‡æ–™")
        except Exception as e:
            print_flush(f"åˆªé™¤å¤±æ•—: {e}")

def _check_api_connection_status():
    """æª¢æŸ¥ API é€£ç·šç‹€æ…‹"""
    print_flush("\nã€API é€£ç·šç‹€æ…‹æª¢æŸ¥ã€‘")
    endpoints = [
        ("TWSE è­‰äº¤æ‰€", "https://www.twse.com.tw"),
        ("TPEx æ«ƒè²·ä¸­å¿ƒ", "https://www.tpex.org.tw"),
        ("FinMind", "https://api.finmindtrade.com")
    ]
    
    for name, url in endpoints:
        try:
            resp = requests.get(url, timeout=5, verify=False)
            status = "æ­£å¸¸" if resp.status_code == 200 else f"ç•°å¸¸ ({resp.status_code})"
            print_flush(f"âœ“ {name}: {status}")
        except Exception as e:
            print_flush(f"âŒ {name}: é€£ç·šå¤±æ•—")

def maintenance_menu():
    """ç³»çµ±ç¶­è­·é¸å–®"""
    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€ç³»çµ±ç¶­è­·ã€‘")
        print_flush("="*60)
        print_flush("[1] è³‡æ–™åº«å‚™ä»½èˆ‡é‚„åŸ")
        print_flush("[2] æª¢æŸ¥ API é€£ç·šç‹€æ…‹")
        print_flush("[3] æª¢æŸ¥è³‡æ–™å®Œæ•´æ€§ (ç©ºå€¼ç‡)")
        print_flush("[4] åˆªé™¤æŒ‡å®šæ—¥æœŸè³‡æ–™")
        print_flush("[5] åŒæ­¥è³‡æ–™åˆ° Supabase")
        print_flush("[0] è¿”å›ä¸»é¸å–®")
        
        ch = read_single_key()
        
        if ch == '0':
            return
            
        if ch == '1':
            backup_menu()
        elif ch == '2':
            _check_api_connection_status()
        elif ch == '3':
            check_db_nulls()
        elif ch == '4':
            delete_data_by_date()
        elif ch == '5':
            step8_sync_supabase()
            
        print_flush("\næŒ‰ Enter ç¹¼çºŒ...")
        sys.stdin.readline()


def display_scan_results_v2(results, title, limit=30, extra_headers=None, extra_func=None, description=""):
    """
    çµ±ä¸€æƒæçµæœé¡¯ç¤ºå‡½æ•¸ (v2) - ç¬¦åˆ SDD è¦ç¯„
    
    Args:
        results: List of dict or tuple. If dict, must contain 'code', 'name', 'close', 'vol_ratio'.
                 If tuple, logic depends on legacy support (try to avoid).
        title: ç­–ç•¥æ¨™é¡Œ
        limit: é¡¯ç¤ºæ•¸é‡
        extra_headers: List[str], é¡å¤–æ¬„ä½åç¨±
        extra_func: Callable(code, item_data) -> List[str], å›å‚³é¡å¤–æ¬„ä½å€¼
        description: ç­–ç•¥èªªæ˜
    """
    if not results:
        print_flush(f"\nâŒ {title}: æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„è‚¡ç¥¨")
        return []

    print_flush("\n" + "="*90)
    print_flush(f"ã€{title}ã€‘ (å‰ {limit} ç­†)")
    
    # æ¨™æº–æ¬„ä½: ä»£è™Ÿ(6) åç¨±(8) æ”¶ç›¤(10) æˆäº¤é‡(é‡æ¯”)(18) VSBCä¸Š/ä¸‹(12) VPä¸Š/ä¸‹(12)
    # ç¸½å¯¬åº¦: 6+1+8+1+10+1+18+1+12+1+12 = 70
    # åŠ ä¸Šé¡å¤–æ¬„ä½
    
    header_str = f"{'ä»£è™Ÿ':<6} {'åç¨±':<8} {'æ”¶ç›¤':<10} {'æˆäº¤é‡(é‡æ¯”)':<18} {'VSBCä¸Š/ä¸‹':<12} {'VPä¸Š/ä¸‹':<12}"
    
    if extra_headers:
        for h in extra_headers:
            header_str += f" {h:<10}"
            
    print_flush(header_str)
    print_flush("-" * len(header_str)) # å‹•æ…‹é•·åº¦
    
    count = 0
    display_codes = []
    reset = reset_color()
    
    # é å…ˆè¼‰å…¥ VSBC/VP æ•¸æ“š (è‹¥ results ä¸­æ²’æœ‰)
    # ç‚ºäº†æ•ˆèƒ½ï¼Œé€™è£¡å‡è¨­ results å·²ç¶“åŒ…å«æˆ–æˆ‘å€‘å³æ™‚è®€å– (cache)
    # è‹¥æ˜¯ tuple æ ¼å¼ï¼Œéœ€è¦ extra_func è™•ç†
    
    for item in results:
        if count >= limit:
            break
            
        try:
            # è§£æ item
            if isinstance(item, dict):
                code = item.get('code')
                name = item.get('name', '')
                close = item.get('close', 0)
                vol = item.get('volume', 0)
                vol_ratio = item.get('vol_ratio', 0)
                item_data = item # Pass full dict to extra_func
            else:
                # Legacy tuple support: (code, sort_val, ind, ...)
                code = item[0]
                ind = item[2] if len(item) > 2 and isinstance(item[2], dict) else {}
                name = ind.get('name', '')
                close = safe_float_preserving_none(ind.get('close')) or 0
                vol = safe_float_preserving_none(ind.get('volume')) or 0
                vol_prev = safe_float_preserving_none(ind.get('vol_prev'))
                vol_ma60 = safe_float_preserving_none(ind.get('vol_ma60')) # å‡è¨­æœ‰
                
                # è¨ˆç®—é‡æ¯” (è‹¥ç„¡)
                if vol_ratio := ind.get('vol_ratio'):
                    pass
                elif vol_prev and vol_prev > 0:
                    vol_ratio = vol / vol_prev
                else:
                    vol_ratio = 0
                
                item_data = ind # Pass indicator dict
                if not name:
                    # Try to get name from meta
                    name = get_correct_stock_name(code)
            
            # å–å¾— VSBC/VP æ•¸æ“š (å¾ Cache æˆ– Item)
            # é€™è£¡ç°¡åŒ–ï¼šè‹¥ item_data æœ‰å‰‡ç”¨ï¼Œç„¡å‰‡é¡¯ç¤º -/-
            vsbc_upper = item_data.get('vsbc_upper') or item_data.get('VSBC_upper') or 0
            vsbc_lower = item_data.get('vsbc_lower') or item_data.get('VSBC_lower') or 0
            vp_upper = item_data.get('vp_upper') or item_data.get('VP_upper') or 0
            vp_lower = item_data.get('vp_lower') or item_data.get('VP_lower') or 0
            
            # æ ¼å¼åŒ–æ•¸å€¼
            c_price = get_color_code(1) # ç°¡åŒ–: é è¨­ç´…è‰²ï¼Œæˆ–éœ€æ¯”è¼ƒæ˜¨æ—¥æ”¶ç›¤
            # è‹¥æœ‰ close_prev å¯æ¯”è¼ƒ
            close_prev = item_data.get('close_prev') or item_data.get('ref_price')
            if close_prev:
                c_price = get_trend_color(close, close_prev)
            
            price_str = f"{c_price}{close:<10.2f}{reset}"
            vol_str = f"{int(vol/1000)}å¼µ({vol_ratio:.1f})"
            
            vsbc_str = f"{int(vsbc_lower)}/{int(vsbc_upper)}" if vsbc_upper else "-/-"
            vp_str = f"{int(vp_lower)}/{int(vp_upper)}" if vp_upper else "-/-"
            
            # çµ„åˆåŸºæœ¬å­—ä¸²
            row_str = f"{code:<6} {name:<8} {price_str} {vol_str:<18} {vsbc_str:<12} {vp_str:<12}"
            
            # è™•ç†é¡å¤–æ¬„ä½
            if extra_func:
                extras = extra_func(code, item_data)
                for e in extras:
                    row_str += f" {str(e):<10}"
            
            print_flush(row_str)
            display_codes.append(code)
            count += 1
            
        except Exception as e:
            # print_flush(f"Error displaying {item}: {e}")
            continue
            
    print_flush("-" * len(header_str))
    if description:
        print_flush(description)
        print_flush("-" * len(header_str))
        
    print_flush(f"å…±æ‰¾åˆ° {len(results)} æª”ç¬¦åˆæ¢ä»¶")
    
    prompt_stock_detail_report(display_codes)
    return display_codes


def _draw_kbar_chart(code, name=""):
    """ç¹ªè£½å€‹è‚¡ K ç·šåœ–ï¼ˆä½¿ç”¨ Plotlyï¼‰"""
    try:
        # å˜—è©¦å°å…¥ plotly
        import plotly.graph_objects as go
        from plotly.subplots import make_subplots
    except ImportError:
        print_flush("âŒ éœ€è¦å®‰è£ plotly å¥—ä»¶: pip install plotly")
        return
    
    print_flush(f"\næ­£åœ¨è¼‰å…¥ {code} çš„æ­·å²è³‡æ–™...")
    
    # å¾è³‡æ–™åº«è®€å–æ­·å²è³‡æ–™
    with db_manager.get_connection() as conn:
        df = pd.read_sql_query("""
            SELECT date_int, open, high, low, close, volume
            FROM stock_history
            WHERE code = ?
            ORDER BY date_int DESC
            LIMIT 120
        """, conn, params=(code,))
    
    if df.empty:
        print_flush(f"âŒ æ‰¾ä¸åˆ° {code} çš„æ­·å²è³‡æ–™")
        return
    
    # è½‰æ›æ—¥æœŸæ ¼å¼
    df['date'] = pd.to_datetime(df['date_int'].astype(str), format='%Y%m%d')
    df = df.sort_values('date')
    df = df.set_index('date')
    
    # è¨ˆç®—ç§»å‹•å¹³å‡ç·š
    df['MA5'] = df['close'].rolling(window=5).mean()
    df['MA20'] = df['close'].rolling(window=20).mean()
    df['MA60'] = df['close'].rolling(window=60).mean()
    
    # å»ºç«‹å­åœ–
    fig = make_subplots(rows=2, cols=1, shared_xaxes=True,
                        vertical_spacing=0.03,
                        subplot_titles=(f'{code} {name} Kç·šåœ–', 'æˆäº¤é‡'),
                        row_heights=[0.7, 0.3])
    
    # K ç·šåœ–
    fig.add_trace(go.Candlestick(
        x=df.index,
        open=df['open'],
        high=df['high'],
        low=df['low'],
        close=df['close'],
        name='Kç·š',
        increasing_line_color='red',
        decreasing_line_color='green'
    ), row=1, col=1)
    
    # ç§»å‹•å¹³å‡ç·š
    fig.add_trace(go.Scatter(x=df.index, y=df['MA5'], opacity=0.7,
                             line=dict(color='blue', width=1), name='MA5'), row=1, col=1)
    fig.add_trace(go.Scatter(x=df.index, y=df['MA20'], opacity=0.7,
                             line=dict(color='orange', width=1), name='MA20'), row=1, col=1)
    fig.add_trace(go.Scatter(x=df.index, y=df['MA60'], opacity=0.7,
                             line=dict(color='purple', width=1), name='MA60'), row=1, col=1)
    
    # æˆäº¤é‡æŸ±ç‹€åœ–
    colors = ['red' if c >= o else 'green' for o, c in zip(df['open'], df['close'])]
    fig.add_trace(go.Bar(x=df.index, y=df['volume'], marker_color=colors, name='æˆäº¤é‡'), row=2, col=1)
    
    # ç‰ˆé¢è¨­å®š
    fig.update_layout(
        title=f'{code} {name} å€‹è‚¡ K ç·šåœ–',
        yaxis_title='åƒ¹æ ¼',
        yaxis2_title='æˆäº¤é‡',
        xaxis_rangeslider_visible=False,
        template='plotly_white',
        height=800,
        hovermode='x unified'
    )
    
    # ç§»é™¤éäº¤æ˜“æ—¥ç©ºç™½
    fig.update_xaxes(type='category')


def _handle_stock_query(code):
    """è™•ç†å€‹è‚¡æŸ¥è©¢ - å®Œæ•´ç‰ˆï¼ˆå³æ™‚ + æ­·å²å¤šå¤©ï¼‰"""
    # å–å¾—è‚¡ç¥¨åç¨±
    name = code
    if code in twstock.codes:
        stock_info = twstock.codes[code]
        name = stock_info.name
    else:
        name = get_correct_stock_name(code)
        if name == code:
            print_flush(f"âŒ æ‰¾ä¸åˆ°è‚¡ç¥¨ä»£è™Ÿ: {code}")
            return

    # è©¢å•é¡¯ç¤ºå¤©æ•¸
    try:
        days_input = input("é¡¯ç¤ºå¤©æ•¸(é è¨­10å¤©): ").strip()
        days = int(days_input) if days_input.isdigit() and int(days_input) > 0 else 10
    except:
        days = 10
    
    print_flush(f"\næ­£åœ¨æŸ¥è©¢ {code} {name} ...\n")
    
    # ===== 1. å³æ™‚è‚¡åƒ¹ (Realtime) =====
    print_flush(f"=== {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} å³æ™‚è‚¡åƒ¹ ({code} {name}) ===")
    try:
        stock_realtime = twstock.realtime.get(code)
        if stock_realtime.get('success'):
            rt = stock_realtime.get('realtime', {})
            info = stock_realtime.get('info', {})
            
            # æ ¼å¼åŒ–æ•¸å­—ç‚ºå°æ•¸é»å¾ŒäºŒä½
            latest_price = safe_float_preserving_none(rt.get('latest_trade_price'))
            if latest_price is None:
                latest_price = safe_float_preserving_none(rt.get('z')) # Fallback to 'z'
            
            open_price = safe_float_preserving_none(rt.get('open'))
            high_price = safe_float_preserving_none(rt.get('high'))
            low_price = safe_float_preserving_none(rt.get('low'))
            volume = safe_int(rt.get('accumulate_trade_volume'), 0)
            
            print_flush(f"è‚¡ç¥¨åç¨±: {info.get('name', name)}")
            print_flush(f"ç›®å‰è‚¡åƒ¹: {latest_price:.2f}" if latest_price else "ç›®å‰è‚¡åƒ¹: N/A")
            print_flush(f"é–‹ç›¤: {open_price:.2f}  æœ€é«˜: {high_price:.2f}  æœ€ä½: {low_price:.2f}  æˆäº¤é‡: {volume:,} å¼µ" if open_price else "é–‹ç›¤: N/A")
        else:
            print_flush(f"âš  å³æ™‚å ±åƒ¹æŸ¥è©¢å¤±æ•—: {stock_realtime.get('rtmessage', 'æœªçŸ¥éŒ¯èª¤')}")
    except Exception as e:
        print_flush(f"âš  å³æ™‚å ±åƒ¹æŸ¥è©¢å¤±æ•—: {e}")
    
    print_flush("\n" + "="*80 + "\n")
    
    # ===== 2. æ­·å²è³‡æ–™ - ä½¿ç”¨æŠ€è¡“æŒ‡æ¨™æ ¼å¼é¡¯ç¤º =====
    print_flush(f"=== ã€{name} {code}ã€‘è¿‘ {days} å¤©èµ°å‹¢ ===")
    print_flush("="*80)
    
    try:
        # ä½¿ç”¨ calculate_stock_history_indicators è¨ˆç®—æŠ€è¡“æŒ‡æ¨™
        indicators_list = calculate_stock_history_indicators(code, display_days=days)
        
        if indicators_list:
            # ä½¿ç”¨ format_scan_result_list çµ±ä¸€é¡¯ç¤ºæ ¼å¼
            print_flush(format_scan_result_list(code, name, indicators_list))
        else:
            print_flush("(ç„¡æ­·å²æ•¸æ“š)")
    except Exception as e:
        print_flush(f"âš  è®€å–æ­·å²æ•¸æ“šå¤±æ•—: {e}")
    
    print_flush("="*80)
    
    # ç¬¦åˆè¦å‰‡ 7 å’Œ 8ï¼šè¼¸å…¥ 1 Kç·šåœ–ï¼ŒæŒ‰ 0 è¿”å›
    print_flush("\nè¼¸å…¥ 1 Kç·šåœ–ï¼Œæˆ–æŒ‰ 0 è¿”å›: ", end="")
    try:
        ch = input().strip()
        if ch == '1':
            _draw_kbar_chart(code, name)
    except:
        pass
    # ç›´æ¥è¿”å›ä¸»é¸å–®

def _get_ranking_params():
    """ç²å–ä½¿ç”¨è€…è¼¸å…¥çš„åƒæ•¸ (æ’è¡Œæ¦œå°ˆç”¨)"""
    # é¸æ“‡æª”æ•¸
    try:
        print("é¸æ“‡æª”æ•¸(é è¨­10æª”): ", end='', flush=True)
        s = sys.stdin.readline().strip()
        top_n = int(s) if s.isdigit() and int(s) > 0 else 10
    except:
        top_n = 10
    
    # é€£çºŒå¤©æ•¸
    try:
        print("é€£çºŒè²·å…¥/è³£å‡ºå¤©æ•¸(é è¨­2å¤©): ", end='', flush=True)
        s = sys.stdin.readline().strip()
        min_days = int(s) if s.isdigit() and int(s) > 0 else 2
    except:
        min_days = 2
    
    # æ’åºæ–¹å¼
    print("[1] ä¾å¼µæ•¸æ’åº [2] ä¾ç¸½é‡‘é¡æ’åº (é è¨­1): ", end='', flush=True)
    try:
        s = sys.stdin.readline().strip()
        sort_by_amount = (s == '2')
    except:
        sort_by_amount = False
    
    return top_n, min_days, sort_by_amount

def _get_ranking_close_prices():
    """å–å¾—æ”¶ç›¤åƒ¹ç”¨æ–¼è¨ˆç®—é‡‘é¡ (æ’è¡Œæ¦œå°ˆç”¨)"""
    try:
        with db_manager.get_connection() as conn:
            cursor = conn.execute("""
                SELECT code, close FROM stock_history 
                WHERE date_int = (SELECT MAX(date_int) FROM stock_history)
            """)
            return {row[0]: row[1] for row in cursor.fetchall()}
    except:
        return {}

def _display_ranking(rank_type, title, top_n, min_days, sort_by_amount):
    """é¡¯ç¤ºæ’è¡Œæ¦œ"""
    
    print_flush(f"\næ­£åœ¨å–å¾— {title}...")
    print_flush(f"(é¡¯ç¤ºå‰{top_n}æª”, é€£çºŒ{min_days}å¤©ä»¥ä¸Š, {'ä¾é‡‘é¡' if sort_by_amount else 'ä¾å¼µæ•¸'})")
    
    try:
        with db_manager.get_connection() as conn:
            # å–å¾—æœ€æ–°æ—¥æœŸ
            cur = conn.execute("SELECT MAX(date_int) FROM institutional_investors")
            res = cur.fetchone()
            latest_date = res[0] if res else None
            
            if not latest_date:
                print_flush("âŒ è³‡æ–™åº«ç„¡æ³•äººè³‡æ–™")
                return

            # åˆ¤æ–·è²·è¶…æˆ–è³£è¶…
            is_buy = 'buy' in rank_type
            
            # æŸ¥è©¢æ’è¡Œ
            if 'foreign' in rank_type:
                order_col = 'foreign_buy - foreign_sell'
            elif 'trust' in rank_type:
                order_col = 'trust_buy - trust_sell'
            else:
                order_col = 'dealer_buy - dealer_sell'
            
            # è²·è¶…: DESC (æ­£æ•¸è¶Šå¤§è¶Šå¥½), è³£è¶…: ASC (è² æ•¸è¶Šå¤§è¶Šå¥½)
            order_dir = 'DESC' if is_buy else 'ASC'

            # SQL query to get ranking
            sql = f"""
                SELECT stock_id, {order_col} as net_buy
                FROM institutional_investors
                WHERE date_int = ?
                ORDER BY {order_col} {order_dir}
            """
            cur = conn.execute(sql, (latest_date,))
            rows = cur.fetchall()
            
            # Filter and process results
            data = []
            close_prices = _get_ranking_close_prices()
            
            # Get names
            cur = conn.execute("SELECT code, name FROM stock_snapshot")
            stock_names = {row[0]: row[1] for row in cur.fetchall()}

            for row in rows:
                code = row[0]
                net_buy = row[1]
                
                # Filter by direction (Buy > 0, Sell < 0)
                if is_buy and net_buy <= 0: continue
                if not is_buy and net_buy >= 0: continue
                
                data.append({
                    'stock_id': code,
                    'net_buy': net_buy
                })
            
            # Sort by amount if needed
            if sort_by_amount:
                for item in data:
                    code = item['stock_id']
                    close = close_prices.get(code, 0)
                    item['amount'] = abs(item['net_buy']) * 1000 * close
                data.sort(key=lambda x: x.get('amount', 0), reverse=True)
            
            # Display
            print_flush(f"\nã€{title}ã€‘ ({latest_date})")
            print_flush(f"{'#':<3} {'è‚¡ç¥¨åç¨±':<16} | {'è²·è³£è¶…(å¼µ)':>10} | {'é‡‘é¡(è¬)':>10} | {'é€£çºŒå¤©æ•¸':>8}")
            print_flush("-" * 60)
            
            count = 0
            for i, row in enumerate(data):
                if count >= top_n:
                    break
                
                code = row['stock_id']
                net_buy = row['net_buy']
                close = close_prices.get(code, 0)
                amount = (net_buy * 1000 * close) / 10000 if close else 0
                name = stock_names.get(code, '')
                display_name = f"{name}({code})" if name else code
                
                # Placeholder for consecutive days
                consec_days = "-"
                
                print_flush(f"{i+1:<3} {display_name:<16} | {net_buy:>10,} | {amount:>10,.0f} | {consec_days:>8}")
                count += 1
                
    except Exception as e:
        print_flush(f"âŒ æŸ¥è©¢å¤±æ•—: {e}")

def institutional_menu():
    """æ³•äººè²·è³£è¶…æ’è¡Œé¸å–®"""
    while True:
        print_flush("\n" + "="*60)
        print_flush("ã€æ³•äººè²·è³£è¶…æ’è¡Œã€‘")
        print_flush("="*60)
        print_flush("[1] å¤–è³‡è²·è¶…æ’è¡Œ")
        print_flush("[2] æŠ•ä¿¡è²·è¶…æ’è¡Œ")
        print_flush("[3] è‡ªç‡Ÿå•†è²·è¶…æ’è¡Œ")
        print_flush("[4] å¤–è³‡è³£è¶…æ’è¡Œ")
        print_flush("[5] æŠ•ä¿¡è³£è¶…æ’è¡Œ")
        print_flush("[6] è‡ªç‡Ÿå•†è³£è¶…æ’è¡Œ")
        print_flush("[0] è¿”å›ä¸»é¸å–®")
        print_flush("-" * 60)
        print_flush("ğŸ’¡ è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ (å¦‚ 2330) å¯ç›´æ¥æŸ¥çœ‹å€‹è‚¡")
        
        ch = read_single_key()
        
        if ch == '0':
            return
            
        # è‚¡ç¥¨ä»£è™ŸæŸ¥è©¢
        if ch.isdigit() and len(ch) == 4:
            _handle_stock_query(ch)
            continue
            
        rank_map = {
            '1': ('foreign_buy', 'å¤–è³‡è²·è¶…æ’è¡Œ'),
            '2': ('trust_buy', 'æŠ•ä¿¡è²·è¶…æ’è¡Œ'),
            '3': ('dealer_buy', 'è‡ªç‡Ÿå•†è²·è¶…æ’è¡Œ'),
            '4': ('foreign_sell', 'å¤–è³‡è³£è¶…æ’è¡Œ'),
            '5': ('trust_sell', 'æŠ•ä¿¡è³£è¶…æ’è¡Œ'),
            '6': ('dealer_sell', 'è‡ªç‡Ÿå•†è³£è¶…æ’è¡Œ')
        }
        
        if ch in rank_map:
            rank_type, title = rank_map[ch]
            top_n, min_days, sort_by_amount = _get_ranking_params()
            _display_ranking(rank_type, title, top_n, min_days, sort_by_amount)
            
            # Pause to let user read
            print_flush("\næŒ‰ Enter ç¹¼çºŒ...")
            sys.stdin.readline()

def main_menu():
    """ä¸»é¸å–®"""
    global GLOBAL_INDICATOR_CACHE
    
    # åˆå§‹åŒ–
    try:
        ensure_db()
    except Exception as e:
        print_flush(f"DB Error: {e}")

    if GLOBAL_INDICATOR_CACHE is None:
        GLOBAL_INDICATOR_CACHE = IndicatorCacheManager()

    # è¡¨é©…å‹•æ³•ï¼šä¸»é¸å–®åŠŸèƒ½æ˜ å°„
    MAIN_MENU_ACTIONS = {
        '1': data_management_menu,
        '2': market_scan_menu,
        '3': institutional_menu,
        '4': maintenance_menu,
    }

    while True:
        # é¡¯ç¤ºç³»çµ±ç‹€æ…‹è³‡è¨Š
        display_system_status()
        
        print_flush("\n" + "="*60)
        print_flush("ã€å°ç£è‚¡å¸‚åˆ†æç³»çµ± v40 Enhancedã€‘")
        print_flush("="*60)
        print_flush("[1] è³‡æ–™ç®¡ç†èˆ‡æ›´æ–°")
        print_flush("[2] å¸‚å ´æƒæ (æŠ€è¡“æŒ‡æ¨™)")
        print_flush("[3] æ³•äººè²·è³£è¶…æ’è¡Œ")
        print_flush("[4] ç³»çµ±ç¶­è­·")
        print_flush("[0] é›¢é–‹ç³»çµ±")
        print_flush("-" * 60)
        print_flush("ğŸ’¡ è¼¸å…¥è‚¡ç¥¨ä»£è™Ÿ (å¦‚ 2330) å¯ç›´æ¥æŸ¥çœ‹å€‹è‚¡")
        
        choice = input("è«‹é¸æ“‡: ").strip().upper()
        
        # è¡›èªå¥ï¼šé›¢é–‹
        if choice == '0':
            print_flush("ğŸ‘‹ ç³»çµ±å·²é€€å‡º")
            sys.exit(0)
        
        # è¡›èªå¥ï¼šè‚¡ç¥¨ä»£è™ŸæŸ¥è©¢
        if choice.isdigit() and len(choice) == 4:
            _handle_stock_query(choice)
            continue
        
        # è¡¨é©…å‹•æ³•ï¼šæŸ¥æ‰¾ä¸¦åŸ·è¡Œ
        action = MAIN_MENU_ACTIONS.get(choice)
        if action:
            action()
        elif choice:
            print_flush("âŒ ç„¡æ•ˆè¼¸å…¥ï¼Œè«‹é‡æ–°é¸æ“‡")

# ==============================
# ä¸»ç¨‹å¼å…¥å£
# ==============================
if __name__ == "__main__":
    # è¨­ç½®æ—¥èªŒ
    log_file = WORK_DIR / 'system.log'
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    if not logger.handlers:
        file_handler = logging.FileHandler(log_file, encoding='utf-8', mode='a')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
        logger.addHandler(file_handler)
    
    # å•Ÿå‹•ä¸»é¸å–®
    if len(sys.argv) > 1 and sys.argv[1] == '--auto-update':
        # åˆå§‹åŒ–è³‡æ–™åº«
        try:
            ensure_db()
        except Exception as e:
            print(f"DB Init Error: {e}")

        # åˆå§‹åŒ–å…¨åŸŸå¿«å–
        if GLOBAL_INDICATOR_CACHE is None:
            GLOBAL_INDICATOR_CACHE = IndicatorCacheManager()
            
        print_flush("[AUTO] å•Ÿå‹•è‡ªå‹•æ›´æ–°æ¨¡å¼ (Steps 1-8)...")
        step1_fetch_stock_list()
        step2_download_tpex_daily()
        step3_download_twse_daily()
        step3_5_download_institutional(days=3)  # æ³•äººè³‡æ–™ (æ™ºæ…§è£œæ¼)
        step3_6_download_major_holders()        # é›†ä¿å¤§æˆ¶
        step3_7_fetch_margin_data()             # èè³‡èåˆ¸
        step3_8_fetch_market_index()            # å¤§ç›¤æŒ‡æ•¸
        step4_check_data_gaps()
        step5_clean_delisted()
        data = step4_load_data()
        updated_codes = step6_verify_and_backfill(data, resume=True)
        
        # å¦‚æœæœ‰æ›´æ–°ï¼Œé‡æ–°è¨ˆç®—æŒ‡æ¨™
        if updated_codes:
            print_flush("âœ“ åµæ¸¬åˆ°è³‡æ–™æ›´æ–°ï¼Œæ¸…é™¤å¿«å–...")
            if GLOBAL_INDICATOR_CACHE:
                GLOBAL_INDICATOR_CACHE.clear()
        
        step7_calc_indicators(data)
        step8_sync_supabase()
        
        print_flush("[DONE] è‡ªå‹•æ›´æ–°å®Œæˆ")
        sys.exit(0)

    main_menu()

# ==============================
# ç¨‹å¼å…¥å£é»
# ==============================
if __name__ == '__main__':
    import multiprocessing
    multiprocessing.freeze_support()
    main_menu()
